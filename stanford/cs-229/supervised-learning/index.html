<!DOCTYPE html><html lang=en><head><!-- base href=../../ --><title>CS 229 - Supervised Learning Cheatsheet</title><meta charset=utf-8><meta content="Teaching page of Shervine Amidi, Graduate Student at Stanford University." name=description><meta content="teaching, shervine, shervine amidi, data science" name=keywords><meta content="width=device-width, initial-scale=1" name=viewport>
<link href=https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning rel=canonical>
<link href=../../css/bootstrap.min.css rel=stylesheet>
<link href=../../css/font-awesome.min.css rel=stylesheet>
<link href=../../css/katex.min.css rel=stylesheet>
<link href=../../css/style.min.css rel=stylesheet type=text/css>
<link href=../../css/article.min.css rel=stylesheet>
<script src=../../js/jquery.min.js></script>
<script src=../../js/bootstrap.min.js></script>
<script defer src=../../js/underscore-min.js type=text/javascript></script>
<script defer src=../../js/katex.min.js></script>
<script defer src=../../js/auto-render.min.js></script>
<script defer src=../../js/article.min.js></script>
<script defer src=../../js/lang.min.js></script>
<script async defer src=../../js/buttons.js></script>
</head> <body data-offset=50 data-spy=scroll data-target=.navbar> <!-- HEADER <nav class="navbar navbar-inverse navbar-static-top"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class="collapse navbar-collapse" id=myNavbar> <ul class="nav navbar-nav"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class="nav navbar-nav navbar-center"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class="collapse navbar-collapse" data-target=None id=HiddenNavbar> <ul class="nav navbar-nav navbar-right"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style="padding: 0px;"> <img alt=MIT netsrc=images/ src=../../images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style="padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;"> </a> </ul> </div> </div> </div> </nav> --> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title style=display:none;> <!-- DISPLAY:NONE --> <a href=teaching/cs-229 onclick=trackOutboundLink(this);><img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48 style="width: 15px;">   <b>CS 229 - Machine Learning</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>Supervised Learning</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#introduction>Введение</a></div> <div class=dropdown-container> <a href=#introduction><span>Тип предсказания</span></a> <a href=#introduction><span>Тип модели</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#notations>Обозначения и основные понятия</a></div> <div class=dropdown-container> <a href=#notations><span>Функция потерь</span></a> <a href=#notations><span>Градиентный спуск</span></a> <a href=#notations><span>Правдоподобие</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#linear-models>Линейные модели</a></div> <div class=dropdown-container> <a href=#linear-models><span>Линейная регрессия</span></a> <a href=#linear-models><span>Логистическая регрессия</span></a> <a href=#linear-models><span>Обобщенные линейные модели</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#svm>Метод Опорных Векторов</a></div> <div class=dropdown-container> <a href=#svm><span>Оптимальный классификатор с зазором</span></a> <a href=#svm><span>Hinge loss</span></a> <a href=#svm><span>Ядро</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#generative-learning>Генеративное обучение</a></div> <div class=dropdown-container> <a href=#generative-learning><span>Гауссовский дискриминантный анализ</span></a> <a href=#generative-learning><span>Наивный Байес</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#tree>Деревья и ансамблевые методы</a></div> <div class=dropdown-container> <a href=#tree><span>CART</span></a> <a href=#tree><span>Случайный лес</span></a> <a href=#tree><span>Boosting</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#other>Другие методы</a></div> <div class=dropdown-container> <a href=#other><span>k-NN</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#learning-theory>Теория вычислительного обучения</a></div> <div class=dropdown-container> <a href=#learning-theory><span>Неравенство Хёфдинга</span></a> <a href=#learning-theory><span>PAC</span></a> <a href=#learning-theory><span>VC размерность</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-supervised-learning.pdf onclick=trackOutboundLink(this); style="color: white; text-decoration:none;"> <i aria-hidden=false class="fa fa-github fa-fw"></i> Посмотреть PDF-версию на GitHub </a> </li> </div> </center> </div> <article class="markdown-body entry-content" itemprop=text>
<div class="alert alert-primary" role=alert style=display:none;> <!-- DISPLAY:NONE -->
  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!
</div>
<div class=title-lang style=display:none;> <!-- DISPLAY:NONE -->
  <a aria-hidden=true class=anchor-bis href=#cs-229---machine-learning id=cs-229---machine-learning></a><a href=teaching/cs-229 onclick=trackOutboundLink(this);>CS 229 - Machine Learning</a>
  
  <div style=float:right;display:none;> <!-- DISPLAY:NONE -->
    <div class=input-group>
      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>
        <option value=ar>العربية</option>
        <option selected value=en>English</option>
        <option value=es>Español</option>
        <option value=fa>فارسی</option>
        <option value=fr>Français</option>
        <option value=ko>한국어</option>
        <option value=pt>Português</option>
        <option value=tr>Türkçe</option>
        <option value=vi>Tiếng Việt</option>
        <option value=zh>简中</option>
        <option value=zh-tw>繁中</option>
      </select>
      <div class=input-group-addon><i class=fa></i></div>
    </div>
  </div>
</div>
<br>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button>CS 221 - Artificial Intelligence</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button><B>CS 229 - Machine Learning</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button>CS 230 - Deep Learning</button>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/probability/index.html'; oldhref='teaching/cs-229/refresher-probabilities-statistics'" type=button>Probabilities</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/linear-algebra/index.html'; oldhref='teaching/cs-229/refresher-algebra-calculus'" type=button>Algebra</button>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button><B>Supervised Learning</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/unsupervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-unsupervised-learning'" type=button>Unsupervised Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/deep-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-deep-learning'" type=button>Deep Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/machine-learning-tips-and-tricks/index.html'; oldhref='teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks'" type=button>Tips and tricks</button>
  </div>
</div>
<h1>
  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>Шпаргалка по обучению с учителем
  <div style=float:right;display:none;> <!-- DISPLAY:NONE --><a aria-label="Star afshinea/stanford-cs-229-machine-learning on GitHub" class="github-button fa-fw" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-229-machine-learning onclick=trackOutboundLink(this);>Star</a></div>
</h1>
<i><!-- By --><a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> и <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a>;<a href=https://github.com/AlexandrParkhomenko onclick=trackOutboundLink(this);> Alexandr Parkhomenko</a> и {здесь можете быть Вы}</i>
<h2><a aria-hidden=true class=anchor href=#introduction id=introduction></a>Введение в обучение с учителем</h2>
<p>Задан набор точек данных $\{x^{(1)}, ..., x^{(m)}\}$ связанный с набором результатов $\{y^{(1)}, ..., y^{(m)}\}$, мы хотим создать классификатор, который научится предсказывать $y$ по $x$.</p>
<p><span class="new-item item-b">Тип предсказания</span> Различные типы прогнозных моделей перечислены в таблице ниже:</p>
<div class=mobile-container>
<center>
<table>
<tbody>
<tr>
<td align=center><b></b></td>
<td align=center><b>Регрессия</b></td>
<td align=center><b>Классификация</b></td>
</tr>
<tr>
<td align=center><b>Результат</b></td>
<td align=center>Непрерывный</td>
<td align=center>Класс</td>
</tr>
<tr>
<td align=center><b>Примеры</b></td>
<td align=center>Линейная регрессия</td>
<td align=center>Логистическая регрессия, SVM, Наивный Байес</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-b">Тип модели</span> Различные модели перечислены в таблице ниже:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:300px;">
  <colgroup>
    <col style=width:120px>
    <col style=width:50%>
    <col style=width:50%>
  </colgroup>
<tbody>
<tr>
<td align=center></td>
<td align=center><b>Дискриминационная модель</b></td>
<td align=center><b>Генеративная модель</b></td>
</tr>
<tr>
<td align=center><b>Цель</b></td>
<td align=left>Прямо оценить $P(y|x)$</td>
<td align=left>Оценить $P(x|y)$ затем Вывести $P(y|x)$</td>
</tr>
<tr>
<td align=center><b>Что изучено</b></td>
<td align=left>Граница решения</td>
<td align=left>Распределения вероятностей данных</td>
</tr>
<tr>
<td align=center><b>Иллюстрация</b></td>
<td align=center style="width: 41%;"><img alt="Discriminative model" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=discriminative-model.png?767b34c21d43a4fd8b59683578e132f9></td>
<td align=center style="width: 41%;"><img alt="Generative model" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=generative-model.png?df0642cec6e99ac162cd4848d26f41c3></td>
</tr>
<tr>
<td align=center><b>Примеры</b></td>
<td align=left>Регрессии, SVMs</td>
<td align=left>GDA, Наивный Байес</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#notations id=notations></a>Обозначения и основные понятия</h2>
<p><span class="new-item item-b">Гипотеза</span> Гипотеза обозначена $h_\theta$ и является выбранной нами моделью. Для заданных входных данных $x^{(i)}$ предсказанный результат модели обозначен $h_\theta(x^{(i)})$.</p>
<br>
<p><span class="new-item item-r">Функция потерь</span> это функция $L:(z,y)\in\mathbb{R}\times Y\longmapsto L(z,y)\in\mathbb{R}$ которая принимает в качестве входных данных прогнозируемое значение $z$, соответствующее значению реальных данных $y$, и выводит, насколько они различны. Общие функции потерь приведены в таблице ниже:</p>
<div class=mobile-container>
<center>
  <table style="table-layout:fixed; width:100%; min-width:820px;">
    <colgroup>
      <col style=width:25%>
      <col style=width:25%>
      <col style=width:25%>
      <col style=width:25%>
    </colgroup>
<tbody>
<tr>
<td align=center><b>Метод наименьших квадратов (LSE)</b></td>
<td align=center><b>Логистическая функция потерь</b></td>
<td align=center><b>Hinge loss</b></td>
<td align=center><b>Перекрёстная энтропия</b></td>
</tr>
<tr>
<td align=center>$\displaystyle\frac{1}{2}(y-z)^2$</td>
<td align=center>$\displaystyle\log(1+\exp(-yz))$</td>
<td align=center>$\displaystyle\max(0,1-yz)$</td>
<td align=center style=vertical-align:middle><div id=some_math style=font-size:75%>$\displaystyle-\Big[y\log(z)+(1-y)\log(1-z)\Big]$</div></td>
</tr>
<tr>
<td align=center style="width: 25%;"><img alt="Least squared error" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=least-square-error.png?63fef2552284b0dc15f27d1ef0b79fea></td>
<td align=center style="width: 25%;"><img alt="Logistic loss" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=logistic-loss.png?1bc1cb6d682c1bbfb978ec894afdf588></td>
<td align=center style="width: 25%;"><img alt="Hinge loss" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=hinge-loss.png?3f1b26410c446f52885dcc5266937c84></td>
<td align=center style="width: 25%;"><img alt="Cross entropy" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=cross-entropy.png?037ea4073873c9be4a7de099dac6d3b5></td>
</tr>
<tr>
<td align=center>Линейная регрессия</td>
<td align=center>Логистическая регрессия</td>
<td align=center>SVM</td>
<td align=center>Нейронная сеть</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-r">Функция стоимости</span> функция стоимости $J$ обычно используется для оценки предсказательной способности модели и определяется функцией потерь $L$ следующим образом:</p>
<div class=mobile-container>
\[\boxed{J(\theta)=\sum_{i=1}^mL(h_\theta(x^{(i)}), y^{(i)})}\]
</div>
<br>
<p><span class="new-item item-r">Градиентный спуск</span> правило обновления для градиентного спуска выражается скоростью обучения $\alpha\in\mathbb{R}$ и функцией стоимости $J$ следующим образом:</p>
<div class=mobile-container>
\[\boxed{\theta\longleftarrow\theta-\alpha\nabla J(\theta)}\]
</div>
<br>
<center>
  <img alt="Gradient descent" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=gradient-descent.png?01662c4a8147a55ba09f4f5c047641ba style=width:100%;max-width:500px>
</center>
<br>
<p><span class=remark>Примечание: Стохастический градиентный спуск (SGD) обновляет параметры на основе случайного обучающего примера, а пакетный градиентный спуск обновляет параметры используя пакет обучающих примеров.</span></p>
<br>
<p><span class="new-item item-b">Правдоподобие модели</span> Likelihood $L(\theta)$ при заданных параметрах $\theta$ используется для поиска оптимальных параметров $\theta$ посредством максимизации правдоподобия.</p>
<div class=mobile-container>
\[\boxed{\theta^{\textrm{opt}}=\underset{\theta}{\textrm{arg max }}L(\theta)}\]
</div>
<p><span class=remark>Примечание: На практике мы используем логарифмическую вероятность $\ell(\theta)=\log(L(\theta))$, которую легче оптимизировать. У нас есть:</span></p>
<br>
<p><span class="new-item item-r">Алгоритм Ньютона</span> это численный метод, который находит такое $\theta$, что $\ell'(\theta)=0$. Его правила обновления следующие:</p>
<div class=mobile-container>
\[\boxed{\theta\leftarrow\theta-\frac{\ell'(\theta)}{\ell''(\theta)}}\]
</div>
<p><span class=remark>Примечание: многомерное обобщение, также известное как метод Ньютона-Рафсона, имеет следующее правило обновления:</span></p>
<div class=mobile-container>
\[\theta\leftarrow\theta-\left(\nabla_\theta^2\ell(\theta)\right)^{-1}\nabla_\theta\ell(\theta)\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#linear-models id=linear-models></a>Линейные модели</h2>
<h3>Линейная регрессия</h3>
<p>Мы предполагаем здесь что $y|x;\theta\sim\mathcal{N}(\mu,\sigma^2)$</p>
<p><span class="new-item item-g">Нормальные уравнения</span> Обозначим $X$ как матрицу данных (объекты-признаки), значение $\theta$, которое минимизирует функцию стоимости, является решением в аналитическом виде, так что:</p>
<div class=mobile-container>
\[\boxed{\theta=(X^TX)^{-1}X^Ty}\]
</div>
<br>
<p><span class="new-item item-g">Алгоритм LMS</span> обозначим $\alpha$ скорость обучения, правило обновления алгоритма наименьших средних квадратов (LMS) для обучающего набора из $m$ точек данных, которое также известно как правило обучения Уидроу-Хоффа, выглядит следующим образом:</p>
<div class=mobile-container>
\[\boxed{\forall j,\quad \theta_j \leftarrow \theta_j+\alpha\sum_{i=1}^m\left[y^{(i)}-h_\theta(x^{(i)})\right]x_j^{(i)}}\]
</div>
<p><span class=remark>Примечание: правило обновления - это частный случай градиентного подъема.</span></p>
<br>
<p><span class="new-item item-b">LWR</span> Locally Weighted Regression - Локально взвешенная регрессия, представляет собой вариант линейной регрессии, который взвешивает каждый обучающий пример в его функции стоимости по $w^{(i)}(x)$, которая определяется параметром $\tau\in\mathbb{R}$ как:</p>
<div class=mobile-container>
\[\boxed{w^{(i)}(x)=\exp\left(-\frac{(x^{(i)}-x)^2}{2\tau^2}\right)}\]
</div>
<br>
<h3>Классификация и логистическая регрессия</h3>
<p><span class="new-item item-b">Сигмоидальная функция</span> сигмовидная функция $g$, также известная как логистическая функция, определяется следующим образом:</p>
<div class=mobile-container>
\[\forall z\in\mathbb{R},\quad\boxed{g(z)=\frac{1}{1+e^{-z}}\in]0,1[}\]
</div>
<br>
<p><span class="new-item item-b">Логистическая регрессия</span> Мы предполагаем здесь $y|x;\theta\sim\textrm{Bernoulli}(\phi)$. Имеем следующий вид:</p>
<div class=mobile-container>
\[\boxed{\phi=p(y=1|x;\theta)=\frac{1}{1+\exp(-\theta^Tx)}=g(\theta^Tx)}\]
</div>
<p><span class=remark>Remark: logistic regressions do not have closed form solutions.</span></p>
<br>
<p><span class="new-item item-b">Регрессия Softmax</span> Регрессия softmax, также называемая многоклассовой логистической регрессией, используется для обобщения логистической регрессии, когда существует более двух классов результатов. По соглашению мы полагаем $\theta_K=0$, что делает параметр Бернулли $\phi_i$ каждого класса $i$ равным:</p>
<div class=mobile-container>
\[\boxed{\displaystyle\phi_i=\frac{\exp(\theta_i^Tx)}{\displaystyle\sum_{j=1}^K\exp(\theta_j^Tx)}}\]
</div>
<br>
<h3>Обобщенные линейные модели</h3>
<p><span class="new-item item-r">Экспоненциальное семейство</span> класс распределений называется экспоненциальным семейством, если его можно записать в терминах естественного параметра, также называемого каноническим параметром или функцией связи, $\eta$, достаточной статистикой $T(y)$ и лог-статистическую сумму $a(\eta)$ следующим образом:</p>
<div class=mobile-container>
\[\boxed{p(y;\eta)=b(y)\exp(\eta T(y)-a(\eta))}\]
</div>
<p><span class=remark>Примечание: у нас будет часто $T(y)=y$. Также, $\exp(-a(\eta))$ можно рассматривать как параметр нормализации, который гарантирует, что сумма вероятностей равна единице.</span></p>
<p>Наиболее распространенные экспоненциальные распределения приведенны в следующей таблице:</p>
<div class=mobile-container>
<center>
<table>
<tbody>
<tr>
<td align=center><b>Распределения</b></td>
<td align=center><b>$\eta$</b></td>
<td align=center><b>$T(y)$</b></td>
<td align=center><b>$a(\eta)$</b></td>
<td align=center><b>$b(y)$</b></td>
</tr>
<tr>
<td align=center>Бернулли</td>
<td align=center>$\log\left(\frac{\phi}{1-\phi}\right)$</td>
<td align=center>$y$</td>
<td align=center>$\log(1+\exp(\eta))$</td>
<td align=center>$1$</td>
</tr>
<tr>
<td align=center>Гаусса</td>
<td align=center>$\mu$</td>
<td align=center>$y$</td>
<td align=center>$\frac{\eta^2}{2}$</td>
<td align=center>$\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{y^2}{2}\right)$</td>
</tr>
<tr>
<td align=center>Пуассона</td>
<td align=center>$\log(\lambda)$</td>
<td align=center>$y$</td>
<td align=center>$e^{\eta}$</td>
<td align=center>$\displaystyle\frac{1}{y!}$</td>
</tr>
<tr>
<td align=center>Геометрическое</td>
<td align=center>$\log(1-\phi)$</td>
<td align=center>$y$</td>
<td align=center>$\log\left(\frac{e^\eta}{1-e^\eta}\right)$</td>
<td align=center>$1$</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-r">Предположения GLM</span> Обобщенные линейные модели (GLM) нацелены на предсказание случайной величины $y$ как функции от $x\in\mathbb{R}^{n+1}$ и основываются на следующих трех предположениях:</p>
<div class=row>
 <div class=col-sm-4>$(1)\quad\boxed{y|x;\theta\sim\textrm{ExpFamily}(\eta)}$</div>
 <div class=col-sm-4>$(2)\quad\boxed{h_\theta(x)=E[y|x;\theta]}$</div>
 <div class=col-sm-4>$(3)\quad\boxed{\eta=\theta^Tx}$</div>
</div>
<br>
<p><span class=remark>Примечание: обыкновенный метод наименьших квадратов и логистическая регрессия являются частными случаями обобщенных линейных моделей.</span></p>
<br>
<h2><a aria-hidden=true class=anchor href=#svm id=svm></a>Метод Опорных Векторов</h2>
<p>Задача метода опорных векторов - найти линию, которая максимизирует минимальное расстояние до линии.</p>
<p><span class="new-item item-b">Оптимальный классификатор с зазором</span> $h$ таков, что:</p>
<div class=mobile-container>
\[\boxed{h(x)=\textrm{sign}(w^Tx-b)}\]
</div>
<p>где $(w, b)\in\mathbb{R}^n\times\mathbb{R}$ является решением следующей задачи оптимизации:</p>
<div class=mobile-container>
\[\boxed{\min\frac{1}{2}||w||^2}\quad\quad\textrm{так что }\quad \boxed{y^{(i)}(w^Tx^{(i)}-b)\geqslant1}\]
</div>
<center>
  <img alt=SVM class=img-responsive netsrc=teaching/cs-229/illustrations/ src=svm-en.png?d23456fe589935f26cf32c1664c90851 style=width:100%;max-width:600px>
</center>
<p><span class=remark>Примечание: граница решения определяется как $\boxed{w^Tx-b=0}$.</span></p>
<br>
<p><span class="new-item item-b">Hinge loss</span> используется в качестве функции потерь SVM и определяются следующим образом:</p>
<div class=mobile-container>
\[\boxed{L(z,y)=[1-yz]_+=\max(0,1-yz)}\]
</div>
<br>
<p><span class="new-item item-b">Kernel</span> Учитывая отображение признаков $\phi$, мы определяем ядро $K$ как:</p>
<div class=mobile-container>
\[\boxed{K(x,z)=\phi(x)^T\phi(z)}\]
</div>
<p>На практике ядро $K$ определяется как $K(x,z)=\exp\left(-\frac{||x-z||^2}{2\sigma^2}\right)$ называется ядром Гаусса и чаще всего используется.</p>
<center>
  <img alt="SVM kernel" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=svm-kernel-en.png?43f2af419ba926948a5bbf3289f2cf39>
</center>
<br>
<p><span class=remark>Примечание: мы говорим, что используем "трюк с ядром" для вычисления функции стоимости с использованием ядра, потому что нам фактически не нужно знать явное отображение $\phi$, которое часто бывает очень сложным. Вместо этого нужны только значения $K(x,z)$.</span></p>
<br>
<p><span class="new-item item-r">Лагранжиан</span> Определим лагранжиан $\mathcal{L}(w,b)$ следующим образом:</p>
<div class=mobile-container>
\[\boxed{\mathcal{L}(w,b)=f(w)+\sum_{i=1}^l\beta_ih_i(w)}\]
</div>
<p><span class=remark>Примечание: коэффициенты $\beta_i$ называются множителями Лагранжа.</span></p>
<br>
<h2><a aria-hidden=true class=anchor href=#generative-learning id=generative-learning></a>Генеративное обучение</h2>
<p>Генеративная модель сначала пытается узнать, как генерируются данные, оценивая $P(x|y)$, которое мы затем можем использовать для оценки $P(y|x)$ с помощью правила Байеса.</p>
<h3>Гауссовский дискриминантный анализ</h3>
<p><span class="new-item item-b">Настройка</span> Гауссовский дискриминантный анализ предполагает, что $y$ и $x|y=0$ и $x|y=1$ таковы, что:</p>
<div class=row>
 <div class=col-sm-4>$(1)\quad\boxed{y\sim\textrm{Bernoulli}(\phi)}$</div>
 <div class=col-sm-4>$(2)\quad\boxed{x|y=0\sim\mathcal{N}(\mu_0,\Sigma)}$</div>
 <div class=col-sm-4>$(3)\quad\boxed{x|y=1\sim\mathcal{N}(\mu_1,\Sigma)}$</div>
</div>
<br>
<p><span class="new-item item-b">Оценка</span> В следующей таблице суммированы оценки, которые мы находим при максимизации правдоподобия:</p>
<div class=mobile-container>
<center>
<table>
<tbody>
<tr>
<td align=center><b>$\widehat{\phi}$</b></td>
<td align=right><b>$\widehat{\mu_j}\quad{\small(j=0,1)}$</b></td>
<td align=center><b>$\widehat{\Sigma}$</b></td>
</tr>
<tr>
<td align=center>$\displaystyle\frac{1}{m}\sum_{i=1}^m1_{\{y^{(i)}=1\}}$</td>
<td align=center>$\displaystyle\frac{\sum_{i=1}^m1_{\{y^{(i)}=j\}}x^{(i)}}{\sum_{i=1}^m1_{\{y^{(i)}=j\}}}$</td>
<td align=center>$\displaystyle\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T$</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<h3>Наивный Байес</h3>
<p><span class="new-item item-b">Допущение</span> Наивная байесовская модель предполагает, что все характеристики каждой точки данных независимы:</p>
<div class=mobile-container>
\[\boxed{P(x|y)=P(x_1,x_2,...|y)=P(x_1|y)P(x_2|y)...=\prod_{i=1}^nP(x_i|y)}\]
</div>
<br>
<p><span class="new-item item-r">Решения</span> Максимизация логарифмического правдоподобия дает следующие решения:
</p><div class=mobile-container>
\[\boxed{P(y=k)=\frac{1}{m}\times\#\{j|y^{(j)}=k\}}\quad\textrm{ and }\quad\boxed{P(x_i=l|y=k)=\frac{\#\{j|y^{(j)}=k\textrm{ and }x_i^{(j)}=l\}}{\#\{j|y^{(j)}=k\}}}\]
</div>
с $k\in\{0,1\}$ and $l\in[\![1,L]\!]$<p></p>
<p><span class=remark>Примечание: Наивный байесовский метод широко используется для классификации текста и обнаружения спама.</span></p>
<br>
<h2><a aria-hidden=true class=anchor href=#tree id=tree></a>Древовидные и ансамблевые методы</h2>
<p>Эти методы можно использовать как для регрессионных, так и для классификационных задач.</p>
<p><span class="new-item item-b">Classification and Regression Trees (CART)</span> Деревья классификации и регрессии, широко известные как деревья решений, могут быть представлены как двоичные деревья. Их преимущество в том, что они легко интерпретируемы.</p>
<br>
<p><span class="new-item item-b">Случайный лес</span> это метод, основанный на деревьях, который использует большое количество деревьев решений, построенных из случайно выбранных наборов функций. В отличие от простого дерева решений, его трудно интерпретировать, но в целом хорошая производительность делает его популярным алгоритмом.</p>
<p><span class=remark>Примечание: случайные леса - это разновидность ансамблевых методов.</span></p>
<br>
<p><span class="new-item item-b">Boosting (усиление)</span> идея методов усиления заключается в объединении нескольких слабых учеников, чтобы сформировать более сильного. Основные из них приведены в таблице ниже:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:760px;">
<colgroup>
<col style=width:50%>
<col style=width:50%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Адаптивный бустинг</b></td>
<td align=center><b>Градиентный бустинг</b></td>
</tr>
<tr>
<td align=left>• Ошибкам уделяется большое внимание, чтобы их можно было исправить на следующем этапе усиления<br>
• Известный как Adaboost</td>
<td align=left>• Слабые ученики обучаются на разнице<br>
• Примеры включают XGBoost</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#other id=other></a>Другие непараметрические подходы</h2>
<p><span class="new-item item-b">$k$-ближайшие соседи</span> Алгоритм $k$-ближайших соседей, широко известный как $k$-NN, представляет собой непараметрический подход, в котором ответ точки данных определяется природой её $k$ соседей из обучающего набора. Его можно использовать как для классификации, так и для настройки регрессии.</p>
<p><span class=remark>Примечание: Чем выше параметр $k$, тем выше смещение, а чем ниже параметр $k$, тем выше дисперсия.</span></p>
<div class=mobile-container>
<center>
  <img alt="k nearest neighbors" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=k-nearest-neighbors.png?02f80a524bb11e2b7a70b58c9ed3b0f4 style=width:100%;max-width:740px>
</center>
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#learning-theory id=learning-theory></a>Теория вычислительного обучения</h2>
<p><span class="new-item item-r">Связанный союз</span> Пусть $A_1, ..., A_k$ есть $k$ событий. Мы имеем:</p>
<div class=mobile-container>
\[\boxed{P(A_1\cup ...\cup A_k)\leqslant P(A_1)+...+P(A_k)}\]
</div>
<center>
<img alt="Union bound" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=union-bound.png?aab917859fa8e260e865def69a2889b8 style=width:100%;max-width:700px>
</center>
<br>
<p><span class="new-item item-r">Неравенство Хёфдинга</span> Пусть $Z_1, .., Z_m$ - $m$ независимых и одинаково распределенных переменных, взятых из распределения Бернулли для параметра $\phi$. Пусть $\widehat{\phi}$ их выборочное среднее, а $\gamma&gt;0$ фиксированное. У нас есть:</p>
<div class=mobile-container>
\[\boxed{P(|\phi-\widehat{\phi}|&gt;\gamma)\leqslant2\exp(-2\gamma^2m)}\]
</div>
<p><span class=remark>Примечание: это неравенство также известно как граница Чернова.</span></p>
<br>
<p><span class="new-item item-g">Ошибка тренировки</span> Для данного классификатора $h$ мы определяем ошибку обучения $\widehat{\epsilon}(h)$, также известную как эмпирический риск или эмпирическая ошибка, следующим образом:</p>
<div class=mobile-container>
\[\boxed{\widehat{\epsilon}(h)=\frac{1}{m}\sum_{i=1}^m1_{\{h(x^{(i)})\neq y^{(i)}\}}}\]
</div>
<br>
<p><span class="new-item item-g">Probably Approximately Correct (PAC)</span> Вероятно приближённо корректное обучение - эта схема, в рамках которой были доказаны многочисленные результаты по теории обучения, и имеет следующий набор предположений:</p>
<ul>
	<li>наборы для обучения и тестирования имеют одинаковое распределение</li>
	<li>обучающие примеры строятся независимо</li>
</ul>
<br>
<p><span class="new-item item-g">Разрушение (Shattering)</span> Для набора $S=\{x^{(1)},...,x^{(d)}\}$ и набора классификаторов $\mathcal{H}$ мы говорим, что $\mathcal{H}$ разрушает $S$, если для любого набора меток $\{y^{(1)}, ..., y^{(d)}\}$, у нас есть:</p>
<div class=mobile-container>
\[\boxed{\exists h\in\mathcal{H}, \quad \forall i\in[\![1,d]\!],\quad h(x^{(i)})=y^{(i)}}\]
</div>
<br>
<p><span class="new-item item-r">Теорема о верхней оценке</span> Пусть $\mathcal{H}$ - конечный класс гипотез, такой что $|\mathcal{H}|=k$, и пусть $\delta$ и размер выборки $m$ фиксированы. Тогда с вероятностью не менее $1-\delta$, у нас есть:</p>
<div class=mobile-container>
\[\boxed{\epsilon(\widehat{h})\leqslant\left(\min_{h\in\mathcal{H}}\epsilon(h)\right)+2\sqrt{\frac{1}{2m}\log\left(\frac{2k}{\delta}\right)}}\]
</div>
<br>
<p><span class="new-item item-g">VC размерность</span> Размерность Вапника-Червоненкиса (VC) данного бесконечного класса гипотез $\mathcal{H}$, обозначается как $\textrm{VC}(\mathcal{H})$, - это размер самого большого множества, которое разрушается $\mathcal{H}$.</p>
<p><span class=remark>Примечание: VC размерность ${\small\mathcal{H}=\{\textrm{набор линейных классификаторов в 2-х измерениях}\}}$ это 3.</span></p>
<center>
  <img alt="VC dimension" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=vc-dimension.png?73859dedcc66a0e47526936f801b7b56>
</center>
<br>
<p><span class="new-item item-r">Теорема (Вапника)</span> Пусть задано $\mathcal{H}$, $\textrm{VC}(\mathcal{H})=d$ и $m$ - количество обучающих примеров. По крайней мере, с вероятностью $1-\delta$, у нас есть:</p>
<div class=mobile-container>
\[\boxed{\epsilon(\widehat{h})\leqslant \left(\min_{h\in\mathcal{H}}\epsilon(h)\right) + O\left(\sqrt{\frac{d}{m}\log\left(\frac{m}{d}\right)+\frac{1}{m}\log\left(\frac{1}{\delta}\right)}\right)}\]
</div>
</article> </div> <!-- FOOTER <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-twitter fa-3x fa-fw"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class="fa fa-linkedin fa-3x fa-fw"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-github fa-3x fa-fw"></i></a> <a href="https://scholar.google.com/citations?user=nMnMTm8AAAAJ" onclick=trackOutboundLink(this);><i class="fa fa-google fa-3x fa-fw"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick="trackOutboundLink(this); window.location.href = 'mailto:' + this.dataset.name + '@' + this.dataset.domain + '.' + this.dataset.tld"><i class="fa fa-envelope fa-3x fa-fw"></i></a> </div> </div> </footer> --> </body></html>
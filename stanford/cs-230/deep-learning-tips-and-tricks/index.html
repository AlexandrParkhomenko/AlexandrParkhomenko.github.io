<!DOCTYPE html><html lang=en><head><!-- base href=../../ --><title>CS 230 - Deep Learning Tips and Tricks Cheatsheet</title><meta charset=utf-8><meta content="Teaching page of Shervine Amidi, Graduate Student at Stanford University." name=description><meta content="teaching, shervine, shervine amidi, data science" name=keywords><meta content="width=device-width, initial-scale=1" name=viewport>
<link href=https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks rel=canonical>
<link href=../../css/bootstrap.min.css rel=stylesheet>
<link href=../../css/font-awesome.min.css rel=stylesheet>
<link href=../../css/katex.min.css rel=stylesheet>
<link href=../../css/style.min.css rel=stylesheet type=text/css>
<link href=../../css/article.min.css rel=stylesheet>
<script src=../../js/jquery.min.js></script>
<script src=../../js/bootstrap.min.js></script>
<script defer src=../../js/underscore-min.js type=text/javascript></script>
<script defer src=../../js/katex.min.js></script>
<script defer src=../../js/auto-render.min.js></script>
<script defer src=../../js/article.min.js></script>
<script defer src=../../js/lang.min.js></script>
<script async defer src=../../js/buttons.js></script>
</head> <body data-offset=50 data-spy=scroll data-target=.navbar> <!-- HEADER <nav class="navbar navbar-inverse navbar-static-top"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class="collapse navbar-collapse" id=myNavbar> <ul class="nav navbar-nav"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class="nav navbar-nav navbar-center"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class="collapse navbar-collapse" data-target=None id=HiddenNavbar> <ul class="nav navbar-nav navbar-right"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style="padding: 0px;"> <img alt=MIT netsrc=images/ src=../../images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style="padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;"> </a> </ul> </div> </div> </div> </nav> --> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title style=display:none;> <!-- DISPLAY:NONE --> <a href=teaching/cs-230 onclick=trackOutboundLink(this);><img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48 style="width: 15px;">   <b>CS 230 - Deep Learning</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>Tips and tricks</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#data-processing>Data processing</a></div> <div class=dropdown-container> <a href=#data-processing><span>Data augmentation</span></a> <a href=#data-processing><span>Batch normalization</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#running-nn>Training a neural network</a></div> <div class=dropdown-container> <a href=#running-nn><span>Epoch</span></a> <a href=#running-nn><span>Mini-batch</span></a> <a href=#running-nn><span>Cross-entropy loss</span></a> <a href=#running-nn><span>Backpropagation</span></a> <a href=#running-nn><span>Gradient descent</span></a> <a href=#running-nn><span>Updating weights</span></a> <a href=#running-nn><span>Gradient checking</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#parameter-tuning>Parameter tuning</a></div> <div class=dropdown-container> <a href=#parameter-tuning><span>Xavier initialization</span></a> <a href=#parameter-tuning><span>Transfer learning</span></a> <a href=#parameter-tuning><span>Learning rate</span></a> <a href=#parameter-tuning><span>Adaptive learning rates</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#regularization>Regularization</a></div> <div class=dropdown-container> <a href=#regularization><span>Dropout</span></a> <a href=#regularization><span>Weight regularization</span></a> <a href=#regularization><span>Early stopping</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#good-practices>Good practices</a></div> <div class=dropdown-container> <a href=#good-practices><span>Overfitting small batch</span></a> <a href=#good-practices><span>Gradient checking</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/en/cheatsheet-deep-learning-tips-tricks.pdf onclick=trackOutboundLink(this); style="color: white; text-decoration:none;"> <i aria-hidden=false class="fa fa-github fa-fw"></i> View PDF version on GitHub </a> </li> </div> </center> </div> <article class="markdown-body entry-content" itemprop=text>
<div class="alert alert-primary" role=alert style=display:none;> <!-- DISPLAY:NONE -->
  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!
</div>
<div class=title-lang style=display:none;> <!-- DISPLAY:NONE --><a aria-hidden=true class=anchor-bis href=#cs-230---deep-learning id=cs-230---deep-learning></a><a href=teaching/cs-230 onclick=trackOutboundLink(this);>CS 230 - Deep Learning</a>
  
  <div style=float:right;display:none;> <!-- DISPLAY:NONE -->
    <div class=input-group>
      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>
        <option selected value=en>English</option>
        <option value=fa>فارسی</option>
        <option value=fr>Français</option>
        <option value=ja>日本語</option>
        <option value=ko>한국어</option>
        <option value=tr>Türkçe</option>
        <option value=vi>Tiếng Việt</option>
      </select>
      <div class=input-group-addon><i class=fa></i></div>
    </div>
  </div>
</div>
<br>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button>CS 221 - Artificial Intelligence</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button>CS 229 - Machine Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button><B>CS 230 - Deep Learning</B></BUTTON>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button>Convolutional Neural Networks</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/recurrent-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-recurrent-neural-networks'" type=button>Recurrent Neural Networks</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-230/deep-learning-tips-and-tricks/index.html'; oldhref='teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks'" type=button><B>Tips and tricks</B></BUTTON>
  </div>
</div>

<h1>
  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>Deep Learning Tips and Tricks cheatsheet
  <div style=float:right;display:none;> <!-- DISPLAY:NONE --><a aria-label="Star afshinea/stanford-cs-230-deep-learning on GitHub" class="github-button fa-fw" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-230-deep-learning onclick=trackOutboundLink(this);>Star</a></div>
</h1>
<i>By <a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> and <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a></i>
<h2><a aria-hidden=true class=anchor href=#data-processing id=data-processing></a>Data processing</h2>
<p><span class="new-item item-r">Увеличение данных (augmentation)</span> Для правильного обучения моделям глубокого обучения обычно требуется много данных. Часто бывает полезно получить больше данных из существующих, используя методы увеличения данных. Основные из них приведены в таблице ниже. Точнее, с учетом следующего входного изображения, вот методы, которые мы можем применить:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:500px;">
<colgroup>
<col style=width:25%>
<col style=width:25%>
<col style=width:25%>
<col style=width:25%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Original</b></td>
<td align=center><b>Flip</b></td>
<td align=center><b>Rotation</b></td>
<td align=center><b>Random crop</b></td>
</tr>
<tr>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=augmentation-original.jpg?fa9abcd4dce13c776d7d10b3a1ec8fad></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=augmentation-flip.jpg?da4ff42b5d8fc1f2759675e0c6323f8e></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=augmentation-rotation.jpg?ab180edcbeb006bfbad4fd57aa6433bf></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=augmentation-crop.jpg?811ab0236837a5eb3106e761b6046424></td>
</tr>
<tr>
<td align=left valign=top>• Image without any modification</td>
<td align=left valign=top>• Flipped with respect to an axis for which the meaning of the image is preserved</td>
<td align=left valign=top>• Rotation with a slight angle<br>
                 • Simulates incorrect horizon calibration</td>
<td align=left valign=top>• Random focus on one part of the image<br>
                              • Several random crops can be done in a row</td>
</tr>
</tbody>
</table>
</center>
</div>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:500px;">
<colgroup>
<col style=width:25%>
<col style=width:25%>
<col style=width:25%>
<col style=width:25%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Color shift</b></td>
<td align=center><b>Noise addition</b></td>
<td align=center><b>Information loss</b></td>
<td align=center><b>Contrast change</b></td>
</tr>
<tr>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=augmentation-shift.jpg?fca805477d241c88166819240ab65ba4></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=augmentation-noise.jpg?f56920da7d2d08d1041f1b01b92df1f8></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=augmentation-information-loss.jpg?8563bbe135ffcbe57b9b8529dd80fcb9></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=augmentation-contrast.jpg?776cd26f62f336949ee237817f6af3a5></td>
</tr>
<tr>
<td align=left valign=top>• Nuances of RGB is slightly changed<br>
                 • Captures noise that can occur with light exposure
</td>
<td align=left valign=top>• Addition of noise<br>
                 • More tolerance to quality variation of inputs
</td>
<td align=left valign=top>• Parts of image ignored<br>
                              • Mimics potential loss of parts of image</td>
<td align=left valign=top>• Luminosity changes<br>
                              • Controls difference in exposition due to time of day</td>
</tr>
</tbody>
</table>
</center>
</div>
<p><span class=remark>Remark: data is usually augmented on the fly during training.</span></p>
<br>
<p><span class="new-item item-b">Batch normalization</span> It is a step of hyperparameter $\gamma, \beta$ that normalizes the batch $\{x_i\}$. By noting $\mu_B, \sigma_B^2$ the mean and variance of that we want to correct to the batch, it is done as follows:
</p><div class=mobile-container>
\[\boxed{x_i\longleftarrow\gamma\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}+\beta}\]
</div>
It is usually done after a fully connected/convolutional layer and before a non-linearity layer and aims at allowing higher learning rates and reducing the strong dependence on initialization.<p></p>
<br>
<h2><a aria-hidden=true class=anchor href=#running-nn id=running-nn></a>Training a neural network</h2>
<h3>Definitions</h3>
<p><span class="new-item item-b">Эпоха</span> в контексте обучения модели эпоха - это термин, используемый для обозначения одной итерации, когда модель видит весь обучающий набор для обновления своих весов.</p>
<br>
<p><span class="new-item item-r">Мини-пакетный градиентный спуск</span> на этапе обучения обновление весов обычно не основывается на всем обучающем наборе сразу из-за сложности вычислений или одной точки данных из-за проблем с шумом. Вместо этого этап обновления выполняется для мини-пакетов, где количество точек данных в пакете является гиперпараметром, который мы можем настроить.</p>
<br>
<p><span class="new-item item-b">Loss function</span> In order to quantify how a given model performs, the loss function $L$ is usually used to evaluate to what extent the actual outputs $y$ are correctly predicted by the model outputs $z$.</p>
<br>
<p><span class="new-item item-b">Cross-entropy loss</span> In the context of binary classification in neural networks, the cross-entropy loss $L(z,y)$ is commonly used and is defined as follows:</p>
<div class=mobile-container>
\[\boxed{L(z,y)=-\Big[y\log(z)+(1-y)\log(1-z)\Big]}\]
</div>
<br>
<h3>Finding optimal weights</h3>
<p><span class="new-item item-b">Backpropagation</span> Backpropagation is a method to update the weights in the neural network by taking into account the actual output and the desired output. The derivative with respect to each weight $w$ is computed using the chain rule.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=backpropagation-ltr.png?7ab0719a4825260ef208d87cdd195b9f style="width:100%; max-width: 500px;">
</center>
</div>
<br>
<p>Using this method, each weight is updated with the rule:</p>
<div class=mobile-container>
\[\boxed{w\longleftarrow w-\alpha\frac{\partial L(z,y)}{\partial w}}\]
</div>
<br>
<p><span class="new-item item-g">Обновление весов</span> В нейронной сети веса обновляются следующим образом:</p>
<p>• Step 1: Take a batch of training data and perform forward propagation to compute the loss.
<br>• Step 2: Backpropagate the loss to get the gradient of the loss with respect to each weight.
<br>• Step 3: Use the gradients to update the weights of the network.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=update-weights-en.png?5394a7bb976892418366d08646a0fd09 style="width:100%; min-width: 500px;">
</center>
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#parameter-tuning id=parameter-tuning></a>Parameter tuning</h2>
<h3>Weights initialization</h3>
<p><span class="new-item item-b">Метод инициализации Завьера (Xavier)</span> вместо того, чтобы инициализировать веса чисто случайным образом, инициализация Xavier позволяет иметь начальные веса, которые учитывают характеристики, уникальные для архитектуры.</p>
<br>
<p><span class="new-item item-r">Трансферное обучение</span> для обучения модели глубокого обучения требуется много данных и, что ещё более важно, много времени. Часто бывает полезно воспользоваться предварительно обученными весами для огромных наборов данных, обучение которых занимало дни/недели, и использовать их в нашем случае использования. В зависимости от того, сколько данных у нас под рукой, есть несколько способов использовать это:</p>
<div class=mobile-container>
<center>
<table>
<colgroup><col width=18%>
<col width=52%>
<col width=30%>
</colgroup><tbody>
<tr>
<td align=center><b>Training size</b></td>
<td align=center><b>Illustration</b></td>
<td align=center><b>Explanation</b></td>
</tr>
<tr>
<td align=center>Small</td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=transfer-learning-small-ltr.png?bee5e73de8fb2c6297a3a88804fabf5e></td>
<td align=left>Freezes all layers, trains weights on softmax</td>
</tr>
<tr>
<td align=center>Medium</td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=transfer-learning-medium-ltr.png?650eae780cff79a9ff0123d62e5812ad></td>
<td align=left>Freezes most layers, trains weights on last layers and softmax</td>
</tr>
<tr>
<td align=center>Large</td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=transfer-learning-large-ltr.png?ccdc27259583a57319e0bc39f5d858df></td>
<td align=left>Trains weights on layers and softmax by initializing weights on pre-trained ones</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<h3>Optimizing convergence</h3>
<p><span class="new-item item-b">Learning rate</span> The learning rate, often noted $\alpha$ or sometimes $\eta$, indicates at which pace the weights get updated. It can be fixed or adaptively changed. The current most popular method is called Adam, which is a method that adapts the learning rate.</p>
<br>
<p><span class="new-item item-g">Адаптивная скорость обучения</span> Изменение скорости обучения при обучении модели может сократить время обучения и улучшить численное оптимальное решение. Хотя оптимизатор Адама является наиболее часто используемым методом, другие также могут быть полезны. Они приведены в таблице ниже:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:660px;">
<colgroup>
<col style=width:110px>
<col style=width:33%>
<col style=width:33%>
<col style=width:33%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Method</b></td>
<td align=center><b>Explanation</b></td>
<td align=center><b>Update of $w$</b></td>
<td align=center><b>Update of $b$</b></td>
</tr>
<tr>
<td align=center>Momentum</td>
<td align=left>• Dampens oscillations<br>
                 • Improvement to SGD<br>
                 • 2 parameters to tune</td>
<td align=center>$\displaystyle w-\alpha v_{dw}$</td>
<td align=center>$\displaystyle b-\alpha v_{db}$</td>
</tr>
<tr>
<td align=center>RMSprop</td>
<td align=left>• Root Mean Square propagation<br>
                 • Speeds up learning algorithm by controlling oscillations</td>
<td align=center>$\displaystyle w-\alpha\frac{dw}{\sqrt{s_{dw}}}$</td>
<td align=center>$\displaystyle b\longleftarrow b-\alpha\frac{db}{\sqrt{s_{db}}}$</td>
</tr>
<tr>
<td align=center>Adam</td>
<td align=left>• Adaptive Moment estimation<br>
                 • Most popular method<br>
                 • 4 parameters to tune</td>
<td align=center>$\displaystyle w-\alpha\frac{v_{dw}}{\sqrt{s_{dw}}+\epsilon}$</td>
<td align=center>$\displaystyle b\longleftarrow b-\alpha\frac{v_{db}}{\sqrt{s_{db}}+\epsilon}$</td>
</tr>
</tbody>
</table>
</center>
</div>
<p><span class=remark>Remark: other methods include Adadelta, Adagrad and SGD.</span></p>
<br>
<h2><a aria-hidden=true class=anchor href=#regularization id=regularization></a>Regularization</h2>
<p><span class="new-item item-r">Dropout</span> Dropout is a technique used in neural networks to prevent overfitting the training data by dropping out neurons with probability $p &gt;0$. It forces the model to avoid relying too much on particular sets of features.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=dropout-ltr.png?27ab877c24a915d22e598fb772fcdc96 style="width:100%; max-width: 500px;">
</center>
</div>
<p><span class=remark>Remark: most deep learning frameworks parametrize dropout through the 'keep' parameter $1-p$.</span></p>
<br>
<p><span class="new-item item-r">Регуляризация веса</span> Чтобы убедиться, что веса не слишком велики и что модель не переобучается на обучающей выборке, обычно выполняются методы регуляризации для весов модели. Основные из них приведены в таблице ниже:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:760px;">
  <colgroup>
    <col style=width:33%>
    <col style=width:33%>
    <col style=width:33%>
  </colgroup>
<tbody>
<tr>
<td align=center><b>LASSO</b></td>
<td align=center><b>Ridge</b></td>
<td align=center><b>Elastic Net</b></td>
</tr>
<tr>
<td align=left>• Shrinks coefficients to 0<br>• Good for variable selection</td>
<td align=left>Makes coefficients smaller</td>
<td align=left>Tradeoff between variable selection and small coefficients</td>
</tr>
<tr>
<td align=center><img alt=Lasso class=img-responsive netsrc=teaching/cs-229/illustrations/ src=lasso.png?ad67282f00fc8b2a529e5b15a856f91b></td>
<td align=center><img alt=Ridge class=img-responsive netsrc=teaching/cs-229/illustrations/ src=ridge.png?77abafe4253433af93fb8ffc7d4f6bc7></td>
<td align=center><img alt="Elastic Net" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=elastic-net.png?8cd93eb9df1b6ae667d8eb69d20bf4a1></td>
</tr>
<tr>
<td align=left>$...+\lambda||\theta||_1$<br>$\lambda\in\mathbb{R}$</td>
<td align=left>$...+\lambda||\theta||_2^2$<br>$\lambda\in\mathbb{R}$</td>
<td align=left>$...+\lambda\Big[(1-\alpha)||\theta||_1+\alpha||\theta||_2^2\Big]$<br>$\lambda\in\mathbb{R},\alpha\in[0,1]$</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-g">Ранняя остановка</span> Этот метод регуляризации останавливает процесс обучения, как только потеря валидации достигает плато или начинает увеличиваться.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=early-stopping-en.png?a8aacdfe0c39776d86764857222e19dd style="width:100%; max-width: 500px;">
</center>
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#good-practices id=good-practices></a>Good practices</h2>
<p><span class="new-item item-g">Переобучение на небольших пакетах</span> При отладке модели часто бывает полезно провести быстрые тесты, чтобы увидеть, есть ли какие-либо серьезные проблемы с архитектурой самой модели. В частности, чтобы убедиться, что модель может быть должным образом обучена, внутри сети передается мини-пакет, чтобы увидеть, может ли он переобучиться. Если это невозможно, это означает, что модель либо слишком сложна, либо недостаточно сложна, чтобы даже переобучиться на небольшой партии, не говоря уже об обучающем наборе нормального размера.</p>
<br>
<p><span class="new-item item-r">Проверка градиента</span> это метод, используемый во время реализации обратного прохода нейронной сети. Он сравнивает значение аналитического градиента с числовым градиентом в заданных точках и играет роль проверки правильности.</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:650px; max-width:900px;">
<colgroup>
<col style=width:110px>
<col style=width:50%>
<col style=width:50%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Type</b></td>
<td align=center><b>Numerical gradient</b></td>
<td align=center><b>Analytical gradient</b></td>
</tr>
<tr>
<td align=center>Formula</td>
<td align=center>$\displaystyle\frac{df}{dx}(x) \approx \frac{f(x+h) - f(x-h)}{2h}$</td>
<td align=center>$\displaystyle\frac{df}{dx}(x) = f'(x)$</td>
</tr>
<tr>
<td align=center>Comments</td>
<td align=left valign=top>• Expensive; loss has to be computed two times per dimension<br>
                              • Used to verify correctness of analytical implementation<br>
                              • Trade-off in choosing $h$ not too small (numerical instability) nor too large (poor gradient approximation)</td>
<td align=left valign=top>• 'Exact' result<br>
                              • Direct computation<br>
                              • Used in the final implementation</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
</article> </div> <!-- FOOTER <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-twitter fa-3x fa-fw"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class="fa fa-linkedin fa-3x fa-fw"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-github fa-3x fa-fw"></i></a> <a href="https://scholar.google.com/citations?user=nMnMTm8AAAAJ" onclick=trackOutboundLink(this);><i class="fa fa-google fa-3x fa-fw"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick="trackOutboundLink(this); window.location.href = 'mailto:' + this.dataset.name + '@' + this.dataset.domain + '.' + this.dataset.tld"><i class="fa fa-envelope fa-3x fa-fw"></i></a> </div> </div> </footer> --> </body></html>
<!DOCTYPE html><html lang=en><head><!-- base href=../../ --><title>CS 221 - States-based Models Cheatsheet</title><meta charset=utf-8><meta content="Teaching page of Shervine Amidi, Graduate Student at Stanford University." name=description><meta content="teaching, shervine, shervine amidi, data science" name=keywords><meta content="width=device-width, initial-scale=1" name=viewport>
<link href=https://stanford.edu/~shervine/teaching/cs-221/cheatsheet-states-models rel=canonical>
<link href=../../css/bootstrap.min.css rel=stylesheet>
<link href=../../css/font-awesome.min.css rel=stylesheet>
<link href=../../css/katex.min.css rel=stylesheet>
<link href=../../css/style.min.css rel=stylesheet type=text/css>
<link href=../../css/article.min.css rel=stylesheet>
<script src=../../js/jquery.min.js></script>
<script src=../../js/bootstrap.min.js></script>
<script defer src=../../js/underscore-min.js type=text/javascript></script>
<script defer src=../../js/katex.min.js></script>
<script defer src=../../js/auto-render.min.js></script>
<script defer src=../../js/article.min.js></script>
<script defer src=../../js/lang.min.js></script>
<script async defer src=../../js/buttons.js></script>
</head> <body data-offset=50 data-spy=scroll data-target=.navbar> <!-- HEADER <nav class="navbar navbar-inverse navbar-static-top"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class="collapse navbar-collapse" id=myNavbar> <ul class="nav navbar-nav"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class="nav navbar-nav navbar-center"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class="collapse navbar-collapse" data-target=None id=HiddenNavbar> <ul class="nav navbar-nav navbar-right"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style="padding: 0px;"> <img alt=MIT netsrc=images/ src=../../images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style="padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;"> </a> </ul> </div> </div> </div> </nav> --> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title style=display:none;> <!-- DISPLAY:NONE --> <a href=teaching/cs-221 onclick=trackOutboundLink(this);><img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48 style="width: 12px;">   <b>CS 221 - Artificial Intelligence</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>States-based models</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#tree-search>Поиск по дереву</a></div> <div class=dropdown-container> <a href=#tree-search><span>Поиск с возвратом</span></a> <a href=#tree-search><span>Поиск в ширину</span></a> <a href=#tree-search><span>Поиск в глубину</span></a> <a href=#tree-search><span>С итеративным углублением</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#graph-search>Поиск по графу</a></div> <div class=dropdown-container> <a href=#graph-search><span>Динамическое программирование</span></a> <a href=#graph-search><span>Поиск по единой стоимости</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#learning-costs>Стоимость обучения</a></div> <div class=dropdown-container> <a href=#learning-costs><span>Структурированный перцептрон</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#a-star>Алгоритм поиска A*</a></div> <div class=dropdown-container> <a href=#a-star><span>Эвристическая функция</span></a> <a href=#a-star><span>Алгоритм</span></a> <a href=#a-star><span>Consistency, correctness</span></a> <a href=#a-star><span>Admissibility, efficiency</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#relaxation>Ослабление ограничений</a></div> <div class=dropdown-container> <a href=#relaxation><span>Задача с ослабленными ограничениями поиска</span></a> <a href=#relaxation><span>Ослабленная эвристика</span></a> <a href=#relaxation><span>Max эвристика</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#markov-decision-processes>Марковские процессы принятия решений</a></div> <div class=dropdown-container> <a href=#markov-decision-processes><span>Обзор</span></a> <a href=#markov-decision-processes><span>Оценка политики</span></a> <a href=#markov-decision-processes><span>Итерация ценности</span></a> <a href=#markov-decision-processes><span>Transitions, rewards</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#game-playing>Игровой процесс</a></div> <div class=dropdown-container> <a href=#game-playing><span>Expectimax</span></a> <a href=#game-playing><span>Minimax</span></a> <a href=#game-playing><span>Ускорение minimax</span></a> <a href=#game-playing><span>Одновременные игры</span></a> <a href=#game-playing><span>Игры с ненулевой суммой</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-221-artificial-intelligence/blob/master/en/cheatsheet-states-models.pdf onclick=trackOutboundLink(this); style="color: white; text-decoration:none;"> <i aria-hidden=false class="fa fa-github fa-fw"></i> Посмотреть PDF-версию на GitHub </a> </li> </div> </center> </div> <article class="markdown-body entry-content" itemprop=text>
<div class="alert alert-primary" role=alert style=display:none;> <!-- DISPLAY:NONE -->
  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!
</div>
<div class=title-lang style=display:none;> <!-- DISPLAY:NONE -->
  <a aria-hidden=true class=anchor-bis href=#cs-221---artificial-intelligence id=cs-221---artificial-intelligence></a><a href=teaching/cs-221 onclick=trackOutboundLink(this);>CS 221 - Artificial Intelligence</a>
  
  <div style=float:right;display:none;> <!-- DISPLAY:NONE -->
    <div class=input-group>
      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>
        <option selected value=en>English</option>
        <option value=fr>Français</option>
        <option value=tr>Türkçe</option>
      </select>
      <div class=input-group-addon><i class=fa></i></div>
    </div>
  </div>
</div>
<br>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button><B>CS 221 - Artificial Intelligence</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button>CS 229 - Machine Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button>CS 230 - Deep Learning</button>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button>Reflex</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-221/states-models/index.html'; oldhref='teaching/cs-221/cheatsheet-states-models'" type=button><B>States</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/variables-models/index.html'; oldhref='teaching/cs-221/cheatsheet-variables-models'" type=button>Variables</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/logic-models/index.html'; oldhref='teaching/cs-221/cheatsheet-logic-models'" type=button>Logic</button>
  </div>
</div>
<h1>
  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>Модели на основе состояний с оптимизацией поиска и Марковским процессом принятия решений (MDP)
</h1>
<i><!-- By --><a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> и <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a>;<a href=https://github.com/AlexandrParkhomenko onclick=trackOutboundLink(this);> Alexandr Parkhomenko</a> и <a href=https://github.com/geotrush onclick=trackOutboundLink(this);>Труш Георгий (Georgy Trush)</a></i>
  <div style=float:right;display:none;> <!-- DISPLAY:NONE --><a aria-label="Star afshinea/stanford-cs-221-artificial-intelligence on GitHub" class="github-button fa-fw" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-221-artificial-intelligence onclick=trackOutboundLink(this);>Star</a></div>
<h2>Оптимизация поиска</h2>
<p>В этом разделе мы предполагаем, что, выполняя действие $a$ из состояния $s$, мы детерминированно приходим в состояние $\textrm{Succ}(s,a)$. Здесь цель - определить последовательность действий $(a_1,a_2,a_3,a_4,...)$ которая начинается из начального состояния и приводит к конечному состоянию. Для решения задачи нам надо найти путь с минимальными затратами, с помощью моделей на основе состояний.</p>
<h3><a aria-hidden=true class=anchor href=#tree-search id=ts></a>Поиск по дереву</h3>
<p>Эта категория алгоритмов на основе состояний исследует все возможные состояния и действия. Он довольно эффективен с точки зрения памяти и подходит для огромных пространств состояний, но в худших случаях время выполнения может стать экспоненциальным.</p>
<div class=mobile-container>
<center>
  <img alt=Tree class=img-responsive netsrc=teaching/cs-221/illustrations/ src=tree.png?2d18a75bbffc54c7db56dd83aed1e224 style=width:100%;max-width:900px>
</center>
</div>
<br>
<p><span class="new-item item-g">Задача поиска</span> определяется с помощью:
</p><ul>
  <li>начальное состояние $s_{\textrm{start}}$
  </li><li>возможные действия $\textrm{Actions}(s)$ из состояния $s$
  </li><li>стоимость действия $\textrm{Cost}(s,a)$ из состояния $s$ с действием $a$
  </li><li>преемник $\textrm{Succ}(s,a)$ состояния $s$ после действия $a$
  </li><li>было ли достигнуто конечное состояние $\textrm{IsEnd}(s)$
</li></ul>
<p></p>
<div class=mobile-container>
<center>
  <img alt="Search problem" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=search-problem.png?5b6272e510ca6d44c87c01a68ccaca3a style=width:100%;max-width:465px>
</center>
</div>
<p>Цель состоит в том, чтобы найти путь, который минимизирует затраты.</p>
<br>
<p><span class="new-item item-r">Backtracking</span> Поиск с возвратом - это наивный рекурсивный алгоритм, который пробует все возможности, чтобы найти путь с минимальной стоимостью. Здесь затраты на действия могут быть как положительными, так и отрицательными.</p>
<br>
<p><span class="new-item item-b">Поиск в ширину (BFS)</span> Поиск в ширину - это алгоритм поиска по графу, который выполняет обход уровня за уровнем. Мы можем реализовать это итеративно с помощью очереди, в которой на каждом шаге хранятся будущие узлы, которые нужно посетить. Для этого алгоритма можно считать, что затраты на действия равны константе $c\geqslant0$.</p>
<div class=mobile-container>
<center>
  <img alt=BFS class=img-responsive netsrc=teaching/cs-221/illustrations/ src=bfs-stack.png?28d6deef49cde3d173b8dc167e28ad4a style=width:100%;max-width:420px>
</center>
</div>
<br>
<p><span class="new-item item-b">Поиск в глубину (DFS)</span> Поиск в глубину - это алгоритм поиска, который обходит граф, прослеживая каждый путь как можно глубже. Мы можем реализовать это рекурсивно или итеративно с помощью стека, в котором на каждом шаге хранятся будущие узлы, которые необходимо посетить. Для этого алгоритма предполагается, что затраты на действия равны 0.</p>
<div class=mobile-container>
<center>
  <img alt=DFS class=img-responsive netsrc=teaching/cs-221/illustrations/ src=dfs-stack.png?5616ba59bd361e621870e036829b95d9 style=width:100%;max-width:420px>
</center>
</div>
<br>
<p><span class="new-item item-b">С итеративным углублением</span> Поиск в глубину с итеративным углублением - это модификация алгоритма поиска в глубину, так что он останавливается после достижения определенной глубины, что гарантирует оптимальность, когда все затраты на действия равны. Здесь мы предполагаем, что затраты на действия равны константе $c\geqslant0$.</p>
<br>
<p><span class="new-item item-r">Сводка алгоритмов поиска по дереву</span> отметив $b$ как количество действий на состояние, $d$ - глубину решения и $D$ - максимальную глубину, мы имеем:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:500px;">
<colgroup>
<col style=width:35%>
<col style=width:21.75%>
<col style=width:21.75%>
<col style=width:21.75%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Алгоритм</b></td>
<td align=center><b>Стоимость действий</b></td>
<td align=center><b>Пространство</b></td>
<td align=center><b>Время</b></td>
</tr>
<tr>
<td align=center valign=top>Поиск с возвратом</td>
<td align=center valign=top>any</td>
<td align=center valign=top>$\mathcal{O}(D)$</td>
<td align=center valign=top>$\mathcal{O}(b^D)$</td>
</tr>
<tr>
<td align=center valign=top>Поиск в ширину</td>
<td align=center valign=top>$c\geqslant0$</td>
<td align=center valign=top>$\mathcal{O}(b^d)$</td>
<td align=center valign=top>$\mathcal{O}(b^d)$</td>
</tr>
<tr>
<td align=center valign=top>Поиск в глубину</td>
<td align=center valign=top>0</td>
<td align=center valign=top>$\mathcal{O}(D)$</td>
<td align=center valign=top>$\mathcal{O}(b^D)$</td>
</tr>
<tr>
<td align=center valign=top>Поиск в глубину с итеративным углублением</td>
<td align=center valign=top>$c\geqslant0$</td>
<td align=center valign=top>$\mathcal{O}(d)$</td>
<td align=center valign=top>$\mathcal{O}(b^d)$</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<h3><a aria-hidden=true class=anchor href=#graph-search id=graph-search></a>Поиск по графу</h3>
<p>Эта категория алгоритмов на основе состояний направлена на построение оптимальных путей, обеспечивающих экспоненциальную экономию. В этом разделе мы сосредоточимся на динамическом программировании и поиске по единой стоимости.</p>
<p><span class="new-item item-g">Граф</span> граф состоит из набора вершин $V$ (также называемых узлами) и набора ребер $E$ (также называемых связями).</p>
<div class=mobile-container>
<center>
  <img alt=Graph class=img-responsive netsrc=teaching/cs-221/illustrations/ src=graph.png?76351911a7eff56d9f7de999bd7997ab style=width:100%;max-width:450px>
</center>
</div>
<p><span class=remark>Примечание: граф называется ациклическим, когда нет цикла.</span></p>
<br>
<p><span class="new-item item-r">Состояние</span> это совокупность всех прошлых действий, достаточная для оптимального выбора будущих действий.</p>
<br>
<p><span class="new-item item-b">Динамическое программирование (DP)</span> это алгоритм поиска с возвратом и запоминанием (то есть частичные результаты сохраняются), цель которого - найти путь с минимальной стоимостью от состояния $s$ в конечное состояние $s_\textrm{end}$. Он потенциально может иметь экспоненциальную экономию по сравнению с классическими алгоритмами поиска по графу и имеет свойство работать только для ациклических графов. Для любого заданного состояния $s$ будущая стоимость рассчитывается следующим образом:</p>
<div class=mobile-container>
\[\boxed{\textrm{FutureCost}(s)=\left\{\begin{array}{lc}0 &amp; \textrm{if IsEnd}(s)\\\underset{a\in\textrm{Actions}(s)}{\textrm{min}}\big[\textrm{Cost}(s,a)+\textrm{FutureCost(Succ}(s,a))\big] &amp; \textrm{otherwise}\end{array}\right.}\]
</div>
<div class=mobile-container>
<center>
  <img alt="Dynamic Programming" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=dynamic-programming.png?6efc6c44c8c7bce9c2c576ecb82ec98c style=width:100%;max-width:450px>
</center>
</div>
<p><span class=remark>Примечание: на рисунке выше показан подход снизу вверх, тогда как формула дает интуитивное представление о решении проблемы сверху вниз.</span></p>
<br>
<p><span class="new-item item-g">Типы состояний</span> В таблице ниже представлена терминология, когда речь идет о состояниях в контексте поиска по единой стоимости:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:350px;">
<colgroup>
<col style=width:140px>
<col style=width:100%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Состояние</b></td>
<td align=center><b>Объяснение</b></td>
</tr>
<tr>
<td align=center valign=center>Explored $\mathcal{E}$</td>
<td align=left valign=top>Состояния c уже найденным оптимальным путем</td>
</tr>
<tr>
<td align=center valign=center>Frontier $\mathcal{F}$</td>
<td align=left valign=top>Видны состояния (нам неизвестен путь к ним с самой низкой ценой)</td>
</tr>
<tr>
<td align=center valign=center>Unexplored $\mathcal{U}$</td>
<td align=left valign=top>Состояний пока не видно</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-g">Поиск по единой стоимости (UCS)</span> Поиск по единой стоимости - это алгоритм поиска, который направлен на поиск кратчайшего пути от состояния $s_\textrm{start}$ до конечного состояния $s_\textrm{end}$. Он исследует состояния в порядке возрастания $\textrm{PastCost}(s)$ и полагается на тот факт, что все затраты на действия неотрицательны.</p>
<div class=mobile-container>
<center>
  <img alt="Uniform Cost Search" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=ucs-example.png?e565704b2370d49fa14f56f8259d3515 style=width:100%;max-width:450px>
</center>
</div>
<p><span class=remark>Замечание 1: алгоритм UCS логически эквивалентен алгоритму Дейкстры.</span></p>
<p><span class=remark>Замечание 2: алгоритм не будет работать для задачи с отрицательными затратами действий, и добавление положительной константы, чтобы сделать их неотрицательными, не решит проблему, так как в конечном итоге это будет другой проблемой.</span></p>
<br>
<p><span class="new-item item-r">Теорема корректности</span> Когда состояние $s$ выталкивается из границы $\mathcal{F}$ и перемещается в исследуемый набор $\mathcal{E}$, его приоритет равен $\textrm{PastCost}(s)$, который представляет собой путь с минимальной стоимостью от $s_\textrm{start}$ до $s$.</p>
<br>
<p><span class="new-item item-b">Сводка алгоритмов поиска по графу</span> Обозначим $N$ общее количество состояний, $n$ из которых исследуются до конечного состояния $s_\textrm{end}$, у нас есть:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:500px;">
<colgroup>
<col style=width:35%>
<col style=width:21.75%>
<col style=width:21.75%>
<col style=width:21.75%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Алгоритм</b></td>
<td align=center><b>Ацикличность</b></td>
<td align=center><b>Стоимость</b></td>
<td align=center><b>Время/пространство</b></td>
</tr>
<tr>
<td align=center valign=top>Динамическое программирование</td>
<td align=center valign=top>yes</td>
<td align=center valign=top>any</td>
<td align=center valign=top>$\mathcal{O}(N)$</td>
</tr>
<tr>
<td align=center valign=top>Поиск по единой стоимости</td>
<td align=center valign=top>no</td>
<td align=center valign=top>$c\geqslant0$</td>
<td align=center valign=top>$\mathcal{O}(n\log(n))$</td>
</tr>
</tbody>
</table>
</center>
</div>
<p><span class=remark>Примечание: обратный отсчет сложности предполагает, что количество возможных действий для каждого состояния будет постоянным.</span></p>
<br>
<h3><a aria-hidden=true class=anchor href=#learning-costs id=learning-costs></a>Стоимость обучения</h3>
<p>Предположим, нам не даны значения $\textrm{Cost}(s,a)$, мы хотим оценить эти количества из обучающего набора последовательности действий $(a_1, a_2, ..., a_k)$ с минимизацией затрат пути.</p>
<p><span class="new-item item-g">Структурированный перцептрон</span> Структурированный перцептрон - это алгоритм итеративного изучения стоимости каждой пары состояние-действие. На каждом шагу, он:
</p><ul>
<li> уменьшает оценочную стоимость каждого действия состояния истинного пути минимизации $y$ (заданного обучающими данными),
</li><li> увеличивает оценочную стоимость каждого действия состояния текущего прогнозируемого пути $y'$ (выведенную из изученных весов).
</li></ul><p></p>
<p><span class=remark>Примечание: существует несколько версий алгоритма, одна из которых упрощает задачу до изучения только стоимости каждого действия $a$, другая параметризует $\textrm{Cost}(s,a)$ вектором признаков обучаемых весов.</span></p>
<br>
<h3><a aria-hidden=true class=anchor href=#a-star id=a-star></a>$\textrm{A}^{\star}$ search</h3>
<p><span class="new-item item-g">Эвристическая функция</span> эвристика - это функция $h$ по состояниям $s$, где каждый $h(s)$ направлен на оценку $\textrm{FutureCost}(s)$, стоимость пути от $s$ до $s_\textrm{end}$.</p>
<div class=mobile-container>
<center>
  <img alt="A star" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=a-star.png?e4f6b57d6b77c9ecf9bdd5d7d491e703 style=width:100%;max-width:465px>
</center>
</div>
<br>
<p><span class="new-item item-r">Алгоритм</span> $A^{*}$ - это алгоритм поиска, цель которого - найти кратчайший путь от состояния $s$ до конечного состояния $s_\textrm{end}$. Он исследует состояния в порядке возрастания $\textrm{PastCost}(s) + h(s)$. Это эквивалентно поиску по единой стоимости с краевыми затратами $\textrm{Cost}'(s,a)$, задаваемыми выражением:</p>
<div class=mobile-container>
\[\boxed{\textrm{Cost}'(s,a)=\textrm{Cost}(s,a)+h(\textrm{Succ}(s,a))-h(s)}\]
</div>
<p><span class=remark>Примечание: этот алгоритм можно рассматривать как предвзятую версию исследования состояний UCS, которые оцениваются как более близкие к конечному состоянию.</span></p>
<br>
<p><span class="new-item item-b">Консистентность</span> Эвристика $h$ называется согласованной (консистентной) при удовлетворении двух следующих свойств:
</p><ul>
<li>Для всех состояний $s$ и действий $a$,
<div class=mobile-container>
\[\boxed{h(s) \leqslant \textrm{Cost}(s,a)+h(\textrm{Succ}(s,a))}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Consistency class=img-responsive netsrc=teaching/cs-221/illustrations/ src=consistency-1.png?4cbccf6f546df017acee0dc45d740b92 style=width:100%;max-width:465px>
</center>
</div>
</li><li>Конечное состояние подтверждает следующее:
<div class=mobile-container>
\[\boxed{h(s_{\textrm{end}})=0}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Consistency class=img-responsive netsrc=teaching/cs-221/illustrations/ src=consistency-2.png?539ae50a1bbf1b85bf3f8d0b32c48445 style=width:100%;max-width:203px>
</center>
</div>
</li></ul>
<p></p>
<br>
<p><span class="new-item item-r">Корректность</span> Если $h$ согласован, то $A^*$ возвращает путь с минимальной стоимостью.</p>
<br>
<p><span class="new-item item-g">Допустимость</span> Говорят, что эвристика $h$ допустима, если у нас есть:</p>
<div class=mobile-container>
\[\boxed{h(s)\leqslant\textrm{FutureCost}(s)}\]
</div>
<br>
<p><span class="new-item item-r">Теорема</span> Пусть $h(s)$ - заданная эвристика. У нас есть:</p>
<div class=mobile-container>
\[\boxed{h(s)\textrm{ consistent}\Longrightarrow h(s)\textrm{ admissible}}\]
</div>
<br>
<p><span class="new-item item-r">Эффективность</span> $A^*$ исследует все состояния $s$, удовлетворяющие следующему уравнению:</p>
<div class=mobile-container>
\[\boxed{\textrm{PastCost}(s)\leqslant\textrm{PastCost}(s_{\textrm{end}})-h(s)}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Efficiency class=img-responsive netsrc=teaching/cs-221/illustrations/ src=efficiency.png?6306c2c390e4b08c0fe37bc99e6caf5d style=width:100%;max-width:465px>
</center>
</div>
<p><span class=remark>Примечание: большие значения $h(s)$ лучше, поскольку это уравнение показывает, что оно ограничивает набор состояний $s$, которые будут исследованы.</span></p>
<br>
<h3><a aria-hidden=true class=anchor href=#relaxation id=relaxation></a>Ослабление ограничений</h3>
<p>Это основа для создания последовательной эвристики. Идея состоит в том, чтобы найти сокращенные затраты в аналитическом виде, удалив ограничения и используя их в качестве эвристики.</p>
<br>
<p><span class="new-item item-g">Задача с ослабленными ограничениями поиска</span> Ослабление задачи поиска $P$ с затратами $\textrm{Cost}$ обозначается $P_{\textrm{rel}}$ с затратами $\textrm{Cost}_{\textrm{rel}}$ и удовлетворяет тождеству:</p>
<div class=mobile-container>
\[\boxed{\textrm{Cost}_{\textrm{rel}}(s,a)\leqslant\textrm{Cost}(s,a)}\]
</div>
<br>
<p><span class="new-item item-r">Ослабленная эвристика</span> Задана упрощенная задача поиска $P_{\textrm{rel}}$, мы определяем ослабленную эвристику $h(s)=\textrm{FutureCost}_{\textrm{rel}}(s)$ как путь с минимальной стоимостью от $s$ до конечного состояния в графе затрат $\textrm{Cost}_{\textrm{rel}}(s,a)$.</p>
<br>
<p><span class="new-item item-b">Консистентность ослабленной эвристики</span> Пусть $P_{\textrm{rel}}$ - заданная ослабленная задача. По теореме у нас есть:</p>
<div class=mobile-container>
\[\boxed{h(s)=\textrm{FutureCost}_{\textrm{rel}}(s)\Longrightarrow h(s)\textrm{ consistent}}\]
</div>
<br>
<p><span class="new-item item-g">Компромисс при выборе эвристики</span> При выборе эвристики необходимо уравновесить два аспекта:
</p><ul>
<li>Вычислительная эффективность: $h(s)=\textrm{FutureCost}_{\textrm{rel}}(s)$ должно быть легко вычислить. Она должна создать аналитический вид, более легкий поиск и независимые подзадачи.
</li><li>Достаточно хорошее приближение: эвристика $h(s)$ должна быть близка к $\textrm{FutureCost}(s)$ , и мы не должны удалять слишком много ограничений.
</li></ul><p></p>
<br>
<p><span class="new-item item-g">Max эвристика</span> пусть $h_1(s)$, $h_2(s)$ - две эвристики. У нас есть следующее свойство:</p>
<div class=mobile-container>
\[\boxed{h_1(s),\textrm{ }h_2(s)\textrm{ consistent}\Longrightarrow h(s)=\max\{h_1(s),\textrm{ }h_2(s)\}\textrm{ consistent}}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#markov-decision-processes id=markov-decision-processes></a>Марковские процессы принятия решений</h2>
<p>В этом разделе мы предполагаем, что выполнение действия $a$ из состояния $s$ может привести к нескольким состояниям $s_1',s_2',...$ вероятностным образом. Чтобы найти путь между начальным и конечным состояниями, наша цель будет заключаться в том, чтобы найти политику максимальной ценности с помощью марковских процессов принятия решений, которые помогают нам справляться со случайностью и неопределенностью.</p>
<h3>Обозначения</h3>
<p><span class="new-item item-b">Определение</span> Цель марковского процесса принятия решений - максимизировать вознаграждение. Он определяется с помощью:
  </p><ul>
    <li>начальное состояние $s_{\textrm{start}}$
    </li><li>возможные действия $\textrm{Actions}(s)$ из состояния $s$
    </li><li>вероятности перехода $T(s,a,s')$ из $s$ в $s'$ с действием $a$
    </li><li>награды $\textrm{Reward}(s,a,s')$ из $s$ в $s'$ с действием $a$
    </li><li>было ли достигнуто конечное состояние $\textrm{IsEnd}(s)$
    </li><li>коэффициент дисконтирования $0\leqslant\gamma\leqslant1$
  </li></ul>
<p></p>
<div class=mobile-container>
<center>
  <img alt="Markov Decision Process" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=markov-decision-process.png?bdaeab5d6b9b89652d54601199c83c85 style=width:100%;max-width:465px>
</center>
</div>
<br>
<p><span class="new-item item-r">Вероятности перехода</span> Вероятность перехода $T(s,a,s')$ определяет вероятность перехода в состояние $s'$ после того, как действие $a$ будет выполнено в состоянии $s$. Каждый $s' \mapsto T(s,a,s')$ представляет собой распределение вероятностей, что означает, что:</p>
<div class=mobile-container>
\[\forall s,a,\quad\boxed{\displaystyle\sum_{s'\in\textrm{ States}}T(s,a,s')=1}\]
</div>
<br>
<p><span class="new-item item-b">Политика</span> Политика $\pi$ - это функция, которая сопоставляет каждое состояние $s$ с действием $a$, то есть</p>
<div class=mobile-container>
\[\boxed{\pi : s \mapsto a}\]
</div>
<br>
<p><span class="new-item item-g">Полезность</span> Полезность пути $(s_0, ..., s_k)$ - это дисконтированная сумма вознаграждений на этом пути. Другими словами,</p>
<div class=mobile-container>
\[\boxed{u(s_0,...,s_k)=\sum_{i=1}^{k}r_i\gamma^{i-1}}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Utility class=img-responsive netsrc=teaching/cs-221/illustrations/ src=utility.png?858a1983bd7bb62a99dcbdad079e57c7 style=width:100%;max-width:465px>
</center>
</div>
<p><i>На рисунке выше показан случай $k=4$.</i></p>
<br>
<p><span class="new-item item-g">$Q$-ценность</span> $Q$-ценность политики $\pi$ в состоянии $s$ с действием $a$, также обозначаемое как $Q_{\pi}(s,a)$, является ожидаемой полезностью из состояния $s$ после выполнения действия $a$ и последующего исполнения политики $\pi$. Это определяется следующим образом:</p>
<div class=mobile-container>
\[\boxed{Q_{\pi}(s,a)=\sum_{s'\in\textrm{ States}}T(s,a,s')\left[\textrm{Reward}(s,a,s')+\gamma V_\pi(s')\right]}\]
</div>
<br>
<p><span class="new-item item-g">Ценность политики</span> Ценность политики $\pi$ из состояния $s$, также обозначаемое как $V_{\pi}(s)$, является ожидаемой полезностью при следовании политике $\pi$ из состояния $s$ по случайным путям. Это определяется следующим образом:</p>
<div class=mobile-container>
\[\boxed{V_\pi(s)=Q_\pi(s,\pi(s))}\]
</div>
<p><span class=remark>Примечание: $V_\pi(s)$ равно 0, если $s$ - конечное состояние.</span></p>
<br>
<h3>Приложения</h3>
<p><span class="new-item item-r">Оценка политики</span> Задана политика $\pi$, оценка политики представляет собой итеративный алгоритм. Он нацелен на оценку $V_\pi$. Делается это следующим образом:
</p><ul>
<li>Инициализация: для всех состояний $s$, имеем
<div class=mobile-container>
\[\boxed{V_\pi^{(0)}(s)\longleftarrow0}\]
</div>
</li><li>Итерация: для $t$ от 1 до $T_{\textrm{PE}}$, имеем
<div class=mobile-container>
\[\forall s,\quad\boxed{V_\pi^{(t)}(s)\longleftarrow Q_\pi^{(t-1)}(s,\pi(s))}\]
</div>
с
<div class=mobile-container>
\[\boxed{Q_\pi^{(t-1)}(s,\pi(s))=\sum_{s'\in\textrm{ States}}T(s,\pi(s),s')\Big[\textrm{Reward}(s,\pi(s),s')+\gamma V_\pi^{(t-1)}(s')\Big]}\]
</div>
</li></ul><p></p>
<p><span class=remark>Примечание: обозначим $S$ - количество состояний, $A$ - количество действий на состояние, $S'$ - количество преемников и $T$ - количество итераций, то временная сложность равна $\mathcal{O}(T_{\textrm{PE}}SS')$.</span></p>
<br>
<p><span class="new-item item-b">Оптимальная $Q$-ценность</span> Оптимальная $Q$-ценность $Q_{\textrm{opt}}(s,a)$ состояния $s$ с действием $a$ определяется как максимальная $Q$-ценность, достигаемая при любой стратегии. Она рассчитывается следующим образом:</p>
<div class=mobile-container>
\[\boxed{Q_{\textrm{opt}}(s,a)=\sum_{s'\in\textrm{ States}}T(s,a,s')\left[\textrm{Reward}(s,a,s')+\gamma V_\textrm{opt}(s')\right]}\]
</div>
<br>
<p><span class="new-item item-b">Оптимальная ценность</span> Оптимальная ценность $V_{\textrm{opt}}(s)$ состояния $s$ определяется как максимальное ценность, достигаемая любой политикой. Она рассчитывается следующим образом:</p>
<div class=mobile-container>
\[\boxed{V_{\textrm{opt}}(s)=\underset{a\in\textrm{ Actions}(s)}{\textrm{max}}Q_\textrm{opt}(s,a)}\]
</div>
<br>
<p><span class="new-item item-b">Оптимальная политика</span> Оптимальная политика $\pi_{\textrm{opt}}$ определяется как политика, которая приводит к оптимальным ценностям. Она определяется:</p>
<div class=mobile-container>
\[\forall s,\quad\boxed{\pi_{\textrm{opt}}(s)=\underset{a\in\textrm{ Actions}(s)}{\textrm{argmax}}Q_\textrm{opt}(s,a)}\]
</div>
<br>
<p><span class="new-item item-r">Итерация ценности</span> Итерация ценности - это ищущий оптимальное значение $V_{\textrm{opt}}$ и оптимальную политику $\pi_{\textrm{opt}}$ алгоритм. Делается это следующим образом:
</p><ul>
<li>Инициализация: для всех состояний $s$, имеем
<div class=mobile-container>
\[\boxed{V_{\textrm{opt}}^{(0)}(s)\longleftarrow0}\]
</div>
</li><li>Итерация: для $t$ от 1 до $T_{\textrm{VI}}$, имеем
<div class=mobile-container>
\[\forall s,\quad\boxed{V_\textrm{opt}^{(t)}(s)\longleftarrow \underset{a\in\textrm{ Actions}(s)}{\textrm{max}}Q_\textrm{opt}^{(t-1)}(s,a)}\]
</div>
с
<div class=mobile-container>
\[\boxed{Q_\textrm{opt}^{(t-1)}(s,a)=\sum_{s'\in\textrm{ States}}T(s,a,s')\Big[\textrm{Reward}(s,a,s')+\gamma V_\textrm{opt}^{(t-1)}(s')\Big]}\]
</div>
</li></ul><p></p>
<p><span class=remark>Примечание: если у нас либо $\gamma &lt; 1$, либо граф MDP ацикличен, то алгоритм итерации значений гарантированно сходится к правильному ответу.</span></p>
<br>
<h3>Когда неизвестны переходы и награды</h3>
<p>Теперь предположим, что вероятности перехода и награды неизвестны.</p>
<p><span class="new-item item-r">Моделирование по методу Монте-Карло</span> Моделирование по методу Монте-Карло направлено на оценку $T(s,a,s')$ и $\textrm{Reward}(s,a,s')$ путем проведения Монте-Карло экспериментов с:
</p><div class=mobile-container>
\[\boxed{\widehat{T}(s,a,s')=\frac{\#\textrm{ times }(s,a,s')\textrm{ occurs}}{\#\textrm{ times }(s,a)\textrm{ occurs}}}\]
</div>
и
<div class=mobile-container>
\[\boxed{\widehat{\textrm{Reward}}(s,a,s')=r\textrm{ in }(s,a,r,s')}\]
</div>
Эти оценки затем будут использоваться для вывода $Q$-ценностей, включая $Q_\pi$ и $Q_\textrm{opt}.$<p></p>
<p><span class=remark>Примечание: Моделирование по методу Монте-Карло относится к методам вне политики, потому что оценка не зависит от конкретной политики.</span></p>
<br>
<p><span class="new-item item-r">Безмодельный метод Монте-Карло</span> Безмодельный метод Монте-Карло направлен на прямую оценку $Q_{\pi}$ следующим образом:
</p><div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)=\textrm{average of }u_t\textrm{ where }s_{t-1}=s, a_t=a}\]
</div>
где $u_t$ обозначает полезность, начиная с шага $t$ данного эпизода.<p></p>
<p><span class=remark>Примечание: Безмодельный метод Монте-Карло считается действующим в соответствии с политикой, поскольку оценочное значение зависит от политики $\pi$, используемой для генерации данных.</span></p>
<br>
<p><span class="new-item item-r">Эквивалентная формулировка</span> Путем введения константы $\eta=\frac{1}{1+(\#\textrm{updates to }(s,a))}$ и для каждого $(s,a,u)$ обучающего набора, правило обновления безмодельного метода Монте-Карло имеет формулировку выпуклой комбинации:
</p><div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)\leftarrow(1-\eta)\widehat{Q}_\pi(s,a)+\eta u}\]
</div>
а также формулировку стохастического градиента:
<div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)\leftarrow\widehat{Q}_\pi(s,a) - \eta (\widehat{Q}_\pi(s,a) - u)}\]
</div>
<p></p>
<br>
<p><span class="new-item item-g">State-action-reward-state-action (SARSA)</span> Состояние-действие-награда-состояние-действие - это бутстрэп метод оценки $Q_\pi$ с использованием как необработанных данных, так и оценок как части правила обновления. Для каждого $(s,a,r,s',a')$, у нас есть:</p>
<div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)\longleftarrow(1-\eta)\widehat{Q}_\pi(s,a)+\eta\Big[r+\gamma\widehat{Q}_\pi(s',a')\Big]}\]
</div>
<p><span class=remark>Примечание: оценка SARSA обновляется "на лету", в отличие от безмодельного метода Монте-Карло, где оценка может быть обновлена только в конце эпизода.</span></p>
<br>
<p><span class="new-item item-b">$Q$-обучение</span> $Q$-обучение - это алгоритм вне политики, который производит оценку $Q_\textrm{opt}$. На каждой $(s,a,r,s',a')$, у нас есть:</p>
<div class=mobile-container>
\[\boxed{\widehat{Q}_{\textrm{opt}}(s,a)\leftarrow(1-\eta)\widehat{Q}_{\textrm{opt}}(s,a)+\eta\Big[r+\gamma\underset{a'\in\textrm{ Actions}(s')}{\textrm{max}}\widehat{Q}_{\textrm{opt}}(s',a')\Big]}\]
</div>
<br>
<p><span class="new-item item-g">Эпсилон-жадная политика</span> Эпсилон-жадная политика - это алгоритм, который уравновешивает исследование с вероятностью $\epsilon$ и использование с вероятностью $1-\epsilon$. Для заданного состояния $s$ политика $\pi_{\textrm{act}}$ вычисляется следующим образом:</p>
<div class=mobile-container>
\[\boxed{\pi_\textrm{act}(s)=\left\{\begin{array}{ll}\underset{a\in\textrm{ Actions}}{\textrm{argmax }}\widehat{Q}_\textrm{opt}(s,a) &amp; \textrm{with proba }1-\epsilon\\\textrm{random from Actions}(s) &amp; \textrm{with proba }\epsilon\end{array}\right.}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#game-playing id=game-playing></a>Игровой процесс</h2>
<p>В играх (например, шахматы, нарды, го) присутствуют другие агенты, которые необходимо учитывать при построении нашей политики.</p>
<p><span class="new-item item-b">Дерево игры</span> Дерево игры - это дерево, которое описывает возможности игры. В частности, каждый узел является точкой принятия решения для игрока, а каждый путь от корня к листу - это возможный результат игры.</p>
<br>
<p><span class="new-item item-b">Игра для двух игроков с нулевой суммой</span> это игра с полным соблюдением каждого состояния и попеременным ходом игроков. Она определяется с помощью:
</p><ul>
  <li>начальное состояние $s_{\textrm{start}}$
  </li><li>возможные действия $\textrm{Actions}(s)$ из состояния $s$
  </li><li>преемники $\textrm{Succ}(s,a)$ из состояния $s$ с действиями $a$
  </li><li>было ли достигнуто конечное состояние $\textrm{IsEnd}(s)$
  </li><li>полезность агента $\textrm{Utility}(s)$ в конечном состоянии $s$
  </li><li>игрок $\textrm{Player}(s)$ контролирует состояние $s$
</li></ul><p></p>
<p><span class=remark>Примечание: предположим, что полезность агента имеет противоположенный знак по сравнению с полезностью оппонента.</span></p>
<br>
<p><span class="new-item item-g">Типы политик</span> Есть два типа политик:
</p><ul>
<li>Детерминистские политики, обозначаются $\pi_p(s)$, действия игрока $p$ в состоянии $s$.
</li><li>Стохастические политики, denoted $\pi_p(s,a)\in[0,1]$, являющиеся вероятностями совершения игроком $p$ действия $a$ в состоянии $s$.
</li></ul><p></p>
<br>
<p><span class="new-item item-g">Expectimax</span> Для данного состояния $s$ значение expectimax $V_{\textrm{exptmax}}(s)$ является максимальной ожидаемой полезностью любой политики агента при игре относительно фиксированной и известной политики противника $\pi_{\textrm{opp}}$. Он рассчитывается следующим образом:</p>
<div class=mobile-container>
\[\boxed{V_{\textrm{exptmax}}(s)=\left\{\begin{array}{ll}\textrm{Utility}(s) &amp; \textrm{IsEnd}(s)\\\underset{a\in\textrm{Actions}(s)}{\textrm{max}}V_{\textrm{exptmax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{agent}\\\displaystyle\sum_{a\in\textrm{Actions}(s)}\pi_{\textrm{opp}}(s,a)V_{\textrm{exptmax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{opp}\end{array}\right.}\]
</div>
<p><span class=remark>Примечание: expectimax - аналог итерации значений для MDP.</span></p>
<div class=mobile-container>
<center>
  <img alt=Expectimax class=img-responsive netsrc=teaching/cs-221/illustrations/ src=expectimax.png?e45e88a1bab91386c3e1d4ff401a1d97 style=width:100%;max-width:600px>
</center>
</div>
<br>
<p><span class="new-item item-b">Minimax</span> Цель минимаксных политик - найти оптимальную политику против противника, предполагая наихудший случай, то есть то, что противник делает все, чтобы минимизировать полезность агента. Делается это следующим образом:</p>
<div class=mobile-container>
\[\boxed{V_{\textrm{minimax}}(s)=\left\{\begin{array}{ll}\textrm{Utility}(s) &amp; \textrm{IsEnd}(s)\\\underset{a\in\textrm{Actions}(s)}{\textrm{max}}V_{\textrm{minimax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{agent}\\\underset{a\in\textrm{Actions}(s)}{\textrm{min}}V_{\textrm{minimax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{opp}\end{array}\right.}\]
</div>
<p><span class=remark>Примечание: мы можем извлечь $\pi_{\textrm{max}}$ и $\pi_{\textrm{min}}$ из минимаксного значения $V_{\textrm{minimax}}$.</span></p>
<div class=mobile-container>
<center>
  <img alt=Minimax class=img-responsive netsrc=teaching/cs-221/illustrations/ src=minimax.png?a10b34193ca7a93d1c7a764ee19de949 style=width:100%;max-width:600px>
</center>
</div>
<br>
<p><span class="new-item item-r">Свойства minimax</span> Обозначим $V$ как функцию ценности, необходимо иметь в виду 3 свойства вокруг минимакса:
</p><ul>
<li>Свойство 1: если агент изменит свою политику на какой-либо $\pi_{\textrm{agent}}$, то ему не будет лучше.
<div class=mobile-container>
\[\boxed{\forall \pi_{\textrm{agent}},\quad V(\pi_{\textrm{max}},\pi_{\textrm{min}})\geqslant V(\pi_{\textrm{agent}},\pi_{\textrm{min}})}\]
</div>
</li><li>Свойство 2: если противник изменит свою политику с $\pi_{\textrm{min}}$ на $\pi_{\textrm{opp}}$, то ему не станет лучше.
<div class=mobile-container>
\[\boxed{\forall \pi_{\textrm{opp}},\quad V(\pi_{\textrm{max}},\pi_{\textrm{min}})\leqslant V(\pi_{\textrm{max}},\pi_{\textrm{opp}})}\]
</div>
</li><li>Свойство 3: если известно, что противник не использует состязательную политику, то минимаксная политика может быть неоптимальной для агента.
<div class=mobile-container>
\[\boxed{\forall \pi,\quad V(\pi_{\textrm{max}},\pi)\leqslant V(\pi_{\textrm{exptmax}},\pi)}\]
</div>
</li></ul>
В итоге имеем следующие отношения:
<div class=mobile-container>
\[\boxed{V(\pi_{\textrm{exptmax}},\pi_{\textrm{min}})\leqslant V(\pi_{\textrm{max}},\pi_{\textrm{min}})\leqslant V(\pi_{\textrm{max}},\pi)\leqslant V(\pi_{\textrm{exptmax}},\pi)}\]
</div>
<p></p>
<br>
<h3>Ускорение minimax</h3>
<p><span class="new-item item-g">Функция оценки</span> Функция оценки - это зависящая от предметной области и приблизительная оценка значения $V_{\textrm{minimax}}(s)$. Обозначается $\textrm{Eval}(s)$.</p>
<p><span class=remark>Примечание: $\textrm{FutureCost}(s)$ - аналог задачи поиска.</span></p>
<br>
<p><span class="new-item item-b">Альфа-бета обрезка</span> Alpha-beta pruning - это общий для предметной области точный метод, оптимизирующий алгоритм минимакса, избегая ненужного исследования частей игрового дерева. Для этого каждый игрок отслеживает наилучшее значение, на которое он может надеяться (сохраняется в $\alpha$ для максимизирующего игрока и в $\beta$ для минимизирующего игрока). На данном шаге условие $\beta &lt; \alpha$ означает, что оптимальный путь не будет в текущей ветви, поскольку у игрока на предыдущем шаге имелся более хороший вариант.</p>
<div class=mobile-container>
<center>
  <img alt="Alpha beta pruning" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=alpha-beta-pruning.png?30f2f19c5b0621ca55fcce4bdb6f914c style=width:100%;max-width:600px>
</center>
</div>
<br>
<p><span class="new-item item-b">Обучение временной разнице</span> Обучение Temporal difference (TD) используется, когда мы не знаем переходов/наград. Значение основано на политике разведки. Чтобы использовать его, нам необходимо знать правила игры $\textrm{Succ}(s,a)$. Для каждого $(s,a,r,s')$ обновление выполняется следующим образом:</p>
<div class=mobile-container>
\[\boxed{w\longleftarrow w-\eta\big[V(s,w)-(r+\gamma V(s',w))\big]\nabla_wV(s,w)}\]
</div>
<br>
<h3>Одновременные игры</h3>
<p>Это полная противоположность пошаговым играм, где нет упорядочивания ходов игрока.</p>
<br>
<p><span class="new-item item-g">Одновременная игра с одним ходом</span> пусть есть два игрока $A$ и $B$ с заданными возможными действиями. Мы обозначаем, что $V(a,b)$ является полезностью $A$, если $A$ выбирает действие $a$, $B$ выбирает действие $b$. $V$ называется матрицей выигрыша.</p>
<br>
<p><span class="new-item item-r">Стратегии</span> Есть два основных типа стратегий:
</p><ul>
<li>Чистая стратегия - это одно действие:
<div class=mobile-container>
\[\boxed{a\in\textrm{Actions}}\]
</div>
</li><li>Смешанная стратегия - это распределение вероятностей по действиям:
<div class=mobile-container>
\[\forall a\in\textrm{Actions},\quad\boxed{0\leqslant\pi(a)\leqslant1}\]
</div>
</li></ul><p></p>
<br>
<p><span class="new-item item-b">Оценка игры</span> Ценность игры $V(\pi_A,\pi_B)$, когда игрок $A$ следует за $\pi_A$, а игрок $B$ следует за $\pi_B$, такова, что:</p>
<div class=mobile-container>
\[\boxed{V(\pi_A,\pi_B)=\sum_{a,b}\pi_A(a)\pi_B(b)V(a,b)}\]
</div>
<br>
<p><span class="new-item item-r">Minimax теорема</span> Положим, что $\pi_A,\pi_B$ пробегают по смешанным стратегиям для любой одновременной игры двух игроков с нулевой суммой и конечного числа действий, у нас есть:</p>
<div class=mobile-container>
\[\boxed{\max_{\pi_A}\min_{\pi_B}V(\pi_A,\pi_B)=\min_{\pi_B}\max_{\pi_A}V(\pi_A,\pi_B)}\]
</div>
<br>
<h3>Игры с ненулевой суммой</h3>
<p><span class="new-item item-b">Матрица выигрыша</span> Мы определяем $V_p(\pi_A,\pi_B)$ как полезность для игрока $p$.</p>
<br>
<p><span class="new-item item-r">Nash equilibrium</span> Равновесие по Нэшу - это $(\pi_A^*,\pi_B^*)$ такое, что ни у одного игрока нет стимула изменить свою стратегию, у нас есть:</p>
<div class=mobile-container>
\[\boxed{\forall \pi_A, V_A(\pi_A^*,\pi_B^*)\geqslant V_A(\pi_A,\pi_B^*)}\quad\textrm{и}\quad\boxed{\forall \pi_B, V_B(\pi_A^*,\pi_B^*)\geqslant V_B(\pi_A^*,\pi_B)}\]
</div>
<p><span class=remark>Примечание: в любой игре с конечным числом игроков и конечным числом действий существует хотя бы одно равновесие по Нэшу.</span></p>
</article> </div> <!-- FOOTER <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-twitter fa-3x fa-fw"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class="fa fa-linkedin fa-3x fa-fw"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-github fa-3x fa-fw"></i></a> <a href="https://scholar.google.com/citations?user=nMnMTm8AAAAJ" onclick=trackOutboundLink(this);><i class="fa fa-google fa-3x fa-fw"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick="trackOutboundLink(this); window.location.href = 'mailto:' + this.dataset.name + '@' + this.dataset.domain + '.' + this.dataset.tld"><i class="fa fa-envelope fa-3x fa-fw"></i></a> </div> </div> </footer> --> </body></html>

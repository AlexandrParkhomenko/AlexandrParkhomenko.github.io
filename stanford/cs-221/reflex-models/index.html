<!DOCTYPE html><html lang=en><head><!-- base href=../../ --><title>CS 221 - Reflex-based Models Cheatsheet</title><meta charset=utf-8><meta content="Teaching page of Shervine Amidi, Graduate Student at Stanford University." name=description><meta content="teaching, shervine, shervine amidi, data science" name=keywords><meta content="width=device-width, initial-scale=1" name=viewport><link href=https://stanford.edu/~shervine/teaching/cs-221/cheatsheet-reflex-models rel=canonical>
<link href=../../css/bootstrap.min.css rel=stylesheet>
<link href=../../css/font-awesome.min.css rel=stylesheet>
<link href=../../css/katex.min.css rel=stylesheet>
<link href=../../css/style.min.css rel=stylesheet type=text/css>
<link href=../../css/article.min.css rel=stylesheet>
<script src=../../js/jquery.min.js></script>
<script src=../../js/bootstrap.min.js></script>
<script defer src=../../js/underscore-min.js type=text/javascript></script>
<script defer src=../../js/katex.min.js></script>
<script defer src=../../js/auto-render.min.js></script>
<script defer src=../../js/article.min.js></script>
<script defer src=../../js/lang.min.js></script>
<script async defer src=../../js/buttons.js></script>
</head> <body data-offset=50 data-spy=scroll data-target=.navbar> <!-- HEADER <nav class="navbar navbar-inverse navbar-static-top"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class="collapse navbar-collapse" id=myNavbar> <ul class="nav navbar-nav"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class="nav navbar-nav navbar-center"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class="collapse navbar-collapse" data-target=None id=HiddenNavbar> <ul class="nav navbar-nav navbar-right"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style="padding: 0px;"> <img alt=MIT netsrc=images/ src=../../images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style="padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;"> </a> </ul> </div> </div> </div> </nav> --> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title style=display:none;> <!-- DISPLAY:NONE --> <a href=teaching/cs-221 onclick=trackOutboundLink(this);><img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48 style="width: 12px;">   <b>CS 221 - Artificial Intelligence</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>Reflex-based models</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#linear-predictors>Линейные предсказатели</a></div> <div class=dropdown-container> <a href=#linear-predictors><span>Вектор признаков</span></a> <a href=#linear-predictors><span>Линейный классификатор/регрессия</span></a> <a href=#linear-predictors><span>Зазор</span></a> <a href=#linear-predictors><span>Разность</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#loss-minimization>Минимизация потерь</a></div> <div class=dropdown-container> <a href=#loss-minimization><span>Функция потерь</span></a> <a href=#loss-minimization><span>Фреймворк</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#non-linear-predictors>Нелинейные предсказатели</a></div> <div class=dropdown-container> <a href=#non-linear-predictors><span>k-ближайших соседей</span></a> <a href=#non-linear-predictors><span>Нейронные сети</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#stochastic-gradient-descent>Стохастический градиентный спуск</a></div> <div class=dropdown-container> <a href=#stochastic-gradient-descent><span>Градиент</span></a> <a href=#stochastic-gradient-descent><span>Стохастические обновления</span></a> <a href=#stochastic-gradient-descent><span>Пакетные обновления</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#fine-tuning-models>Дообучение моделей</a></div> <div class=dropdown-container> <a href=#fine-tuning-models><span>Класс гипотез</span></a> <a href=#fine-tuning-models><span>Обратное распространение ошибки</span></a> <a href=#fine-tuning-models><span>Регуляризация</span></a> <a href=#fine-tuning-models><span>Наборы словарей</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#unsupervised-learning>Обучение без учителя</a></div> <div class=dropdown-container> <a href=#unsupervised-learning><span>k-средние</span></a> <a href=#unsupervised-learning><span>Метод главных компонент</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-221-artificial-intelligence/blob/master/en/cheatsheet-reflex-models.pdf onclick=trackOutboundLink(this); style="color: white; text-decoration:none;"> <i aria-hidden=false class="fa fa-github fa-fw"></i> Посмотреть PDF-версию на GitHub </a> </li> </div> </center> </div> <article class="markdown-body entry-content" itemprop=text>
<div class="alert alert-primary" role=alert style=display:none;> <!-- DISPLAY:NONE -->
  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!
</div>
<div class=title-lang style=display:none;> <!-- DISPLAY:NONE -->
  <a aria-hidden=true class=anchor-bis href=#cs-221---artificial-intelligence id=cs-221---artificial-intelligence></a><a href=teaching/cs-221 onclick=trackOutboundLink(this);>CS 221 - Artificial Intelligence</a>
  
  <div style=float:right;display:none;> <!-- DISPLAY:NONE -->
    <div class=input-group>
      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>
        <option selected value=en>English</option>
        <option value=fr>Français</option>
        <option value=tr>Türkçe</option>
      </select>
      <div class=input-group-addon><i class=fa></i></div>
    </div>
  </div>
</div>
<br>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button><B>CS 221 - Artificial Intelligence</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button>CS 229 - Machine Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button>CS 230 - Deep Learning</button>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button><B>Reflex</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/states-models/index.html'; oldhref='teaching/cs-221/cheatsheet-states-models'" type=button>States</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/variables-models/index.html'; oldhref='teaching/cs-221/cheatsheet-variables-models'" type=button>Variables</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/logic-models/index.html'; oldhref='teaching/cs-221/cheatsheet-logic-models'" type=button>Logic</button>
  </div>
</div>
<h1>
  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>Модели машинного обучения на основе рефлексов
</h1>
<i><!-- By --><a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> и <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a>;<a href=https://github.com/AlexandrParkhomenko onclick=trackOutboundLink(this);> Alexandr Parkhomenko</a> и <a href=https://github.com/geotrush onclick=trackOutboundLink(this);>Труш Георгий (Georgy Trush)</a></i>
  <div style=float:right;display:none;> <!-- DISPLAY:NONE --><a aria-label="Star afshinea/stanford-cs-221-artificial-intelligence on GitHub" class="github-button fa-fw" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-221-artificial-intelligence onclick=trackOutboundLink(this);>Star</a></div>
<h2><a aria-hidden=true class=anchor href=#linear-predictors id=linear-predictors></a>Линейные предсказатели</h2>
<p>В этом разделе мы рассмотрим модели, основанные на рефлексах, которые улучшаются по мере накопления опыта, обучаясь на парах наблюдений вход-выход.</p>
<br>
<p><span class="new-item item-g">Вектор признаков</span> Вектор признаков входного сигнала $x$ обозначается как $\phi(x)$ и таков, что:</p>
<div class=mobile-container>
\[\boxed{\phi(x)=\left[\begin{array}{c}\phi_1(x)\\\vdots\\\phi_d(x)\end{array}\right]\in\mathbb{R}^d}\]
</div>
<br>
<p><span class="new-item item-r">Оценка</span> Оценка $s(x,w)$ примера $(\phi(x),y) \in \mathbb{R}^d \times \mathbb{R}$, связанного с линейной моделью весов $w \in \mathbb{R}^d$, задается скалярным произведением:</p>
<div class=mobile-container>
\[\boxed{s(x,w)=w\cdot\phi(x)}\]
</div>
<br>
<h3>Классификация</h3>
<p><span class="new-item item-b">Линейный классификатор</span> Для вектора весов $w\in\mathbb{R}^d$ и вектора признаков $\phi(x)\in\mathbb{R}^d$ бинарный линейный классификатор $f_w$ имеет вид:</p>
<div class=mobile-container>
\[\boxed{f_w(x)=\textrm{sign}(s(x,w))=\left\{
  \begin{array}{cr}
  +1 &amp; \textrm{if }w\cdot\phi(x)&gt;0\\
  -1 &amp; \textrm{if }w\cdot\phi(x)&lt; 0\\
  ? &amp; \textrm{if }w\cdot\phi(x)=0
  \end{array}\right.}\]
</div>
<div class=mobile-container>
<center>
<img alt="Linear classifier" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=linear-classifier.png?79f320ac5ba3e9d5dae2c573007dbfb6 style=width:100%;max-width:550px>
</center>
</div>
<br>
<p><span class="new-item item-g">Зазор</span> Отступ $m(x,y,w) \in \mathbb{R}$ примера $(\phi(x),y) \in \mathbb{R}^d \times \{-1,+1\}$, связанная с линейной моделью весов $w\in \mathbb{R}^d$, количественно оценивает уверенность прогноза: большие значения лучше. Задается следующим образом:</p>
<div class=mobile-container>
\[\boxed{m(x,y,w)=s(x,w)\times y}\]
</div>
<br>
<h3>Регрессия</h3>
<p><span class="new-item item-g">Линейная регрессия</span> Для данного вектора весов $w\in\mathbb{R}^d$ и вектора признаков $\phi(x)\in\mathbb{R}^d$ результат линейной регрессии весов $w$, обозначенный как $f_w$, определяется выражением:</p>
<div class=mobile-container>
\[\boxed{f_w(x)=s(x,w)}\]
</div>
<p><span class="new-item item-g">Остаток</span> Разность Residual $\textrm{res}(x,y,w) \in \mathbb{R}$ определяется как величина, на которую прогноз $f_w(x)$ превышает целевой $y$:</p>
<div class=mobile-container>
\[\boxed{\textrm{res}(x,y,w)=f_w(x)-y}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#loss-minimization id=loss-minimization></a>Минимизация потерь</h2>
<p><span class="new-item item-g">Функция потерь</span> Функция потерь $\textrm{Loss}(x,y,w)$ количественно определяет, насколько мы недовольны весами $w$ модели в задаче прогнозирования выхода $y$ по известному входу $x$. Это количество, которое мы хотим минимизировать во время тренировочного процесса.</p><p>
<br>
</p><p><span class="new-item item-r">Случай классификации</span> Классификация выборки $x$ истинной метки $y\in \{-1,+1\}$ с линейной моделью весов $w$ может быть выполнена с помощью предиктора $f_w(x) \triangleq \textrm{sign}(s(x,w))$. В этой ситуации интересующий показатель, определяющий качество классификации, задается зазором $m(x,y,w)$ и может использоваться со следующими функциями потерь:</p>
<div class=mobile-container>
  <table style="table-layout:fixed; width:100%; min-width:725px; max-width:800px;">
    <colgroup>
      <col style=width:125px>
      <col style=width:33%>
      <col style=width:33%>
      <col style=width:33%>
    </colgroup>
<tbody>
<tr>
<td align=center><b>Название</b></td>
<td align=center>Zero-one loss</td>
<td align=center>Hinge loss</td>
<td align=center>Logistic loss</td>
</tr>
<tr>
<td align=center>$\textrm{Loss}(x,y,w)$</td>
<td align=center>$1_{\{m(x,y,w) \leqslant 0\}}$</td>
<td align=center>$\max(1-m(x,y,w), 0)$</td>
<td align=center>$\log(1+e^{-m(x,y,w)})$</td>
</tr>
<tr>
<td align=center><b>График</b></td>
<td align=center style=width:100%;max-width:150px;><img alt="Zero-one loss" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=loss-zero-one.png?f9d7b6349bfa78f70ba7aee99af5e0dc></td>
<td align=center style=width:100%;max-width:150px;><img alt="Hinge loss" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=loss-hinge.png?47a1e04a25970a1ad583d8821ba51ff2></td>
<td align=center style=width:100%;max-width:150px;><img alt="Logistic loss" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=loss-logistic.png?8891a0a53cb3999a126d3df87080e6e5></td>
</tr>
</tbody>
</table>
</div>
<br>
<p><span class="new-item item-b">Случай регрессии</span> Предсказание выборки $x$ истинной метки $y \in \mathbb{R}$ с помощью линейной модели весов $w$ может быть выполнено с помощью предиктора $f_w(x) \triangleq s(x,w)$. В этой ситуации интересующий показатель, количественно оценивающий качество регрессии, задается зазором $\textrm{res}(x,y,w)$ и может использоваться со следующими функциями потерь:</p>
<div class=mobile-container>
  <table style="table-layout:fixed; width:100%; min-width:525px; max-width:575px;">
    <colgroup>
      <col style=width:125px>
      <col style=width:50%>
      <col style=width:50%>
    </colgroup>
<tbody>
<tr>
<td align=center><b>Название</b></td>
<td align=center>Квадратичная потеря</td>
<td align=center>Абсолютное отклонение</td>
</tr>
<tr>
<td align=center>$\textrm{Loss}(x,y,w)$</td>
<td align=center>$(\textrm{res}(x,y,w))^2$</td>
<td align=center>$|\textrm{res}(x,y,w)|$</td>
</tr>
<tr>
<td align=center><b>График</b></td>
<td align=center style=width:100%;max-width:150px;><img alt="Squared loss" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=loss-squared.png?a1af36015fa59fa7f2e0cabeeee938a5></td>
<td align=center style=width:100%;max-width:150px;><img alt="Absolute deviation loss" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=loss-absolute-deviation.png?06b20ac340980e2a131f3de34e7647a3></td>
</tr>
</tbody>
</table>
</div>
<br>
<p><span class="new-item item-g">Фреймворк минимизации потерь</span> чтобы обучить модель, мы хотим минимизировать потери при обучении, которые определяются следующим образом:</p>
<div class=mobile-container>
\[\boxed{\textrm{TrainLoss}(w)=\frac{1}{|\mathcal{D}_{\textrm{train}}|}\sum_{(x,y)\in\mathcal{D}_{\textrm{train}}}\textrm{Loss}(x,y,w)}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#non-linear-predictors id=non-linear-predictors></a>Нелинейные предсказатели</h2>
<p><span class="new-item item-g">Алгоритм $k$-ближайших соседей</span> широко известен как $k$-NN, представляет собой непараметрический подход, в котором метка новой точки данных определяется признаками её $k$ соседей из обучающего набора. Его можно использовать в случаях как классификации, так и регрессии.</p>
<div class=mobile-container>
<center>
<img alt="k nearest neighbors" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=k-nearest-neighbors.png?02f80a524bb11e2b7a70b58c9ed3b0f4 style=width:100%;max-width:740px>
</center>
</div>
<p><span class=remark>Примечание: чем выше параметр $k$, тем выше смещение, а чем ниже параметр $k$, тем выше дисперсия.</span></p>
<br>
<p><span class="new-item item-r">Нейронные сети</span> Нейронные сети - это класс моделей, построенных с использованием слоёв. Обычно используемые типы нейронных сетей включают сверточные и рекуррентные нейронные сети. Словарь архитектур нейронных сетей представлен на рисунке ниже:</p>
<div class=mobile-container>
<center>
<img alt="Neural network" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=neural-network-en.png?835862d448ad85bc5a038848d7d7df0b style=width:100%;max-width:600px>
</center>
</div>
<p>Обозначим $i$ - это $i$-й уровень сети, а $j$ - $j$-й скрытый блок слоя, у нас есть:</p>
<div class=mobile-container>
\[\boxed{z_j^{[i]}={w_j^{[i]}}^Tx+b_j^{[i]}}\]
</div>
<p>где мы обозначаем $w$, $b$, $x$, $z$ вес, смещение, вход и неактивированный выход нейрона соответственно.</p>
<br>
<div class="alert alert-warning" role=alert>Для более подробного обзора приведенных выше концепций ознакомьтесь с <a class=alert-link nethref=teaching/cs-229/cheatsheet-supervised-learning  href=../../cs-229/supervised-learning/index.html onclick=trackOutboundLink(this);>Шпаргалками по контролируемому обучению</a>!</div>
<h2><a aria-hidden=true class=anchor href=#stochastic-gradient-descent id=stochastic-gradient-descent></a>Стохастический градиентный спуск</h2>
<p><span class="new-item item-g">Градиентный спуск</span> Gradient descent - Обозначим $\eta\in\mathbb{R}$ скорость обучения (также называемую размером шага), правило обновления для градиентного спуска выражается с помощью скорости обучения и функции потерь $\textrm{Loss}(x,y,w)$ следующим образом:</p>
<div class=mobile-container>
\[\boxed{w\longleftarrow w-\eta\nabla_w \textrm{Loss}(x,y,w)}\]
</div>
<br>
<div class=mobile-container>
<center>
<img alt="Gradient descent" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=gradient-descent.png?7d7bdb054f6ff01cb4fc07dd4b33fa7d style=width:100%;max-width:500px>
</center>
</div>
<br>
<p><span class="new-item item-r">Стохастические обновления</span> Stochastic gradient descent (SGD) обновляет параметры модели по одному обучающему примеру $(\phi(x),y)\in\mathcal{D}_{\textrm{train}}$ за раз. Этот метод иногда приводит к шумным, но быстрым обновлениям.</p>
<br>
<p><span class="new-item item-b">Пакетные обновления</span> Batch gradient descent (BGD) обновляет параметры модели по одной партии примеров (например, половина обучающего набора) за раз. Этот метод вычисляет стабильные направления обновления с большими вычислительными затратами.</p>
<br>
<h2><a aria-hidden=true class=anchor href=#fine-tuning-models id=fine-tuning-models></a>Дообучение моделей</h2>
<p><span class="new-item item-g">Класс гипотез</span> Класс гипотез $\mathcal{F}$ - это набор возможных предикторов с фиксированным $\phi(x)$ и изменяющимся $w$:</p>
<div class=mobile-container>
\[\boxed{\mathcal{F}=\left\{f_w:w\in\mathbb{R}^d\right\}}\]
</div>
<br>
<p><span class="new-item item-r">Логистическая функция</span> Логистическая функция $\sigma$, также называемая сигмовидной функцией, определяется как:</p>
<div class=mobile-container>
\[\boxed{\forall z\in]-\infty,+\infty[,\quad\sigma(z)=\frac{1}{1+e^{-z}}}\]
</div>
<p><span class=remark>Примечание: у нас есть $\sigma'(z)=\sigma(z)(1-\sigma(z))$.</span></p>
<br>
<p><span class="new-item item-b">Обратное распространение ошибки</span> Backpropagation (Backprop) - Прямой проход сети выполняется через $f_i$, которое является значением подвыражения с индексом $i$, а обратный проход выполняется через $g_i=\frac{\partial\textrm{out}}{\partial f_i}$ отражает то, как сильно $f_i$ влияет на выход.</p>
<div class=mobile-container>
<center>
<img alt=Backpropagation class=img-responsive netsrc=teaching/cs-221/illustrations/ src=backpropagation.png?24671572de19eb4f92606f950c21409d style=width:100%;max-width:450px>
</center>
</div>
<br>
<p><span class="new-item item-g">Ошибка приближения и оценки</span> Ошибка аппроксимации $\epsilon_\text{approx}$ отражает то, насколько далеко весь класс гипотез $\mathcal{F}$ от целевого предиктора $g^*$, в то время как ошибка оценки $\epsilon_{\text{est}}$ количественно определяет, насколько хорош предиктор $\hat{f}$ по отношению к лучшему предиктору $f^{*}$ из класса гипотез $\mathcal{F}$.</p>
<div class=mobile-container>
<center>
<img alt="Approximation and estimation error" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=approximation-model.png?2598ff172e014fd68e0663c2703b69ef style=width:100%;max-width:450px>
</center>
</div>
<br>
<p><span class="new-item item-g">Регуляризация</span> Процедура регуляризации направлена на то, чтобы модель не переобучалась на данных (запоминала их полностью), и, таким образом, решает проблемы с высокой дисперсией. В следующей таблице суммированы различные типы широко используемых методов регуляризации:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:760px;">
  <colgroup>
    <col style=width:33%>
    <col style=width:33%>
    <col style=width:33%>
  </colgroup>
<tbody>
<tr>
<td align=center><b>LASSO</b></td>
<td align=center><b>Ridge</b></td>
<td align=center><b>Elastic Net</b></td>
</tr>
<tr>
<td align=left>• Уменьшает коэффициенты до 0<br>• Подходит для выбора переменных</td>
<td align=left>Делает коэффициенты меньше</td>
<td align=left>Компромисс между выбором переменных и небольшими коэффициентами</td>
</tr>
<tr>
<td align=center><img alt=Lasso class=img-responsive netsrc=teaching/cs-221/illustrations/ src=regularization-lasso.png?39ee3006c1ac3594160474a29df13d2b></td>
<td align=center><img alt=Ridge class=img-responsive netsrc=teaching/cs-221/illustrations/ src=regularization-ridge.png?229d65c13a8e0bfe4ac8cdcb04972052></td>
<td align=center><img alt="Elastic Net" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=regularization-elastic-net.png?854b4ab93db8881002a2f2ca72d536be></td>
</tr>
<tr>
<td align=left>$...+\lambda||w||_1$<br>$\lambda\in\mathbb{R}$</td>
<td align=left>$...+\lambda||w||_2^2$<br>$\lambda\in\mathbb{R}$</td>
<td align=left>$...+\lambda\Big[(1-\alpha)||w||_1+\alpha||w||_2^2\Big]$<br>$\lambda\in\mathbb{R},\alpha\in[0,1]$</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-r">Гиперпараметры</span> это свойства алгоритма обучения и включают функции, параметр регуляризации $\lambda$, количество итераций $T$, размер шага $\eta$ и так далее.</p>
<br>
<p><span class="new-item item-b">Наборы словарей</span> при выборе модели мы выделяем 3 разные части данных, которые у нас есть, а именно:</p>
<div class=mobile-container>
<center>
<table>
<colgroup><col width=33%>
<col width=33%>
<col width=33%>
</colgroup><tbody>
<tr>
<td align=center><b>Обучающий набор</b></td>
<td align=center><b>Контрольный набор</b></td>
<td align=center><b>Тестовый набор</b></td>
</tr>
<tr>
<td align=left>• Модель обучена<br>
                 • Обычно 80% набора данных</td>
<td align=left>• Модель оценена<br>
                 • Обычно 20% набора данных<br>
                 • Также называется отложенным или набором для разработки</td>
<td align=left>• Модель дает прогнозы<br>
                 • Ранее невиданные данные</td>
</tr>
</tbody>
</table>
</center>
</div>
<p>Как только модель выбрана, она обучается на всем наборе данных и тестируется на невиданном тестовом наборе. Они представлены на рисунке ниже:</p>
<div class=mobile-container>
<center>
<img alt="Partition of the dataset" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=train-val-test-en.png?0949795ac868562e193efdc249ae1066 style=width:100%;max-width:550px>
</center>
</div>
<br>
<div class="alert alert-warning" role=alert>Для более подробного обзора приведенных выше концепций ознакомьтесь с <a class=alert-link nethref=teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks href=../../cs-229/machine-learning-tips-and-tricks/index.html onclick=trackOutboundLink(this);>Шпаргалками с советами и приемами машинного обучения</a>!</div>
<h2><a aria-hidden=true class=anchor href=#unsupervised-learning id=unsupervised-learning></a>Обучение без учителя</h2>
<p>Класс методов обучения без учителя направлен на обнаружение структуры данных, которые могут иметь богатые скрытые структуры.</p>
<h3>$k$-means</h3>
<p><span class="new-item item-g">Кластеризация</span> Дан обучающий набор входных точек $\mathcal{D}_{\textrm{train}}$, цель алгоритма кластеризации состоит в том, чтобы определить каждую точку $\phi(x_i)$ к одному из кластеров $z_i\in\{1,...,k\}$.</p>
<br>
<p><span class="new-item item-r">Целевая функция</span> функция потерь для одного из основных алгоритмов кластеризации, $k$-средних, определяется выражением:</p>
<div class=mobile-container>
\[\boxed{\textrm{Loss}_{\textrm{k-means}}(x,\mu)=\sum_{i=1}^n||\phi(x_i)-\mu_{z_i}||^2}\]
</div>
<br>
<p><span class="new-item item-b">Алгоритм</span> после случайной инициализации центроидов кластера $\mu_1,\mu_2,...,\mu_k\in\mathbb{R}^d$ алгоритм $k$-средних повторяет следующий шаг до сходимости:</p>
<div class=mobile-container>
\[\boxed{z_i=\underset{j}{\textrm{arg min}}||\phi(x_i)-\mu_j||^2}\quad\textrm{и}\quad\boxed{\mu_j=\frac{\displaystyle\sum_{i=1}^n1_{\{z_i=j\}}\phi(x_i)}{\displaystyle\sum_{i=1}^n1_{\{z_i=j\}}}}\]
</div>
<center>
<img alt=Illustration class=img-responsive netsrc=teaching/cs-229/illustrations/ src=k-means-en.png?9925605d814ddadebcae2ae4754ab0a4>
</center>
<br>
<h3>Метод главных компонент - Principal Component Analysis (PCA)</h3>
<p><span class="new-item item-g">Собственное значение, собственный вектор</span> Для данной матрицы $A\in\mathbb{R}^{d\times d}$, $\lambda$ называется собственным значением $A$, если существует вектор $z\in\mathbb{R}^d\backslash\{0\}$, называемый собственным вектором, такой, что у нас есть:</p>
<div class=mobile-container>
\[\boxed{Az=\lambda z}\]
</div>
<br>
<p><span class="new-item item-g">Спектральная теорема</span> Пусть $A\in\mathbb{R}^{d\times d}$. Если $A$ симметрична, то $A$ диагонализуема действительной ортогональной матрицей $U\in\mathbb{R}^{d\times d}$. Обозначим $\Lambda=\textrm{diag}(\lambda_1,...,\lambda_d)$, у нас есть:</p>
<div class=mobile-container>
\[\boxed{\exists\Lambda\textrm{ diagonal},\quad A=U\Lambda U^T}\]
</div>
<br>
<p><span class=remark>Примечание: собственный вектор, связанный с наибольшим собственным значением, называется главным собственным вектором матрицы $A$. (примечание переводчика: Смотри минимизацию нормы Фробениуса матрицы ошибок)</span></p>
<br>
<p><span class="new-item item-r">Алгоритм</span> процедура метода главных компонент - это метод уменьшения размерности, который проецирует данные по $k$ измерениям, максимизируя дисперсию данных следующим образом:
<br>- <u>Шаг 1</u>: Нормализовать данные, чтобы получить среднее значение 0 и стандартное отклонение 1.
</p><div class=mobile-container>
\[\boxed{\phi_j(x_i)\leftarrow\frac{\phi_j(x_i)-\mu_j}{\sigma_j}}\quad\textrm{где}\quad\boxed{\mu_j = \frac{1}{n}\sum_{i=1}^n\phi_j(x_i)}\quad\textrm{и}\quad\boxed{\sigma_j^2=\frac{1}{n}\sum_{i=1}^n(\phi_j(x_i)-\mu_j)^2}\]
</div>
<br>- <u>Шаг 2</u>: Вычислить $\displaystyle\Sigma=\frac{1}{n}\sum_{i=1}^n\phi(x_i){\phi(x_i)}^T\in\mathbb{R}^{d\times d}$, которая симметрична действительным собственным значениям.
<br>- <u>Шаг 3</u>: Вычислить $u_1, ..., u_k\in\mathbb{R}^d$ $k$ ортогональных главных собственных векторов $\Sigma$, т.е. ортогональные собственные векторы $k$ наибольших собственных значений.
<br>- <u>Шаг 4</u>: Спроецировать данные на $\textrm{span}_\mathbb{R}(u_1,...,u_k)$.<p></p>
<p>Эта процедура максимизирует дисперсию всех $k$-мерных пространств.</p>
<center>
<img alt=Illustration class=img-responsive netsrc=teaching/cs-229/illustrations/ src=pca-en.png?4be4617788fd40f4e998b530b75f4149>
</center>
<br>
<div class="alert alert-warning" role=alert>Для более подробного обзора приведенных выше концепций ознакомьтесь с <a class=alert-link nethref=teaching/cs-229/cheatsheet-unsupervised-learning href=../../cs-229/unsupervised-learning/index.html onclick=trackOutboundLink(this);>Шпаргалками по Обучению без учителя</a></div>
</article> </div> <!-- FOOTER <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-twitter fa-3x fa-fw"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class="fa fa-linkedin fa-3x fa-fw"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-github fa-3x fa-fw"></i></a> <a href="https://scholar.google.com/citations?user=nMnMTm8AAAAJ" onclick=trackOutboundLink(this);><i class="fa fa-google fa-3x fa-fw"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick="trackOutboundLink(this); window.location.href = 'mailto:' + this.dataset.name + '@' + this.dataset.domain + '.' + this.dataset.tld"><i class="fa fa-envelope fa-3x fa-fw"></i></a> </div> </div> </footer> --> </body></html>
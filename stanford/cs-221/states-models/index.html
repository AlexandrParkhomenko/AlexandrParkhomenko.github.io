<!DOCTYPE html><html lang=en><head><!-- base href=../../ --><title>CS 221 - States-based Models Cheatsheet</title><meta charset=utf-8><meta content="Teaching page of Shervine Amidi, Graduate Student at Stanford University." name=description><meta content="teaching, shervine, shervine amidi, data science" name=keywords><meta content="width=device-width, initial-scale=1" name=viewport>
<link href=https://stanford.edu/~shervine/teaching/cs-221/cheatsheet-states-models rel=canonical>
<link href=../../css/bootstrap.min.css rel=stylesheet>
<link href=../../css/font-awesome.min.css rel=stylesheet>
<link href=../../css/katex.min.css rel=stylesheet>
<link href=../../css/style.min.css rel=stylesheet type=text/css>
<link href=../../css/article.min.css rel=stylesheet>
<script src=../../js/jquery.min.js></script>
<script src=../../js/bootstrap.min.js></script>
<script defer src=../../js/underscore-min.js type=text/javascript></script>
<script defer src=../../js/katex.min.js></script>
<script defer src=../../js/auto-render.min.js></script>
<script defer src=../../js/article.min.js></script>
<script defer src=../../js/lang.min.js></script>
<script async defer src=../../js/buttons.js></script>
</head> <body data-offset=50 data-spy=scroll data-target=.navbar> <!-- HEADER <nav class="navbar navbar-inverse navbar-static-top"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class="collapse navbar-collapse" id=myNavbar> <ul class="nav navbar-nav"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class="nav navbar-nav navbar-center"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class="collapse navbar-collapse" data-target=None id=HiddenNavbar> <ul class="nav navbar-nav navbar-right"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style="padding: 0px;"> <img alt=MIT netsrc=images/ src=../../images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style="padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;"> </a> </ul> </div> </div> </div> </nav> --> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title style=display:none;> <!-- DISPLAY:NONE --> <a href=teaching/cs-221 onclick=trackOutboundLink(this);><img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48 style="width: 12px;">   <b>CS 221 - Artificial Intelligence</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>States-based models</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#tree-search>Поиск по дереву</a></div> <div class=dropdown-container> <a href=#tree-search><span>Поиск с возвратом</span></a> <a href=#tree-search><span>Поиск в ширину</span></a> <a href=#tree-search><span>Поиск в глубину</span></a> <a href=#tree-search><span>С итеративным углублением</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#graph-search>Поиск по графу</a></div> <div class=dropdown-container> <a href=#graph-search><span>Динамическое программирование</span></a> <a href=#graph-search><span>Поиск по единой стоимости</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#learning-costs>Стоимость обучения</a></div> <div class=dropdown-container> <a href=#learning-costs><span>Структурированный перцептрон</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#a-star>A со звездой поиск</a></div> <div class=dropdown-container> <a href=#a-star><span>Эвристическая функция</span></a> <a href=#a-star><span>Алгоритм</span></a> <a href=#a-star><span>Consistency, correctness</span></a> <a href=#a-star><span>Admissibility, efficiency</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#relaxation>Ослабление ограничений</a></div> <div class=dropdown-container> <a href=#relaxation><span>Задача с ослабленными ограничениями поиска</span></a> <a href=#relaxation><span>Ослабленная эвристика</span></a> <a href=#relaxation><span>Max эвристика</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#markov-decision-processes>Марковские процессы принятия решений</a></div> <div class=dropdown-container> <a href=#markov-decision-processes><span>Обзор</span></a> <a href=#markov-decision-processes><span>Оценка политики</span></a> <a href=#markov-decision-processes><span>Итерация ценности</span></a> <a href=#markov-decision-processes><span>Transitions, rewards</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#game-playing>Игровой процесс</a></div> <div class=dropdown-container> <a href=#game-playing><span>Expectimax</span></a> <a href=#game-playing><span>Minimax</span></a> <a href=#game-playing><span>Ускорение minimax</span></a> <a href=#game-playing><span>Одновременные игры</span></a> <a href=#game-playing><span>Игры с ненулевой суммой</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-221-artificial-intelligence/blob/master/en/cheatsheet-states-models.pdf onclick=trackOutboundLink(this); style="color: white; text-decoration:none;"> <i aria-hidden=false class="fa fa-github fa-fw"></i> Посмотреть PDF-версию на GitHub </a> </li> </div> </center> </div> <article class="markdown-body entry-content" itemprop=text>
<div class="alert alert-primary" role=alert style=display:none;> <!-- DISPLAY:NONE -->
  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!
</div>
<div class=title-lang style=display:none;> <!-- DISPLAY:NONE -->
  <a aria-hidden=true class=anchor-bis href=#cs-221---artificial-intelligence id=cs-221---artificial-intelligence></a><a href=teaching/cs-221 onclick=trackOutboundLink(this);>CS 221 - Artificial Intelligence</a>
  
  <div style=float:right;display:none;> <!-- DISPLAY:NONE -->
    <div class=input-group>
      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>
        <option selected value=en>English</option>
        <option value=fr>Français</option>
        <option value=tr>Türkçe</option>
      </select>
      <div class=input-group-addon><i class=fa></i></div>
    </div>
  </div>
</div>
<br>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button><B>CS 221 - Artificial Intelligence</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button>CS 229 - Machine Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button>CS 230 - Deep Learning</button>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button>Reflex</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-221/states-models/index.html'; oldhref='teaching/cs-221/cheatsheet-states-models'" type=button><B>States</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/variables-models/index.html'; oldhref='teaching/cs-221/cheatsheet-variables-models'" type=button>Variables</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/logic-models/index.html'; oldhref='teaching/cs-221/cheatsheet-logic-models'" type=button>Logic</button>
  </div>
</div>
<h1>
  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>Модели на основе состояний с поисковой оптимизацией и Марковским процессом принятия решений (MDP)
</h1>
<i>By <a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> and <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a></i>
  <div style=float:right;display:none;> <!-- DISPLAY:NONE --><a aria-label="Star afshinea/stanford-cs-221-artificial-intelligence on GitHub" class="github-button fa-fw" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-221-artificial-intelligence onclick=trackOutboundLink(this);>Star</a></div>
<h2>Оптимизация поиска</h2>
<p>In this section, we assume that by accomplishing action $a$ from state $s$, we deterministically arrive in state $\textrm{Succ}(s,a)$. The goal here is to determine a sequence of actions $(a_1,a_2,a_3,a_4,...)$ that starts from an initial state and leads to an end state. In order to solve this kind of problem, our objective will be to find the minimum cost path by using states-based models.</p>
<h3><a aria-hidden=true class=anchor href=#tree-search id=ts></a>Поиск по дереву</h3>
<p>Эта категория алгоритмов на основе состояний исследует все возможные состояния и действия. Он довольно эффективен с точки зрения памяти и подходит для огромных пространств состояний, но в худших случаях время выполнения может стать экспоненциальным.</p>
<div class=mobile-container>
<center>
  <img alt=Tree class=img-responsive netsrc=teaching/cs-221/illustrations/ src=tree.png?2d18a75bbffc54c7db56dd83aed1e224 style=width:100%;max-width:900px>
</center>
</div>
<br>
<p><span class="new-item item-g">Задача поиска</span> определяется с помощью:
</p><ul>
  <li>a starting state $s_{\textrm{start}}$
  </li><li>possible actions $\textrm{Actions}(s)$ from state $s$
  </li><li>action cost $\textrm{Cost}(s,a)$ from state $s$ with action $a$
  </li><li>successor $\textrm{Succ}(s,a)$ of state $s$ after action $a$
  </li><li>whether an end state was reached $\textrm{IsEnd}(s)$
</li></ul>
<p></p>
<div class=mobile-container>
<center>
  <img alt="Search problem" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=search-problem.png?5b6272e510ca6d44c87c01a68ccaca3a style=width:100%;max-width:465px>
</center>
</div>
<p>Цель состоит в том, чтобы найти путь, который минимизирует затраты.</p>
<br>
<p><span class="new-item item-r">Backtracking</span> Поиск с возвратом - это наивный рекурсивный алгоритм, который пробует все возможности, чтобы найти путь с минимальной стоимостью. Здесь затраты на действия могут быть как положительными, так и отрицательными.</p>
<br>
<p><span class="new-item item-b">Breadth-first search (BFS)</span> Breadth-first search is a graph search algorithm that does a level-by-level traversal. We can implement it iteratively with the help of a queue that stores at each step future nodes to be visited. For this algorithm, we can assume action costs to be equal to a constant $c\geqslant0$.</p>
<div class=mobile-container>
<center>
  <img alt=BFS class=img-responsive netsrc=teaching/cs-221/illustrations/ src=bfs-stack.png?28d6deef49cde3d173b8dc167e28ad4a style=width:100%;max-width:420px>
</center>
</div>
<br>
<p><span class="new-item item-b">Depth-first search (DFS)</span> Поиск в глубину - это алгоритм поиска, который просматривает граф, прослеживая каждый путь как можно глубже. Мы можем реализовать это рекурсивно или итеративно с помощью стека, в котором на каждом шаге хранятся будущие узлы, которые необходимо посетить. Для этого алгоритма предполагается, что затраты на действия равны 0.</p>
<div class=mobile-container>
<center>
  <img alt=DFS class=img-responsive netsrc=teaching/cs-221/illustrations/ src=dfs-stack.png?5616ba59bd361e621870e036829b95d9 style=width:100%;max-width:420px>
</center>
</div>
<br>
<p><span class="new-item item-b">Iterative deepening</span> The iterative deepening trick is a modification of the depth-first search algorithm so that it stops after reaching a certain depth, which guarantees optimality when all action costs are equal. Here, we assume that action costs are equal to a constant $c\geqslant0$.</p>
<br>
<p><span class="new-item item-r">Сводка алгоритмов поиска по дереву</span> отметив $b$ количество действий на состояние, $d$ глубину решения и $D$ максимальную глубину, которую мы имеем:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:500px;">
<colgroup>
<col style=width:35%>
<col style=width:21.75%>
<col style=width:21.75%>
<col style=width:21.75%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Алгоритм</b></td>
<td align=center><b>Стоимость действий</b></td>
<td align=center><b>Пространство</b></td>
<td align=center><b>Время</b></td>
</tr>
<tr>
<td align=center valign=top>Backtracking search</td>
<td align=center valign=top>any</td>
<td align=center valign=top>$\mathcal{O}(D)$</td>
<td align=center valign=top>$\mathcal{O}(b^D)$</td>
</tr>
<tr>
<td align=center valign=top>Breadth-first search</td>
<td align=center valign=top>$c\geqslant0$</td>
<td align=center valign=top>$\mathcal{O}(b^d)$</td>
<td align=center valign=top>$\mathcal{O}(b^d)$</td>
</tr>
<tr>
<td align=center valign=top>Depth-first search</td>
<td align=center valign=top>0</td>
<td align=center valign=top>$\mathcal{O}(D)$</td>
<td align=center valign=top>$\mathcal{O}(b^D)$</td>
</tr>
<tr>
<td align=center valign=top>DFS-Iterative deepening</td>
<td align=center valign=top>$c\geqslant0$</td>
<td align=center valign=top>$\mathcal{O}(d)$</td>
<td align=center valign=top>$\mathcal{O}(b^d)$</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<h3><a aria-hidden=true class=anchor href=#graph-search id=graph-search></a>Поиск по графу</h3>
<p>Эта категория алгоритмов на основе состояний направлена на построение оптимальных путей, обеспечивающих экспоненциальную экономию. В этом разделе мы сосредоточимся на динамическом программировании и поиске по единой стоимости.</p>
<p><span class="new-item item-g">Graph</span> A graph is comprised of a set of vertices $V$ (also called nodes) as well as a set of edges $E$ (also called links).</p>
<div class=mobile-container>
<center>
  <img alt=Graph class=img-responsive netsrc=teaching/cs-221/illustrations/ src=graph.png?76351911a7eff56d9f7de999bd7997ab style=width:100%;max-width:450px>
</center>
</div>
<p><span class=remark>Примечание: граф называется ациклическим, когда нет цикла.</span></p>
<br>
<p><span class="new-item item-r">Состояние</span> это совокупность всех прошлых действий, достаточная для оптимального выбора будущих действий.</p>
<br>
<p><span class="new-item item-b">Dynamic programming</span> Dynamic programming (DP) is a backtracking search algorithm with memoization (i.e. partial results are saved) whose goal is to find a minimum cost path from state $s$ to an end state $s_\textrm{end}$. It can potentially have exponential savings compared to traditional graph search algorithms, and has the property to only work for acyclic graphs. For any given state $s$, the future cost is computed as follows:</p>
<div class=mobile-container>
\[\boxed{\textrm{FutureCost}(s)=\left\{\begin{array}{lc}0 &amp; \textrm{if IsEnd}(s)\\\underset{a\in\textrm{Actions}(s)}{\textrm{min}}\big[\textrm{Cost}(s,a)+\textrm{FutureCost(Succ}(s,a))\big] &amp; \textrm{otherwise}\end{array}\right.}\]
</div>
<div class=mobile-container>
<center>
  <img alt="Dynamic Programming" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=dynamic-programming.png?6efc6c44c8c7bce9c2c576ecb82ec98c style=width:100%;max-width:450px>
</center>
</div>
<p><span class=remark>Примечание: на рисунке выше показан подход снизу вверх, тогда как формула дает интуитивное представление о решении проблемы сверху вниз.</span></p>
<br>
<p><span class="new-item item-g">Типы состояний</span> В таблице ниже представлена терминология, когда речь идет о состояниях в контексте поиска по единой стоимости:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:350px;">
<colgroup>
<col style=width:140px>
<col style=width:100%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Состояние</b></td>
<td align=center><b>Объяснение</b></td>
</tr>
<tr>
<td align=center valign=center>Explored $\mathcal{E}$</td>
<td align=left valign=top>Состояния c уже найденным оптимальным путем</td>
</tr>
<tr>
<td align=center valign=center>Frontier $\mathcal{F}$</td>
<td align=left valign=top>Видны состояния (нам неизвестен путь к ним с самой низкой ценой)</td>
</tr>
<tr>
<td align=center valign=center>Unexplored $\mathcal{U}$</td>
<td align=left valign=top>Состояний пока не видно</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-g">Uniform cost search</span> Uniform cost search (UCS) is a search algorithm that aims at finding the shortest path from a state $s_\textrm{start}$ to an end state $s_\textrm{end}$. It explores states $s$ in increasing order of $\textrm{PastCost}(s)$ and relies on the fact that all action costs are non-negative.</p>
<div class=mobile-container>
<center>
  <img alt="Uniform Cost Search" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=ucs-example.png?e565704b2370d49fa14f56f8259d3515 style=width:100%;max-width:450px>
</center>
</div>
<p><span class=remark>Замечание 1: алгоритм UCS логически эквивалентен алгоритму Дейкстры.</span></p>
<p><span class=remark>Замечание 2: алгоритм не будет работать для задачи с отрицательными затратами действий, и добавление положительной константы, чтобы сделать их неотрицательными, не решит проблему, так как в конечном итоге это будет другой проблемой.</span></p>
<br>
<p><span class="new-item item-r">Correctness theorem</span> When a state $s$ is popped from the frontier $\mathcal{F}$ and moved to explored set $\mathcal{E}$, its priority is equal to $\textrm{PastCost}(s)$ which is the minimum cost path from $s_\textrm{start}$ to $s$.</p>
<br>
<p><span class="new-item item-b">Graph search algorithms summary</span> By noting $N$ the number of total states, $n$ of which are explored before the end state $s_\textrm{end}$, we have:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:500px;">
<colgroup>
<col style=width:35%>
<col style=width:21.75%>
<col style=width:21.75%>
<col style=width:21.75%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Алгоритм</b></td>
<td align=center><b>Ацикличность</b></td>
<td align=center><b>Стоимость</b></td>
<td align=center><b>Время/пространство</b></td>
</tr>
<tr>
<td align=center valign=top>Dynamic programming</td>
<td align=center valign=top>yes</td>
<td align=center valign=top>any</td>
<td align=center valign=top>$\mathcal{O}(N)$</td>
</tr>
<tr>
<td align=center valign=top>Uniform cost search</td>
<td align=center valign=top>no</td>
<td align=center valign=top>$c\geqslant0$</td>
<td align=center valign=top>$\mathcal{O}(n\log(n))$</td>
</tr>
</tbody>
</table>
</center>
</div>
<p><span class=remark>Примечание: обратный отсчет сложности предполагает, что количество возможных действий для каждого состояния будет постоянным.</span></p>
<br>
<h3><a aria-hidden=true class=anchor href=#learning-costs id=learning-costs></a>Стоимость обучения</h3>
<p>Suppose we are not given the values of $\textrm{Cost}(s,a)$, we want to estimate these quantities from a training set of minimizing-cost-path sequence of actions $(a_1, a_2, ..., a_k)$.</p>
<p><span class="new-item item-g">Структурированный перцептрон</span> Структурированный перцептрон - это алгоритм итеративного изучения стоимости каждой пары состояние-действие. На каждом шагу, it:
</p><ul>
<li> decreases the estimated cost of each state-action of the true minimizing path $y$ given by the training data,
</li><li> increases the estimated cost of each state-action of the current predicted path $y'$ inferred from the learned weights.
</li></ul><p></p>
<p><span class=remark>Remark: there are several versions of the algorithm, one of which simplifies the problem to only learning the cost of each action $a$, and the other parametrizes $\textrm{Cost}(s,a)$ to a feature vector of learnable weights.</span></p>
<br>
<h3><a aria-hidden=true class=anchor href=#a-star id=a-star></a>$\textrm{A}^{\star}$ search</h3>
<p><span class="new-item item-g">Heuristic function</span> A heuristic is a function $h$ over states $s$, where each $h(s)$ aims at estimating $\textrm{FutureCost}(s)$, the cost of the path from $s$ to $s_\textrm{end}$.</p>
<div class=mobile-container>
<center>
  <img alt="A star" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=a-star.png?e4f6b57d6b77c9ecf9bdd5d7d491e703 style=width:100%;max-width:465px>
</center>
</div>
<br>
<p><span class="new-item item-r">Algorithm</span> $A^{*}$ is a search algorithm that aims at finding the shortest path from a state $s$ to an end state $s_\textrm{end}$. It explores states $s$ in increasing order of $\textrm{PastCost}(s) + h(s)$. It is equivalent to a uniform cost search with edge costs $\textrm{Cost}'(s,a)$ given by:</p>
<div class=mobile-container>
\[\boxed{\textrm{Cost}'(s,a)=\textrm{Cost}(s,a)+h(\textrm{Succ}(s,a))-h(s)}\]
</div>
<p><span class=remark>Примечание: этот алгоритм можно рассматривать как предвзятую версию исследования состояний UCS, которые оцениваются как более близкие к конечному состоянию.</span></p>
<br>
<p><span class="new-item item-b">Консистентность</span> Эвристика $h$ называется согласованной (консистентной) при удовлетворении двух следующих свойств:
</p><ul>
<li>For all states $s$ and actions $a$,
<div class=mobile-container>
\[\boxed{h(s) \leqslant \textrm{Cost}(s,a)+h(\textrm{Succ}(s,a))}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Consistency class=img-responsive netsrc=teaching/cs-221/illustrations/ src=consistency-1.png?4cbccf6f546df017acee0dc45d740b92 style=width:100%;max-width:465px>
</center>
</div>
</li><li>Конечное состояние подтверждает следующее:
<div class=mobile-container>
\[\boxed{h(s_{\textrm{end}})=0}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Consistency class=img-responsive netsrc=teaching/cs-221/illustrations/ src=consistency-2.png?539ae50a1bbf1b85bf3f8d0b32c48445 style=width:100%;max-width:203px>
</center>
</div>
</li></ul>
<p></p>
<br>
<p><span class="new-item item-r">Correctness</span> If $h$ is consistent, then $A^*$ returns the minimum cost path.</p>
<br>
<p><span class="new-item item-g">Допустимость</span> Говорят, что эвристика $h$ допустима, если у нас есть:</p>
<div class=mobile-container>
\[\boxed{h(s)\leqslant\textrm{FutureCost}(s)}\]
</div>
<br>
<p><span class="new-item item-r">Теорема</span> Пусть $h(s)$ - заданная эвристика. У нас есть:</p>
<div class=mobile-container>
\[\boxed{h(s)\textrm{ consistent}\Longrightarrow h(s)\textrm{ admissible}}\]
</div>
<br>
<p><span class="new-item item-r">Efficiency</span> $A^*$ explores all states $s$ satisfying the following equation:</p>
<div class=mobile-container>
\[\boxed{\textrm{PastCost}(s)\leqslant\textrm{PastCost}(s_{\textrm{end}})-h(s)}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Efficiency class=img-responsive netsrc=teaching/cs-221/illustrations/ src=efficiency.png?6306c2c390e4b08c0fe37bc99e6caf5d style=width:100%;max-width:465px>
</center>
</div>
<p><span class=remark>Примечание: большие значения $h(s)$ лучше, поскольку это уравнение показывает, что оно ограничивает набор состояний $s$, которые будут исследованы.</span></p>
<br>
<h3><a aria-hidden=true class=anchor href=#relaxation id=relaxation></a>Ослабление ограничений</h3>
<p>Это основа для создания последовательной эвристики. Идея состоит в том, чтобы найти сокращенные затраты в аналитическом виде, удалив ограничения и используя их в качестве эвристики.</p>
<br>
<p><span class="new-item item-g">Relaxed search problem</span> The relaxation of search problem $P$ with costs $\textrm{Cost}$ is denoted $P_{\textrm{rel}}$ with costs $\textrm{Cost}_{\textrm{rel}}$, and satisfies the identity:</p>
<div class=mobile-container>
\[\boxed{\textrm{Cost}_{\textrm{rel}}(s,a)\leqslant\textrm{Cost}(s,a)}\]
</div>
<br>
<p><span class="new-item item-r">Relaxed heuristic</span> Given a relaxed search problem $P_{\textrm{rel}}$, we define the relaxed heuristic $h(s)=\textrm{FutureCost}_{\textrm{rel}}(s)$ as the minimum cost path from $s$ to an end state in the graph of costs $\textrm{Cost}_{\textrm{rel}}(s,a)$.</p>
<br>
<p><span class="new-item item-b">Consistency of relaxed heuristics</span> Let $P_{\textrm{rel}}$ be a given relaxed problem. By theorem, we have:</p>
<div class=mobile-container>
\[\boxed{h(s)=\textrm{FutureCost}_{\textrm{rel}}(s)\Longrightarrow h(s)\textrm{ consistent}}\]
</div>
<br>
<p><span class="new-item item-g">Компромисс при выборе эвристики</span> При выборе эвристики необходимо уравновесить два аспекта:
</p><ul>
<li>Computational efficiency: $h(s)=\textrm{FutureCost}_{\textrm{rel}}(s)$ must be easy to compute. It has to produce a closed form, более легкий поиск и независимые подзадачи.
</li><li>Good enough approximation: the heuristic $h(s)$ should be close to $\textrm{FutureCost}(s)$ and we have thus to not remove too many constraints.
</li></ul><p></p>
<br>
<p><span class="new-item item-g">Max heuristic</span> Let $h_1(s)$, $h_2(s)$ be two heuristics. We have the following property:</p>
<div class=mobile-container>
\[\boxed{h_1(s),\textrm{ }h_2(s)\textrm{ consistent}\Longrightarrow h(s)=\max\{h_1(s),\textrm{ }h_2(s)\}\textrm{ consistent}}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#markov-decision-processes id=markov-decision-processes></a>Марковские процессы принятия решений</h2>
<p>In this section, we assume that performing action $a$ from state $s$ can lead to several states $s_1',s_2',...$ in a probabilistic manner. In order to find our way between an initial state and an end state, our objective will be to find the maximum value policy by using Markov decision processes that help us cope with randomness and uncertainty.</p>
<h3>Обозначения</h3>
<p><span class="new-item item-b">Определение</span> Цель марковского процесса принятия решений - максимизировать вознаграждение. Он определяется с помощью:
  </p><ul>
    <li>a starting state $s_{\textrm{start}}$
    </li><li>possible actions $\textrm{Actions}(s)$ from state $s$
    </li><li>transition probabilities $T(s,a,s')$ from $s$ to $s'$ with action $a$
    </li><li>rewards $\textrm{Reward}(s,a,s')$ from $s$ to $s'$ with action $a$
    </li><li>whether an end state was reached $\textrm{IsEnd}(s)$
    </li><li>a discount factor $0\leqslant\gamma\leqslant1$
  </li></ul>
<p></p>
<div class=mobile-container>
<center>
  <img alt="Markov Decision Process" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=markov-decision-process.png?bdaeab5d6b9b89652d54601199c83c85 style=width:100%;max-width:465px>
</center>
</div>
<br>
<p><span class="new-item item-r">Transition probabilities</span> The transition probability $T(s,a,s')$ specifies the probability of going to state $s'$ after action $a$ is taken in state $s$. Each $s' \mapsto T(s,a,s')$ is a probability distribution, which means that:</p>
<div class=mobile-container>
\[\forall s,a,\quad\boxed{\displaystyle\sum_{s'\in\textrm{ States}}T(s,a,s')=1}\]
</div>
<br>
<p><span class="new-item item-b">Policy</span> A policy $\pi$ is a function that maps each state $s$ to an action $a$, i.e.</p>
<div class=mobile-container>
\[\boxed{\pi : s \mapsto a}\]
</div>
<br>
<p><span class="new-item item-g">Utility</span> The utility of a path $(s_0, ..., s_k)$ is the discounted sum of the rewards on that path. In other words,</p>
<div class=mobile-container>
\[\boxed{u(s_0,...,s_k)=\sum_{i=1}^{k}r_i\gamma^{i-1}}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Utility class=img-responsive netsrc=teaching/cs-221/illustrations/ src=utility.png?858a1983bd7bb62a99dcbdad079e57c7 style=width:100%;max-width:465px>
</center>
</div>
<p><i>The figure above is an illustration of the case $k=4$.</i></p>
<br>
<p><span class="new-item item-g">Q-value</span> The $Q$-value of a policy $\pi$ at state $s$ with action $a$, also denoted $Q_{\pi}(s,a)$, is the expected utility from state $s$ after taking action $a$ and then following policy $\pi$. It is defined as follows:</p>
<div class=mobile-container>
\[\boxed{Q_{\pi}(s,a)=\sum_{s'\in\textrm{ States}}T(s,a,s')\left[\textrm{Reward}(s,a,s')+\gamma V_\pi(s')\right]}\]
</div>
<br>
<p><span class="new-item item-g">Value of a policy</span> The value of a policy $\pi$ from state $s$, also denoted $V_{\pi}(s)$, is the expected utility by following policy $\pi$ from state $s$ over random paths. It is defined as follows:</p>
<div class=mobile-container>
\[\boxed{V_\pi(s)=Q_\pi(s,\pi(s))}\]
</div>
<p><span class=remark>Remark: $V_\pi(s)$ is equal to 0 if $s$ is an end state.</span></p>
<br>
<h3>Приложения</h3>
<p><span class="new-item item-r">Policy evaluation</span> Given a policy $\pi$, policy evaluation is an iterative algorithm that aims at estimating $V_\pi$. It is done as follows:
</p><ul>
<li><u>Initialization</u>: for all states $s$, we have
<div class=mobile-container>
\[\boxed{V_\pi^{(0)}(s)\longleftarrow0}\]
</div>
</li><li><u>Iteration</u>: for $t$ from 1 to $T_{\textrm{PE}}$, we have
<div class=mobile-container>
\[\forall s,\quad\boxed{V_\pi^{(t)}(s)\longleftarrow Q_\pi^{(t-1)}(s,\pi(s))}\]
</div>
with
<div class=mobile-container>
\[\boxed{Q_\pi^{(t-1)}(s,\pi(s))=\sum_{s'\in\textrm{ States}}T(s,\pi(s),s')\Big[\textrm{Reward}(s,\pi(s),s')+\gamma V_\pi^{(t-1)}(s')\Big]}\]
</div>
</li></ul><p></p>
<p><span class=remark>Remark: by noting $S$ the number of states, $A$ the number of actions per state, $S'$ the number of successors and $T$ the number of iterations, then the time complexity is of $\mathcal{O}(T_{\textrm{PE}}SS')$.</span></p>
<br>
<p><span class="new-item item-b">Optimal Q-value</span> The optimal $Q$-value $Q_{\textrm{opt}}(s,a)$ of state $s$ with action $a$ is defined to be the maximum $Q$-value attained by any policy. It is computed as follows:</p>
<div class=mobile-container>
\[\boxed{Q_{\textrm{opt}}(s,a)=\sum_{s'\in\textrm{ States}}T(s,a,s')\left[\textrm{Reward}(s,a,s')+\gamma V_\textrm{opt}(s')\right]}\]
</div>
<br>
<p><span class="new-item item-b">Optimal value</span> The optimal value $V_{\textrm{opt}}(s)$ of state $s$ is defined as being the maximum value attained by any policy. It is computed as follows:</p>
<div class=mobile-container>
\[\boxed{V_{\textrm{opt}}(s)=\underset{a\in\textrm{ Actions}(s)}{\textrm{max}}Q_\textrm{opt}(s,a)}\]
</div>
<br>
<p><span class="new-item item-b">Optimal policy</span> The optimal policy $\pi_{\textrm{opt}}$ is defined as being the policy that leads to the optimal values. It is defined by:</p>
<div class=mobile-container>
\[\forall s,\quad\boxed{\pi_{\textrm{opt}}(s)=\underset{a\in\textrm{ Actions}(s)}{\textrm{argmax}}Q_\textrm{opt}(s,a)}\]
</div>
<br>
<p><span class="new-item item-r">Value iteration</span> Value iteration is an algorithm that finds the optimal value $V_{\textrm{opt}}$ as well as the optimal policy $\pi_{\textrm{opt}}$. It is done as follows:
</p><ul>
<li><u>Initialization</u>: for all states $s$, we have
<div class=mobile-container>
\[\boxed{V_{\textrm{opt}}^{(0)}(s)\longleftarrow0}\]
</div>
</li><li><u>Iteration</u>: for $t$ from 1 to $T_{\textrm{VI}}$, we have
<div class=mobile-container>
\[\forall s,\quad\boxed{V_\textrm{opt}^{(t)}(s)\longleftarrow \underset{a\in\textrm{ Actions}(s)}{\textrm{max}}Q_\textrm{opt}^{(t-1)}(s,a)}\]
</div>
with
<div class=mobile-container>
\[\boxed{Q_\textrm{opt}^{(t-1)}(s,a)=\sum_{s'\in\textrm{ States}}T(s,a,s')\Big[\textrm{Reward}(s,a,s')+\gamma V_\textrm{opt}^{(t-1)}(s')\Big]}\]
</div>
</li></ul><p></p>
<p><span class=remark>Remark: if we have either $\gamma &lt; 1$ or the MDP graph being acyclic, then the value iteration algorithm is guaranteed to converge to the correct answer.</span></p>
<br>
<h3>Когда неизвестны переходы и награды</h3>
<p>Теперь предположим, что вероятности перехода и награды неизвестны.</p>
<p><span class="new-item item-r">Model-based Monte Carlo</span> The model-based Monte Carlo method aims at estimating $T(s,a,s')$ and $\textrm{Reward}(s,a,s')$ using Monte Carlo simulation with:
</p><div class=mobile-container>
\[\boxed{\widehat{T}(s,a,s')=\frac{\#\textrm{ times }(s,a,s')\textrm{ occurs}}{\#\textrm{ times }(s,a)\textrm{ occurs}}}\]
</div>
and
<div class=mobile-container>
\[\boxed{\widehat{\textrm{Reward}}(s,a,s')=r\textrm{ in }(s,a,r,s')}\]
</div>
These estimations will be then used to deduce $Q$-values, including $Q_\pi$ and $Q_\textrm{opt}.$<p></p>
<p><span class=remark>Примечание: Основанный на модели Монте-Карло считается вне политики, потому что оценка не зависит от конкретной политики.</span></p>
<br>
<p><span class="new-item item-r">Model-free Monte Carlo</span> The model-free Monte Carlo method aims at directly estimating $Q_{\pi}$, as follows:
</p><div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)=\textrm{average of }u_t\textrm{ where }s_{t-1}=s, a_t=a}\]
</div>
where $u_t$ denotes the utility starting at step $t$ of a given episode.<p></p>
<p><span class=remark>Remark: model-free Monte Carlo is said to be on-policy, because the estimated value is dependent on the policy $\pi$ used to generate the data.</span></p>
<br>
<p><span class="new-item item-r">Equivalent formulation</span> By introducing the constant $\eta=\frac{1}{1+(\#\textrm{updates to }(s,a))}$ and for each $(s,a,u)$ of the training set, the update rule of model-free Monte Carlo has a convex combination formulation:
</p><div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)\leftarrow(1-\eta)\widehat{Q}_\pi(s,a)+\eta u}\]
</div>
а также формулировку стохастического градиента:
<div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)\leftarrow\widehat{Q}_\pi(s,a) - \eta (\widehat{Q}_\pi(s,a) - u)}\]
</div>
<p></p>
<br>
<p><span class="new-item item-g">SARSA</span> State-action-reward-state-action (SARSA) is a boostrapping method estimating $Q_\pi$ by using both raw data and estimates as part of the update rule. For each $(s,a,r,s',a')$, we have:</p>
<div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)\longleftarrow(1-\eta)\widehat{Q}_\pi(s,a)+\eta\Big[r+\gamma\widehat{Q}_\pi(s',a')\Big]}\]
</div>
<p><span class=remark>Примечание: оценка SARSA обновляется "на лету", в отличие от оценки Монте-Карло без использования модели, где оценка может быть обновлена только в конце эпизода.</span></p>
<br>
<p><span class="new-item item-b">Q-learning</span> $Q$-learning is an off-policy algorithm that produces an estimate for $Q_\textrm{opt}$. On each $(s,a,r,s',a')$, we have:</p>
<div class=mobile-container>
\[\boxed{\widehat{Q}_{\textrm{opt}}(s,a)\leftarrow(1-\eta)\widehat{Q}_{\textrm{opt}}(s,a)+\eta\Big[r+\gamma\underset{a'\in\textrm{ Actions}(s')}{\textrm{max}}\widehat{Q}_{\textrm{opt}}(s',a')\Big]}\]
</div>
<br>
<p><span class="new-item item-g">Epsilon-greedy</span> The epsilon-greedy policy is an algorithm that balances exploration with probability $\epsilon$ and exploitation with probability $1-\epsilon$. For a given state $s$, the policy $\pi_{\textrm{act}}$ is computed as follows:</p>
<div class=mobile-container>
\[\boxed{\pi_\textrm{act}(s)=\left\{\begin{array}{ll}\underset{a\in\textrm{ Actions}}{\textrm{argmax }}\widehat{Q}_\textrm{opt}(s,a) &amp; \textrm{with proba }1-\epsilon\\\textrm{random from Actions}(s) &amp; \textrm{with proba }\epsilon\end{array}\right.}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#game-playing id=game-playing></a>Игровой процесс</h2>
<p>В играх (например, шахматы, нарды, го) присутствуют другие агенты, которые необходимо учитывать при построении нашей политики.</p>
<p><span class="new-item item-b">Дерево игры</span> Дерево игры - это дерево, которое описывает возможности игры. В частности, каждый узел является точкой принятия решения для игрока, а каждый путь от корня к листу - это возможный результат игры.</p>
<br>
<p><span class="new-item item-b">Игра для двух игроков с нулевой суммой</span> это игра; в которой полностью соблюдается каждое состояние и игроки ходят по очереди. Она определяется с помощью:
</p><ul>
  <li>a starting state $s_{\textrm{start}}$
  </li><li>possible actions $\textrm{Actions}(s)$ from state $s$
  </li><li>successors $\textrm{Succ}(s,a)$ from states $s$ with actions $a$
  </li><li>whether an end state was reached $\textrm{IsEnd}(s)$
  </li><li>the agent's utility $\textrm{Utility}(s)$ at end state $s$
  </li><li>the player $\textrm{Player}(s)$ who controls state $s$
</li></ul><p></p>
<p><span class=remark>Примечание: предположим, что полезность агента противоположна полезности оппонента.</span></p>
<br>
<p><span class="new-item item-g">Типы политик</span> Есть два типа политик:
</p><ul>
<li>Deterministic policies, denoted $\pi_p(s)$, which are actions that player $p$ takes in state $s$.
</li><li>Stochastic policies, denoted $\pi_p(s,a)\in[0,1]$, which are probabilities that player $p$ takes action $a$ in state $s$.
</li></ul><p></p>
<br>
<p><span class="new-item item-g">Expectimax</span> For a given state $s$, the expectimax value $V_{\textrm{exptmax}}(s)$ is the maximum expected utility of any agent policy when playing with respect to a fixed and known opponent policy $\pi_{\textrm{opp}}$. It is computed as follows:</p>
<div class=mobile-container>
\[\boxed{V_{\textrm{exptmax}}(s)=\left\{\begin{array}{ll}\textrm{Utility}(s) &amp; \textrm{IsEnd}(s)\\\underset{a\in\textrm{Actions}(s)}{\textrm{max}}V_{\textrm{exptmax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{agent}\\\displaystyle\sum_{a\in\textrm{Actions}(s)}\pi_{\textrm{opp}}(s,a)V_{\textrm{exptmax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{opp}\end{array}\right.}\]
</div>
<p><span class=remark>Примечание: expectimax - аналог итерации значений для MDP.</span></p>
<div class=mobile-container>
<center>
  <img alt=Expectimax class=img-responsive netsrc=teaching/cs-221/illustrations/ src=expectimax.png?e45e88a1bab91386c3e1d4ff401a1d97 style=width:100%;max-width:600px>
</center>
</div>
<br>
<p><span class="new-item item-b">Minimax</span> Цель минимаксных политик - найти оптимальную политику против противника, предполагая наихудший случай, то есть то, что противник делает все, чтобы минимизировать полезность агента. Делается это следующим образом:</p>
<div class=mobile-container>
\[\boxed{V_{\textrm{minimax}}(s)=\left\{\begin{array}{ll}\textrm{Utility}(s) &amp; \textrm{IsEnd}(s)\\\underset{a\in\textrm{Actions}(s)}{\textrm{max}}V_{\textrm{minimax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{agent}\\\underset{a\in\textrm{Actions}(s)}{\textrm{min}}V_{\textrm{minimax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{opp}\end{array}\right.}\]
</div>
<p><span class=remark>Remark: we can extract $\pi_{\textrm{max}}$ and $\pi_{\textrm{min}}$ from the minimax value $V_{\textrm{minimax}}$.</span></p>
<div class=mobile-container>
<center>
  <img alt=Minimax class=img-responsive netsrc=teaching/cs-221/illustrations/ src=minimax.png?a10b34193ca7a93d1c7a764ee19de949 style=width:100%;max-width:600px>
</center>
</div>
<br>
<p><span class="new-item item-r">Minimax properties</span> By noting $V$ the value function, there are 3 properties around minimax to have in mind:
</p><ul>
<li><i>Property 1</i>: if the agent were to change its policy to any $\pi_{\textrm{agent}}$, then the agent would be no better off.
<div class=mobile-container>
\[\boxed{\forall \pi_{\textrm{agent}},\quad V(\pi_{\textrm{max}},\pi_{\textrm{min}})\geqslant V(\pi_{\textrm{agent}},\pi_{\textrm{min}})}\]
</div>
</li><li><i>Property 2</i>: if the opponent changes its policy from $\pi_{\textrm{min}}$ to $\pi_{\textrm{opp}}$, then he will be no better off.
<div class=mobile-container>
\[\boxed{\forall \pi_{\textrm{opp}},\quad V(\pi_{\textrm{max}},\pi_{\textrm{min}})\leqslant V(\pi_{\textrm{max}},\pi_{\textrm{opp}})}\]
</div>
</li><li><i>Property 3</i>: if the opponent is known to be not playing the adversarial policy, then the minimax policy might not be optimal for the agent.
<div class=mobile-container>
\[\boxed{\forall \pi,\quad V(\pi_{\textrm{max}},\pi)\leqslant V(\pi_{\textrm{exptmax}},\pi)}\]
</div>
</li></ul>
В итоге имеем следующие отношения:
<div class=mobile-container>
\[\boxed{V(\pi_{\textrm{exptmax}},\pi_{\textrm{min}})\leqslant V(\pi_{\textrm{max}},\pi_{\textrm{min}})\leqslant V(\pi_{\textrm{max}},\pi)\leqslant V(\pi_{\textrm{exptmax}},\pi)}\]
</div>
<p></p>
<br>
<h3>Ускорение minimax</h3>
<p><span class="new-item item-g">Evaluation function</span> An evaluation function is a domain-specific and approximate estimate of the value $V_{\textrm{minimax}}(s)$. It is denoted $\textrm{Eval}(s)$.</p>
<p><span class=remark>Remark: $\textrm{FutureCost}(s)$ is an analogy for search problems.</span></p>
<br>
<p><span class="new-item item-b">Alpha-beta pruning</span> Alpha-beta pruning is a domain-general exact method optimizing the minimax algorithm by avoiding the unnecessary exploration of parts of the game tree. To do so, each player keeps track of the best value they can hope for (stored in $\alpha$ for the maximizing player and in $\beta$ for the minimizing player). At a given step, the condition $\beta &lt; \alpha$ means that the optimal path is not going to be in the current branch as the earlier player had a better option at their disposal.</p>
<div class=mobile-container>
<center>
  <img alt="Alpha beta pruning" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=alpha-beta-pruning.png?30f2f19c5b0621ca55fcce4bdb6f914c style=width:100%;max-width:600px>
</center>
</div>
<br>
<p><span class="new-item item-b">TD learning</span> Temporal difference (TD) learning is used when we don't know the transitions/rewards. The value is based on exploration policy. To be able to use it, we need to know rules of the game $\textrm{Succ}(s,a)$. For each $(s,a,r,s')$, the update is done as follows:</p>
<div class=mobile-container>
\[\boxed{w\longleftarrow w-\eta\big[V(s,w)-(r+\gamma V(s',w))\big]\nabla_wV(s,w)}\]
</div>
<br>
<h3>Одновременные игры</h3>
<p>Это полная противоположность пошаговым играм, где нет упорядочивания ходов игрока.</p>
<br>
<p><span class="new-item item-g">Single-move simultaneous game</span> Let there be two players $A$ and $B$, with given possible actions. We note $V(a,b)$ to be $A$'s utility if $A$ chooses action $a$, $B$ chooses action $b$. $V$ is called the payoff matrix.</p>
<br>
<p><span class="new-item item-r">Стратегии</span> Есть два основных типа стратегий:
</p><ul>
<li>Чистая стратегия - это одно действие:
<div class=mobile-container>
\[\boxed{a\in\textrm{Actions}}\]
</div>
</li><li>Смешанная стратегия - это распределение вероятностей по действиям:
<div class=mobile-container>
\[\forall a\in\textrm{Actions},\quad\boxed{0\leqslant\pi(a)\leqslant1}\]
</div>
</li></ul><p></p>
<br>
<p><span class="new-item item-b">Game evaluation</span> The value of the game $V(\pi_A,\pi_B)$ when player $A$ follows $\pi_A$ and player $B$ follows $\pi_B$ is such that:</p>
<div class=mobile-container>
\[\boxed{V(\pi_A,\pi_B)=\sum_{a,b}\pi_A(a)\pi_B(b)V(a,b)}\]
</div>
<br>
<p><span class="new-item item-r">Minimax theorem</span> By noting $\pi_A,\pi_B$ ranging over mixed strategies, for every simultaneous two-player zero-sum game with a finite number of actions, we have:</p>
<div class=mobile-container>
\[\boxed{\max_{\pi_A}\min_{\pi_B}V(\pi_A,\pi_B)=\min_{\pi_B}\max_{\pi_A}V(\pi_A,\pi_B)}\]
</div>
<br>
<h3>Игры с ненулевой суммой</h3>
<p><span class="new-item item-b">Payoff matrix</span> We define $V_p(\pi_A,\pi_B)$ to be the utility for player $p$.</p>
<br>
<p><span class="new-item item-r">Nash equilibrium</span> A Nash equilibrium is $(\pi_A^*,\pi_B^*)$ such that no player has an incentive to change its strategy. We have:</p>
<div class=mobile-container>
\[\boxed{\forall \pi_A, V_A(\pi_A^*,\pi_B^*)\geqslant V_A(\pi_A,\pi_B^*)}\quad\textrm{and}\quad\boxed{\forall \pi_B, V_B(\pi_A^*,\pi_B^*)\geqslant V_B(\pi_A^*,\pi_B)}\]
</div>
<p><span class=remark>Примечание: в любой игре с конечным числом игроков и конечным числом действий существует хотя бы одно равновесие по Нэшу.</span></p>
</article> </div> <!-- FOOTER <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-twitter fa-3x fa-fw"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class="fa fa-linkedin fa-3x fa-fw"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-github fa-3x fa-fw"></i></a> <a href="https://scholar.google.com/citations?user=nMnMTm8AAAAJ" onclick=trackOutboundLink(this);><i class="fa fa-google fa-3x fa-fw"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick="trackOutboundLink(this); window.location.href = 'mailto:' + this.dataset.name + '@' + this.dataset.domain + '.' + this.dataset.tld"><i class="fa fa-envelope fa-3x fa-fw"></i></a> </div> </div> </footer> --> </body></html>
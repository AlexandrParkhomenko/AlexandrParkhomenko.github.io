<!DOCTYPE html><html lang=en><head><!-- base href=../../ --><title>CS 230 - Recurrent Neural Networks Cheatsheet</title><meta charset=utf-8><meta content="Teaching page of Shervine Amidi, Graduate Student at Stanford University." name=description><meta content="teaching, shervine, shervine amidi, data science" name=keywords><meta content="width=device-width, initial-scale=1" name=viewport>
<link href=https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks rel=canonical>
<link href=../../css/bootstrap.min.css rel=stylesheet>
<link href=../../css/font-awesome.min.css rel=stylesheet>
<link href=../../css/katex.min.css rel=stylesheet>
<link href=../../css/style.min.css rel=stylesheet type=text/css>
<link href=../../css/article.min.css rel=stylesheet>
<script src=../../js/jquery.min.js></script>
<script src=../../js/bootstrap.min.js></script>
<script defer src=../../js/underscore-min.js type=text/javascript></script>
<script defer src=../../js/katex.min.js></script>
<script defer src=../../js/auto-render.min.js></script>
<script defer src=../../js/article.min.js></script>
<script defer src=../../js/lang.min.js></script>
<script async defer src=../../js/buttons.js></script>
</head> <body data-offset=50 data-spy=scroll data-target=.navbar> <!-- HEADER <nav class="navbar navbar-inverse navbar-static-top"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class="collapse navbar-collapse" id=myNavbar> <ul class="nav navbar-nav"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class="nav navbar-nav navbar-center"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class="collapse navbar-collapse" data-target=None id=HiddenNavbar> <ul class="nav navbar-nav navbar-right"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style="padding: 0px;"> <img alt=MIT netsrc=images/ src=../../images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style="padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;"> </a> </ul> </div> </div> </div> </nav> --> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title style=display:none;> <!-- DISPLAY:NONE --> <a href=teaching/cs-230 onclick=trackOutboundLink(this);><img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48 style="width: 15px;">   <b>CS 230 - Глубокое Обучение</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>Recurrent Neural Networks</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#overview>Обзор</a></div> <div class=dropdown-container> <a href=#overview><span>Структура архитектуры</span></a> <a href=#overview><span>Приложения RNN</span></a> <a href=#overview><span>Функция потерь</span></a> <a href=#overview><span>Обратное распространение ошибки</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#architecture>Обработка долгосрочных зависимостей</a></div> <div class=dropdown-container> <a href=#architecture><span>Общие функции активации</span></a> <a href=#architecture><span>Исчезающий/увеличивающийся градиент</span></a> <a href=#architecture><span>Отсечение градиента</span></a> <a href=#architecture><span>GRU/LSTM</span></a> <a href=#architecture><span>Типы вентилей</span></a> <a href=#architecture><span>Двунаправленный RNN</span></a> <a href=#architecture><span>Глубокая RNN.</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#word-representation>Представление обучающих слов</a></div> <div class=dropdown-container> <a href=#word-representation><span>Обозначения</span></a> <a href=#word-representation><span>Embedding matrix</span></a> <a href=#word-representation><span>Word2vec</span></a> <a href=#word-representation><span>Скип-грамм</span></a> <a href=#word-representation><span>Отрицательная выборка</span></a> <a href=#word-representation><span>GloVe</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#comparing-words>Сравнение слов</a></div> <div class=dropdown-container> <a href=#comparing-words><span>Косинусное сходство</span></a> <a href=#comparing-words><span>t-SNE</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#language-model>Языковая модель</a></div> <div class=dropdown-container> <a href=#language-model><span>n-граммы</span></a> <a href=#language-model><span>Метрика Perplexity</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#machine-translation>Машинный перевод</a></div> <div class=dropdown-container> <a href=#machine-translation><span>Лучевой поиск</span></a> <a href=#machine-translation><span>Нормализация длины</span></a> <a href=#machine-translation><span>Анализ ошибок</span></a> <a href=#machine-translation><span>Метрика BLEU</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#attention>Внимание</a></div> <div class=dropdown-container> <a href=#attention><span>Модель внимания</span></a> <a href=#attention><span>Веса внимания</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/en/cheatsheet-recurrent-neural-networks.pdf onclick=trackOutboundLink(this); style="color: white; text-decoration:none;"> <i aria-hidden=false class="fa fa-github fa-fw"></i> Посмотреть PDF-версию на GitHub </a> </li> </div> </center> </div> <article class="markdown-body entry-content" itemprop=text>
<div class="alert alert-primary" role=alert style=display:none;> <!-- DISPLAY:NONE -->
  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!
</div>
<div class=title-lang style=display:none;> <!-- DISPLAY:NONE --><a aria-hidden=true class=anchor-bis href=#cs-230---deep-learning id=cs-230---deep-learning></a><a href=teaching/cs-230 onclick=trackOutboundLink(this);>CS 230 - Глубокое Обучение</a>
  
  <div style=float:right;display:none;> <!-- DISPLAY:NONE -->
    <div class=input-group>
      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>
        <option selected value=en>English</option>
        <option value=fa>فارسی</option>
        <option value=fr>Français</option>
        <option value=ja>日本語</option>
        <option value=ko>한국어</option>
        <option value=tr>Türkçe</option>
        <option value=vi>Tiếng Việt</option>
      </select>
      <div class=input-group-addon><i class=fa></i></div>
    </div>
  </div>
</div>
<br>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button>CS 221 - Artificial Intelligence</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button>CS 229 - Machine Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button><B>CS 230 - Глубокое Обучение</B></BUTTON>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button>Convolutional Neural Networks</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-230/recurrent-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-recurrent-neural-networks'" type=button><B>Recurrent Neural Networks</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/deep-learning-tips-and-tricks/index.html'; oldhref='teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks'" type=button>Tips and tricks</button>
  </div>
</div>

<h1>
  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>Шпаргалка по Рекуррентным Нейронным Сетям
  <div style=float:right;display:none;> <!-- DISPLAY:NONE --><a aria-label="Star afshinea/stanford-cs-230-deep-learning on GitHub" class="github-button fa-fw" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-230-deep-learning onclick=trackOutboundLink(this);>Star</a></div>
</h1>
<i><!-- By --><a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> и <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a>;<a href=https://github.com/AlexandrParkhomenko onclick=trackOutboundLink(this);> Alexandr Parkhomenko</a> и <a href=https://github.com/geotrush onclick=trackOutboundLink(this);>Труш Георгий (Georgy Trush)</a></i>
<h2><a aria-hidden=true class=anchor href=#overview id=overview></a>Обзор</h2>
<p><span class="new-item item-r">Архитектура классической RNN</span> Рекуррентные нейронные сети, также известные как RNN, представляют собой класс нейронных сетей, которые позволяют использовать предыдущие выходы в качестве входов, имея скрытые состояния. Обычно они следующие:</p>
<div class=mobile-container>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc style="width:100%; max-width:700px;">
</div>
<br>
<p>Для каждого временного шага $t$ активация $a^{&lt; t &gt;}$ и выход $y^{&lt; t &gt;}$ выражаются следующим образом:
</p><div class=mobile-container>
\[\boxed{a^{&lt; t &gt;}=g_1(W_{aa}a^{&lt; t-1 &gt;}+W_{ax}x^{&lt; t &gt;}+b_a)}\quad\textrm{и}\quad\boxed{y^{&lt; t &gt;}=g_2(W_{ya}a^{&lt; t &gt;}+b_y)}\]
</div>
где $W_{ax}, W_{aa}, W_{ya}, b_a, b_y$ являются коэффициентами, которые являются одинаковыми для всех ячеек RNN во времени, и функциями активации $g_1, g_2$ .<br>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=description-block-rnn-ltr.png?74e25518f882f8758439bcb3637715e5 style="width:100%; max-width:630px;">
</center>
</div>
<br>
<p>Плюсы и минусы типичной архитектуры RNN перечислены в таблице ниже:</p>
<div class=mobile-container>
<center>
<table>
<colgroup><col width=50%>
<col width=50%>
</colgroup><tbody>
<tr>
<td align=center><b>Преимущества</b></td>
<td align=center><b>Недостатки</b></td>
</tr>
<tr>
<td align=left>• Возможность обработки входа любой длины<br>
                 • Размер модели не увеличивается с размером входных данных<br>
                 • При расчетах учитывается историческая информация<br>
                 • Веса распределяются во времени</td>
<td align=left>• Вычисления идут медленно<br>
                 • Сложность доступа к очень давней информации<br>
                 • Невозможно рассмотреть какие-либо будущие входные данные для текущего состояния</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-g">Применение RNN</span> Модели RNN в основном используются в области обработки естественного языка и распознавания речи. Различные приложения приведены в таблице ниже:</p>
<div class=mobile-container>
<center>
<table>
<colgroup><col width=20%>
<col width=55%>
<col width=25%>
</colgroup><tbody>
<tr>
<td align=center><b>Тип RNN</b></td>
<td align=center><b>Иллюстрация</b></td>
<td align=center><b>Пример</b></td>
</tr>
<tr>
<td align=center>One-to-one<br>$T_x=T_y=1$</td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=rnn-one-to-one-ltr.png?9c8e3b04d222d178d6bee4506cc3f779></td>
<td align=left>Классическая нейронная сеть</td>
</tr>
<tr>
<td align=center>One-to-many<br>$T_x=1, T_y&gt;1$</td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=rnn-one-to-many-ltr.png?d246c2f0d1e0f43a21a8bd95f579cb3b></td>
<td align=left>Генерация музыки</td>
</tr>
<tr>
<td align=center>Many-to-one<br>$T_x&gt;1, T_y=1$</td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=rnn-many-to-one-ltr.png?c8a442b3ea9f4cb81f929c089b910c9d></td>
<td align=left>Определение эмоциональной окраски, Классификация эмоций (Sentiment classification)</td>
</tr>
<tr>
<td align=center>Many-to-many<br>$T_x=T_y$</td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=rnn-many-to-many-same-ltr.png?2790431b32050b34b80011afead1f232></td>
<td align=left>Распознавание именованных сущностей (Name entity recognition NER)</td>
</tr>
<tr>
<td align=center>Many-to-many<br>$T_x\neq T_y$</td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=rnn-many-to-many-different-ltr.png?8ca8bafd1eeac4e8c961d9293858407b></td>
<td align=left>Машинный перевод</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-b">Функция потерь</span> В случае рекуррентной нейронной сети функция потерь $\mathcal{L}$ всех временных шагов определяется на основе значений функции потерь на каждом временном шаге следующим образом:</p>
<div class=mobile-container>
\[\boxed{\mathcal{L}(\widehat{y},y)=\sum_{t=1}^{T_y}\mathcal{L}(\widehat{y}^{&lt; t &gt;},y^{&lt; t &gt;})}\]
</div>
<br>
<p><span class="new-item item-b">Обратное распространение ошибки во времени</span> Обратное распространение выполняется в каждый момент времени. На временном шаге $T$ производная потерь $\mathcal{L}$ по матрице весов $W$ выражается следующим образом:</p>
<div class=mobile-container>
\[\boxed{\frac{\partial \mathcal{L}^{(T)}}{\partial W}=\sum_{t=1}^T\left.\frac{\partial\mathcal{L}^{(T)}}{\partial W}\right|_{(t)}}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#architecture id=architecture></a>Обработка долгосрочных зависимостей</h2>
<p><span class="new-item item-b">Часто используемые функции активации</span> Наиболее распространенные функции активации, используемые в модулях RNN, описаны ниже:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:450px;">
<colgroup>
<col style=width:33%>
<col style=width:33%>
<col style=width:33%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Sigmoid</b></td>
<td align=center><b>Tanh</b></td>
<td align=center><b>RELU</b></td>
</tr>
<tr>
<td align=center>$\displaystyle g(z)=\frac{1}{1+e^{-z}}$</td>
<td align=center>$\displaystyle g(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$</td>
<td align=center>$\displaystyle g(z)=\max(0,z)$</td>
</tr>
<tr>
<td align=center><img alt=Sigmoid class=img-responsive netsrc=teaching/cs-229/illustrations/ src=sigmoid.png?c91b6e5a7d4e78e95880bcf4e39889df style="width:100%; max-width:200px;"></td>
<td align=center><img alt=Tanh class=img-responsive netsrc=teaching/cs-229/illustrations/ src=tanh.png?22ac27f27c510c6414e8a3bb4aca2d80 style="width:100%; max-width:200px;"></td>
<td align=center><img alt=RELU class=img-responsive netsrc=teaching/cs-229/illustrations/ src=relu.png?6c1d78551355db5c6e4f6f8b5282cfa8 style="width:100%; max-width:200px;"></td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-b">Исчезающий/взрывающийся градиент</span> Явления исчезающих и взрывных градиентов часто встречаются в контексте RNN. Причина, по которой они происходят, заключается в том, что трудно зафиксировать долгосрочные зависимости из-за мультипликативного градиента, который может экспоненциально уменьшаться/увеличиваться по отношению к количеству слоев.</p>
<br>
<p><span class="new-item item-b">Отсечение градиента</span> это метод, используемый для решения проблемы взрывных градиентов, иногда возникающей при выполнении обратного распространения ошибки. Ограничивая максимальное значение градиента, это явление контролируется на практике.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=gradient-clipping-en.png?6c3de441dc56aad634dc1a91accb48f2 style="width:100%; max-width:375px;">
</center>
</div>
<br>
<p><span class="new-item item-g">Типы вентилей</span> чтобы решить проблему исчезающего градиента, в некоторых типах RNN используются специфичные вентили, которые обычно имеют четко определенную цель. Обычно они обозначаются $\Gamma$ и равны:</p>
<div class=mobile-container>
\[\boxed{\Gamma=\sigma(Wx^{&lt; t &gt;}+Ua^{&lt; t-1 &gt;}+b)}\]
</div>
<p>где $W, U, b$ ― коэффициенты, относящиеся к вентилю, а $\sigma$ - функция сигмоиды. Основные из них приведены в таблице ниже:</p>
<div class=mobile-container>
<center>
<table>
<colgroup><col width=25%>
<col width=45%>
<col width=30%>
</colgroup><tbody>
<tr>
<td align=center><b>Тип вентиля</b></td>
<td align=center><b>Роль</b></td>
<td align=center><b>Используется в</b></td>
</tr>
<tr>
<td align=center>Update gate $\Gamma_u$</td>
<td align=center>Насколько прошлое должно иметь значение сейчас?</td>
<td align=center>GRU, LSTM</td>
</tr>
<tr>
<td align=center>Relevance gate $\Gamma_r$</td>
<td align=center>Отбросить предыдущую информацию?</td>
<td align=center>GRU, LSTM</td>
</tr>
<tr>
<td align=center>Forget gate $\Gamma_f$</td>
<td align=center>Стереть ячейку или нет?</td>
<td align=center>LSTM</td>
</tr>
<tr>
<td align=center>Output gate $\Gamma_o$</td>
<td align=center>Насколько раскрыть ячейку?</td>
<td align=center>LSTM</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-r">GRU/LSTM</span> Вентильный Рекуррентный Блок (Gated Recurrent Unit, GRU) и Блок с Долгой Краткосрочной Памятью (Long Short-Term Memory units, LSTM) имеет дело с проблемой исчезающего градиента, с которой сталкиваются традиционные RNN, причем LSTM является обобщением GRU. Ниже представлена таблица, в которой перечислены характеризующие уравнения каждой архитектуры:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:725px;">
<colgroup>
<col width=160px>
<col width=50%>
<col width=50%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Характеристика</b></td>
<td align=center><b>Gated Recurrent Unit</b> (GRU)</td>
<td align=center><b>Long Short-Term Memory</b> (LSTM)</td>
</tr>
<tr>
<td align=center>$\tilde{c}^{&lt; t &gt;}$</td>
<td align=center>$\textrm{tanh}(W_c[\Gamma_r\star a^{&lt; t-1 &gt;},x^{&lt; t &gt;}]+b_c)$</td>
<td align=center>$\textrm{tanh}(W_c[\Gamma_r\star a^{&lt; t-1 &gt;},x^{&lt; t &gt;}]+b_c)$</td>
</tr>
<tr>
<td align=center>$c^{&lt; t &gt;}$</td>
<td align=center>$\Gamma_u\star\tilde{c}^{&lt; t &gt;}+(1-\Gamma_u)\star c^{&lt; t-1 &gt;}$</td>
<td align=center>$\Gamma_u\star\tilde{c}^{&lt; t &gt;}+\Gamma_f\star c^{&lt; t-1 &gt;}$</td>
</tr>
<tr>
<td align=center>$a^{&lt; t &gt;}$</td>
<td align=center>$c^{&lt; t &gt;}$</td>
<td align=center>$\Gamma_o\star c^{&lt; t &gt;}$</td>
</tr>
<tr>
<td align=center>Зависимости</td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=gru-ltr.png?00f278f71b4833d32a87ed53d86f251c style="width:100%; max-width:400px;"></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=lstm-ltr.png?4539fbbcbd9fabfd365936131c13476c style="width:100%; max-width:400px;"></td>
</tr>
</tbody>
</table>
</center>
</div>
<p><span class=remark>Примечание: знак $\star$ означает поэлементное умножение двух векторов.</span></p>
<br>
<p><span class="new-item item-r">Варианты RNN</span> В таблице ниже перечислены другие часто используемые архитектуры RNN:</p>
<div class=mobile-container>
<center>
<table>
<colgroup><col width=50%>
<col width=50%>
</colgroup><tbody>
<tr>
<td align=center><b>Bidirectional</b> (BRNN)</td>
<td align=center><b>Deep</b> (DRNN)</td>
</tr>
<tr>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=bidirectional-rnn-ltr.png?e3e66fae56ea500924825017917b464a style="width:100%; max-width:375px;"></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=deep-rnn-ltr.png?f57da6de44ddd4709ad3b696cac6a912 style="width:100%; max-width:375px;"></td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#word-representation id=word-representation></a>Представление обучающих слов</h2>
<p>В этом разделе мы обозначаем словарь $V$ и его размер $|V|$.</p>
<h3>Мотивация и обозначения</h3>
<p><span class="new-item item-g">Методы представления</span> два основных способа представления слов подытожены в таблице ниже:</p>
<div class=mobile-container>
<center>
<table>
<colgroup><col width=50%>
<col width=50%>
</colgroup><tbody>
<tr>
<td align=center><b>One-hot представление</b></td>
<td align=center><b>Представления слов</b></td>
</tr>
<tr>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=one-hot-representation-en.png?c60bb9aafc7b14e1076d43c8f85f63df style="width:100%; max-width:225px;"></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=word-embedding-representation-en.png?2dcceb341d5a5ccc9c7e0da0fd523a74 style="width:100%; max-width:225px;"></td>
</tr>
<tr>
<td align=left>• Обозначено $o_w$<br>
                 • Наивный подход, нет информации о сходстве</td>
<td align=left>• Обозначено $e_w$<br>
                 • Учитывает сходство слов</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-b">Матрица представления (embedding matrix)</span> для данного слова $w$ матрица представления $E$ является матрицей, которая отображает свое one-hot представление $o_w$ на его представление $e_w$ следующим образом:</p>
<div class=mobile-container>
\[\boxed{e_w=Eo_w}\]
</div>
<p><span class=remark>Примечание: получить матрицу представлений можно путем обучения моделей целевого/контекстного правдоподобия.</span></p>
<br>
<h3>Векторные представления слов</h3>
<p><span class="new-item item-r">Word2vec</span> это фреймворк, предназначенный для получения представлений слов путем оценки вероятности того, что данное слово окружено другими словами. Популярные модели включают Skip-gram, Negative sampling и CBoW.</p>
<br>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=word2vec-en.png?4198ce416cd8afb1ba06f2b5f5784583 style="width:100%; max-width:900px;">
</center>
</div>
<br>
<p><span class="new-item item-r">Skip-gram</span> Модель skip-gram word2vec - это алгоритм обучения с учителем, который выучивает представления слов, оценивая правдоподобие того, что любое заданное целевое слово $t$ встречается с контекстным словом $c$. Обозначим $\theta_t$ параметр, связанный с $t$, вероятность $P(t|c)$ определяется выражением:</p>
<div class=mobile-container>
\[\boxed{P(t|c)=\frac{\exp(\theta_t^Te_c)}{\displaystyle\sum_{j=1}^{|V|}\exp(\theta_j^Te_c)}}\]
</div>
<p><span class=remark>Примечание: суммирование по всему словарю в знаменателе части softmax делает эту модель дорогостоящей в вычислительном отношении. CBOW - это еще одна модель word2vec, использующая окружающие слова для предсказания данного слова.</span></p>
<br>
<p><span class="new-item item-b">Negative sampling</span> Отрицательная выборка - набор бинарных классификаторов, использующих логистические регрессии, целью которых является оценка того, как данный контекст и заданные целевые слова могут появляться одновременно, при этом модели обучаются на наборах из $k$ отрицательных примеров и 1 положительного примера. Учитывая контекстное слово $c$ и целевое слово $t$, прогноз выражается следующим образом:</p>
<div class=mobile-container>
\[\boxed{P(y=1|c,t)=\sigma(\theta_t^Te_c)}\]
</div>
<p><span class=remark>Примечание: этот метод менее затратен с точки зрения вычислений, чем модель скип-граммы.</span></p>
<br>
<p><span class="new-item item-r">GloVe</span> Модель GloVe, сокращение от глобальных векторов для представления слов, является методом получения представлений слов, который использует матрицу совпадения $X$, где каждый $X_{i,j}$ обозначает количество раз, когда цель $i$ встречалась с контекстом $j$. Его функция стоимости $J$ выглядит следующим образом:</p>
<div class=mobile-container>
\[\boxed{J(\theta)=\frac{1}{2}\sum_{i,j=1}^{|V|}f(X_{ij})(\theta_i^Te_j+b_i+b_j'-\log(X_{ij}))^2}\]
</div>
<p>где $f$ - такая взвешивающая функция, что $X_{i,j}=0\Longrightarrow f(X_{i,j})=0$.<br>
Учитывая симметрию, которую играют $e$ и $\theta$ в этой модели, последнее представление слов $e_w^{(\textrm{final})}$ задается выражением:</p>
<div class=mobile-container>
\[\boxed{e_w^{(\textrm{final})}=\frac{e_w+\theta_w}{2}}\]
</div>
<p><span class=remark>Примечание: отдельные компоненты векторов представлений слов не обязательно поддаются интерпретации.</span></p>
<br>
<h2><a aria-hidden=true class=anchor href=#comparing-words id=comparing-words></a>Сравнение слов</h2>
<p><span class="new-item item-b">Косинусное сходство</span> косинусное сходство между словами $w_1$ и $w_2$ выражается следующим образом:</p>
<div class=mobile-container>
\[\boxed{\textrm{similarity}=\frac{w_1\cdot w_2}{||w_1||\textrm{ }||w_2||}=\cos(\theta)}\]
</div>
<p><span class=remark>Примечание: $\theta$ - угол между словами $w_1$ и $w_2$.</span></p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=cosine-similarity.png?58c75740dd02237336f5883328d050e9 style="width:100%; max-width:250px;">
</center>
</div>
<br>
<p><span class="new-item item-r">$t$-SNE</span> $t$-распределенное стохастическое соседнее представление ($t$-distributed Stochastic Neighbor Embedding, $t$-SNE) - это метод, направленный на сокращение представлений большой размерности в пространство меньшей размерности. На практике он обычно используется для визуализации векторов слов в 2D-пространстве.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=t-sne-en.png?411bf8ff5d5c06c6e90cae95f7110ff1 style="width:100%; max-width:550px;">
</center>
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#language-model id=language-model></a>Языковая модель</h2>
<p><span class="new-item item-r">Обзор</span> языковая модель предназначена для оценки вероятности предложения $P(y)$.</p>
<br>
<p><span class="new-item item-r">Модель $n$-gram</span> эта модель представляет собой наивный подход, направленный на количественную оценку вероятности того, что выражение появляется в корпусе, путем подсчета его количества появлений в обучающих данных.</p>
<br>
<p><span class="new-item item-b">Метрика Perplexity</span> Perplexity (Недоумение) - языковые модели обычно оцениваются с помощью метрики Perplexity, также известной как PP, которую можно интерпретировать как обратную вероятность набора данных, нормализованную на количество слов $T$. Perplexity таково, что чем оно ниже, тем лучше, и определяется следующим образом:</p>
<div class=mobile-container>
\[\boxed{\textrm{PP}=\prod_{t=1}^T\left(\frac{1}{\sum_{j=1}^{|V|}y_j^{(t)}\cdot \widehat{y}_j^{(t)}}\right)^{\frac{1}{T}}}\]
</div>
<p><span class=remark>Примечание: PP обычно используется в $t$-SNE.</span></p>
<br>
<h2><a aria-hidden=true class=anchor href=#machine-translation id=machine-translation></a>Машинный перевод</h2>
<p><span class="new-item item-r">Обзор</span> модель машинного перевода похожа на языковую модель, за исключением того, что перед ней размещена сеть кодировщика. По этой причине её иногда называют условной моделью языка.</p>
<p>Цель состоит в том, чтобы найти такое предложение $y$, что:</p>
<div class=mobile-container>
\[\boxed{y=\underset{y^{&lt; 1 &gt;}, ..., y^{&lt; T_y &gt;}}{\textrm{arg max}}P(y^{&lt; 1 &gt;},...,y^{&lt; T_y &gt;}|x)}\]
</div>
<br>
<p><span class="new-item item-g">Лучевой поиск</span> это алгоритм эвристического поиска, используемый в машинном переводе и распознавании речи для поиска наиболее вероятного предложения $y$ при вводе $x$.</p>
<p>• Шаг 1: Найти top $B$ наиболее вероятных слов $y^{&lt; 1 &gt;}$
<br>• Шаг 2: Вычислить условные вероятности $y^{&lt; k &gt;}|x,y^{&lt; 1 &gt;},...,y^{&lt; k-1 &gt;}$
<br>• Шаг 3: Сохранить top $B$ комбинации $x,y^{&lt; 1&gt;},...,y^{&lt; k &gt;}$</p>
<br>• Шаг 4: Завершить процесс на стоп-слове</p>
<br>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=beam-search-en.png?3515955a2324591070618dd85812d5d7 style="width:100%; max-width:900px;">
</center>
</div>
<br>
<p><span class=remark>Примечание: если ширина луча установлена на 1, то это равносильно наивному жадному поиску.</span></p>
<br>
<p><span class="new-item item-r">Ширина луча</span> Ширина луча $B$ является параметром лучевого поиска. Большие значения $B$ дают лучший результат, но с меньшей производительностью и увеличенным объёмом памяти. Маленькие значения $B$ приводят к худшим результатам, но требуют меньших вычислительных затрат. Стандартное значение $B$ составляет около 10.</p>
<br>
<p><span class="new-item item-r">Нормализация длины</span> Чтобы улучшить численную стабильность, лучевой поиск обычно применяется к следующей нормализованной цели, часто называемой нормализованной целью логарифмического правдоподобия, определяемой как:</p>
<div class=mobile-container>
\[\boxed{\textrm{Objective } = \frac{1}{T_y^\alpha}\sum_{t=1}^{T_y}\log\Big[p(y^{&lt; t &gt;}|x,y^{&lt; 1 &gt;}, ..., y^{&lt; t-1 &gt;})\Big]}\]
</div>
<p><span class=remark>Примечание: параметр $\alpha$ можно рассматривать как смягчитель, и его значение обычно составляет от 0.5 до 1.</span></p>
<br>
<p><span class="new-item item-b">Анализ ошибок</span> При получении предсказанного перевода $\widehat{y}$, который является плохим, можно задаться вопросом, почему мы не получили хороший перевод $y^*$ , выполнив следующий анализ ошибок:</p>
<div class=mobile-container>
<center>
<table>
<colgroup><col width=25%>
<col width=37.5%>
<col width=37.5%>
</colgroup><tbody>
<tr>
<td align=center><b>Случай</b></td>
<td align=center>$P(y^*|x)&gt;P(\widehat{y}|x)$</td>
<td align=center>$P(y^*|x)\leqslant P(\widehat{y}|x)$</td>
</tr>
<tr>
<td align=center><b>Первопричина</b></td>
<td align=center>Ошибка лучевого поиска</td>
<td align=center>Неисправность RNN</td>
</tr>
<tr>
<td align=center><b>Исправления</b></td>
<td align=center>Увеличить ширину луча</td>
<td align=left>• Попробовать другую архитектуру<br>
                 • Регуляризировать<br>
                 • Взять больше данных</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-b">Метрика BLEU</span> оценка дублера для двуязычной оценки (bilingual evaluation understudy, BLEU) количественно определяет, насколько хорош машинный перевод, путем вычисления оценки сходства на основе точности $n$-грамм. Это определяется следующим образом:
</p><div class=mobile-container>
\[\boxed{\textrm{bleu score}=\exp\left(\frac{1}{n}\sum_{k=1}^np_k\right)}\]
</div>
где $p_n$ - это оценка по $n$-граммам, определяемая только следующим образом:
<div class=mobile-container>
\[p_n=\frac{\displaystyle\sum_{\textrm{n-gram}\in\widehat{y}}\textrm{count}_{\textrm{clip}}(\textrm{n-gram})}{\displaystyle\sum_{\textrm{n-gram}\in\widehat{y}}\textrm{count}(\textrm{n-gram})}\]
</div>
<p><span class=remark>Примечание: к коротким предсказанным переводам может применяться штраф за краткость, чтобы предотвратить искусственно завышенную оценку BLEU.</span></p>
<br>
<h2><a aria-hidden=true class=anchor href=#attention id=attention></a>Внимание</h2>
<p><span class="new-item item-b">Модель внимания</span> эта модель позволяет RNN обращать внимание на определенные части входных данных, которые считаются важными, что на практике улучшает производительность полученной модели. Обозначим $\alpha^{&lt; t, t'&gt;}$ количество внимания, которое выход $y^{&lt; t &gt;}$ должен уделять активации $a^{&lt; t' &gt;}$ и $c^{&lt; t &gt;}$ контексту в момент времени $t$, у нас есть:</p>
<div class=mobile-container>
\[\boxed{c^{&lt; t &gt;}=\sum_{t'}\alpha^{&lt; t, t' &gt;}a^{&lt; t' &gt;}}\quad\textrm{with}\quad\sum_{t'}\alpha^{&lt; t,t' &gt;}=1\]
</div>
<p><span class=remark>Примечание: оценки внимания обычно используются при добавлении субтитров к изображениям и машинном переводе.</span></p>
<br>
<center>
  <div class=row>
      <div class="col-xs-6 col-md-6"><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=attention-model-captioning-1-en.jpeg?88f2fc4c05c8bd834112845a238cfa8b style="width:100%; max-width:330px"></div>
      <div class="col-xs-6 col-md-6"><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=attention-model-captioning-2-en.jpeg?d02de27df05fdb8106bac8f0bfa3a32d style="width:100%; max-width:330px"></div>
  </div>
</center>
<br>
<p><span class="new-item item-b">Вес внимания</span> количество внимания, которое выход $y^{&lt; t &gt;}$ должен уделять активации $a^{&lt; t' &gt;}$, задается выражением $\alpha^{&lt; t,t' &gt;}$, вычисляемым следующим образом:</p>
<div class=mobile-container>
\[\boxed{\alpha^{&lt; t,t' &gt;}=\frac{\exp(e^{&lt; t,t' &gt;})}{\displaystyle\sum_{t''=1}^{T_x}\exp(e^{&lt; t,t'' &gt;})}}\]
</div>
<p><span class=remark>Примечание: сложность вычислений квадратична относительно $T_x$.</span></p>
<br>
</article> </div> <!-- FOOTER <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-twitter fa-3x fa-fw"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class="fa fa-linkedin fa-3x fa-fw"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-github fa-3x fa-fw"></i></a> <a href="https://scholar.google.com/citations?user=nMnMTm8AAAAJ" onclick=trackOutboundLink(this);><i class="fa fa-google fa-3x fa-fw"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick="trackOutboundLink(this); window.location.href = 'mailto:' + this.dataset.name + '@' + this.dataset.domain + '.' + this.dataset.tld"><i class="fa fa-envelope fa-3x fa-fw"></i></a> </div> </div> </footer> --> </body></html>
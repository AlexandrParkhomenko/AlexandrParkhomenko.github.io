<!DOCTYPE html><html lang=en><head><!-- base href=../../ --><title>CS 230 - Convolutional Neural Networks Cheatsheet</title><meta charset=utf-8><meta content="Teaching page of Shervine Amidi, Graduate Student at Stanford University." name=description><meta content="teaching, shervine, shervine amidi, data science" name=keywords><meta content="width=device-width, initial-scale=1" name=viewport>
<link href=https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks rel=canonical>
<link href=../../css/bootstrap.min.css rel=stylesheet>
<link href=../../css/font-awesome.min.css rel=stylesheet>
<link href=../../css/katex.min.css rel=stylesheet>
<link href=../../css/style.min.css rel=stylesheet type=text/css>
<link href=../../css/article.min.css rel=stylesheet>
<script src=../../js/jquery.min.js></script>
<script src=../../js/bootstrap.min.js></script>
<script defer src=../../js/underscore-min.js type=text/javascript></script>
<script defer src=../../js/katex.min.js></script>
<script defer src=../../js/auto-render.min.js></script>
<script defer src=../../js/article.min.js></script>
<script defer src=../../js/lang.min.js></script>
<script async defer src=../../js/buttons.js></script>
</head> <body data-offset=50 data-spy=scroll data-target=.navbar> <!-- HEADER <nav class="navbar navbar-inverse navbar-static-top"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class="collapse navbar-collapse" id=myNavbar> <ul class="nav navbar-nav"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class="nav navbar-nav navbar-center"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class="collapse navbar-collapse" data-target=None id=HiddenNavbar> <ul class="nav navbar-nav navbar-right"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style="padding: 0px;"> <img alt=MIT netsrc=images/ src=../../images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style="padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;"> </a> </ul> </div> </div> </div> </nav> --> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title style=display:none;> <!-- DISPLAY:NONE --> <a href=teaching/cs-230 onclick=trackOutboundLink(this);><img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48 style="width: 15px;">   <b>CS 230 - Deep Learning</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>Convolutional Neural Nets</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#overview>Overview</a></div> <div class=dropdown-container> <a href=#overview><span>Architecture structure</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#layer>Types of layer</a></div> <div class=dropdown-container> <a href=#layer><span>Convolution</span></a> <a href=#layer><span>Pooling</span></a> <a href=#layer><span>Fully connected</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#filter>Filter hyperparameters</a></div> <div class=dropdown-container> <a href=#filter><span>Dimensions</span></a> <a href=#filter><span>Stride</span></a> <a href=#filter><span>Padding</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#hyperparameters>Tuning hyperparameters</a></div> <div class=dropdown-container> <a href=#hyperparameters><span>Parameter compatibility</span></a> <a href=#hyperparameters><span>Model complexity</span></a> <a href=#hyperparameters><span>Receptive field</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#activation-function>Activation functions</a></div> <div class=dropdown-container> <a href=#activation-function><span>Rectified Linear Unit</span></a> <a href=#activation-function><span>Softmax</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#object-detection>Object detection</a></div> <div class=dropdown-container> <a href=#object-detection><span>Types of models</span></a> <a href=#object-detection><span>Detection</span></a> <a href=#object-detection><span>Intersection over Union</span></a> <a href=#object-detection><span>Non-max suppression</span></a> <a href=#object-detection><span>YOLO</span></a> <a href=#object-detection><span>R-CNN</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#face>Face verification/recognition</a></div> <div class=dropdown-container> <a href=#face><span>One shot learning</span></a> <a href=#face><span>Siamese network</span></a> <a href=#face><span>Triplet loss</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#style-transfer>Neural style transfer</a></div> <div class=dropdown-container> <a href=#style-transfer><span>Activation</span></a> <a href=#style-transfer><span>Style matrix</span></a> <a href=#style-transfer><span>Style/content cost function</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#ct-architectures>Computational trick architectures</a></div> <div class=dropdown-container> <a href=#ct-architectures><span>Generative Adversarial Net</span></a> <a href=#ct-architectures><span>ResNet</span></a> <a href=#ct-architectures><span>Inception Network</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/en/cheatsheet-convolutional-neural-networks.pdf onclick=trackOutboundLink(this); style="color: white; text-decoration:none;"> <i aria-hidden=false class="fa fa-github fa-fw"></i> View PDF version on GitHub </a> </li> </div> </center> </div> <article class="markdown-body entry-content" itemprop=text>
<div class="alert alert-primary" role=alert style=display:none;> <!-- DISPLAY:NONE -->
  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!
</div>
<div class=title-lang style=display:none;> <!-- DISPLAY:NONE --><a aria-hidden=true class=anchor-bis href=#cs-230---deep-learning id=cs-230---deep-learning></a><a href=teaching/cs-230 onclick=trackOutboundLink(this);>CS 230 - Deep Learning</a>
  
  <div style=float:right;display:none;> <!-- DISPLAY:NONE -->
    <div class=input-group>
      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>
        <option selected value=en>English</option>
        <option value=fa>فارسی</option>
        <option value=fr>Français</option>
        <option value=ja>日本語</option>
        <option value=ko>한국어</option>
        <option value=tr>Türkçe</option>
        <option value=vi>Tiếng Việt</option>
      </select>
      <div class=input-group-addon><i class=fa></i></div>
    </div>
  </div>
</div>
<br>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button>CS 221 - Artificial Intelligence</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button>CS 229 - Machine Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button><b>CS 230 - Deep Learning</b></button>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button><b>Convolutional Neural Networks</b></button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/recurrent-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-recurrent-neural-networks'" type=button>Recurrent Neural Networks</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/deep-learning-tips-and-tricks/index.html'; oldhref='teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks'" type=button>Tips and tricks</button>
  </div>
</div>

<h1>
  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>Convolutional Neural Networks cheatsheet
  <div style=float:right;display:none;> <!-- DISPLAY:NONE --><a aria-label="Star afshinea/stanford-cs-230-deep-learning on GitHub" class="github-button fa-fw" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-230-deep-learning onclick=trackOutboundLink(this);>Star</a></div>
</h1>
<i>By <a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> and <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a></i>
<h2><a aria-hidden=true class=anchor href=#overview id=overview></a>Overview</h2>
<p><span class="new-item item-r">Architecture of a traditional CNN</span> Convolutional neural networks, also known as CNNs, are a specific type of neural networks that are generally composed of the following layers:</p>
<div class=mobile-container>
<center>
  <img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=architecture-cnn-en.jpeg?3b7fccd728e29dc619e1bd8022bf71cf style=width:100%;max-width:900px>
</center>
</div>
<br>
<p>The convolution layer and the pooling layer can be fine-tuned with respect to hyperparameters that are described in the next sections.</p>
<br>
<h2><a aria-hidden=true class=anchor href=#layer id=layer></a>Types of layer</h2>
<p><span class="new-item item-b">Convolution layer (CONV)</span> The convolution layer (CONV) uses filters that perform convolution operations as it is scanning the input $I$ with respect to its dimensions. Its hyperparameters include the filter size $F$ and stride $S$. The resulting output $O$ is called <i>feature map</i> or <i>activation map</i>.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=convolution-layer-a.png?1c517e00cb8d709baf32fc3d39ebae67 style=width:100%;max-width:450px>
</center>
</div>
<br>
<p><span class=remark>Remark: the convolution step can be generalized to the 1D and 3D cases as well.</span></p>
<br>
<p><span class="new-item item-b">Pooling (POOL)</span> The pooling layer (POOL) is a downsampling operation, typically applied after a convolution layer, which does some spatial invariance. In particular, max and average pooling are special kinds of pooling where the maximum and average value is taken, respectively.</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:400px; max-width:720px;">
<colgroup>
<col style=width:115px>
<col style=width:50%>
<col style=width:50%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Type</b></td>
<td align=center>Max pooling</td>
<td align=center>Average pooling</td>
</tr>
<tr>
<td align=center><b>Purpose</b></td>
<td align=left>Each pooling operation selects the maximum value of the current view</td>
<td align=left>Each pooling operation averages the values of the current view</td>
</tr>
<tr>
<td align=center><b>Illustration</b></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=max-pooling-a.png?711b14799d07f9306864695e2713ae07></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=average-pooling-a.png?58f9ab6d61248c3ec8d526ef65763d2f></td>
</tr>
<tr>
<td align=center><b>Comments</b></td>
<td align=left>• Preserves detected features<br>• Most commonly used
</td>
<td align=left>• Downsamples feature map<br>• Used in LeNet</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-b">Fully Connected (FC)</span> The fully connected layer (FC) operates on a flattened input where each input is connected to all neurons. If present, FC layers are usually found towards the end of CNN architectures and can be used to optimize objectives such as class scores.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=fully-connected-ltr.png?32caf9e07c79d652faa292812579d063 style="width:100%; max-width:500px;">
</center>
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#filter id=filter></a>Filter hyperparameters</h2>
<p>The convolution layer contains filters for which it is important to know the meaning behind its hyperparameters.</p>
<p><span class="new-item item-b">Dimensions of a filter</span> A filter of size $F\times F$ applied to an input containing $C$ channels is a $F \times F \times C$ volume that performs convolutions on an input of size $I \times I \times C$ and produces an output feature map (also called activation map) of size $O \times O \times 1$.
</p><div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=dimensions-filter-en.png?7ce161e129a392a1804a231536b59f45 style=width:100%;>
</center>
</div>
<br>
<i>Remark: the application of $K$ filters of size $F\times F$ results in an output feature map of size $O \times O \times K$.</i>
<br><br>
<p><span class="new-item item-b">Stride</span> For a convolutional or a pooling operation, the stride $S$ denotes the number of pixels by which the window moves after each operation.
</p><div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=stride.png?36b5b2e02f7e02c3c4075a9d836c048c style=width:100%;max-width:700px>
</center>
</div>
<br>
<br>
<p><span class="new-item item-b">Zero-padding</span> Zero-padding denotes the process of adding $P$ zeroes to each side of the boundaries of the input. This value can either be manually specified or automatically set through one of the three modes detailed below:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:700px;">
  <colgroup>
    <col style=width:115px;>
    <col style=width:33%>
    <col style=width:33%>
    <col style=width:33%>
  </colgroup>
<tbody>
<tr>
<td align=center><b>Mode</b></td>
<td align=center>Valid</td>
<td align=center>Same</td>
<td align=center>Full</td>
</tr>
<tr>
<td align=center><b>Value</b></td>
<td align=center>$P = 0$</td>
<td align=left>$P_\text{start} = \Bigl\lfloor\frac{S \lceil\frac{I}{S}\rceil - I + F - S}{2}\Bigr\rfloor$<br>$P_\text{end} = \Bigl\lceil\frac{S \lceil\frac{I}{S}\rceil - I + F - S}{2}\Bigr\rceil$</td>
<td align=left>$P_\text{start}\in[\![0,F-1]\!]$<br><br>$P_\text{end} = F-1$</td>
</tr>
<tr>
<td align=center><b>Illustration</b></td>
<td align=center><img alt="Padding valid" class=img-responsive netsrc=teaching/cs-230/illustrations/ src=padding-valid-a.png?1f58d78612f6202ce201620919d71609></td>
<td align=center><img alt="Padding same" class=img-responsive netsrc=teaching/cs-230/illustrations/ src=padding-same-a.png?8b680283b10a6e131209b74e21a61213></td>
<td align=center><img alt="Padding full" class=img-responsive netsrc=teaching/cs-230/illustrations/ src=padding-full-a.png?b51e98467c8a77574c7e8f108654ad95></td>
</tr>
<tr>
<td align=center><b>Purpose</b></td>
<td align=left>• No padding<br>• Drops last convolution if dimensions do not match</td>
<td align=left>• Padding such that feature map size has size $\Bigl\lceil\frac{I}{S}\Bigr\rceil$<br>• Output size is mathematically convenient<br>• Also called 'half' padding</td>
<td align=left>• Maximum padding such that end convolutions are applied on the limits of the input<br>• Filter 'sees' the input end-to-end</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#hyperparameters id=hyperparameters></a>Tuning hyperparameters</h2>
<p><span class="new-item item-r">Parameter compatibility in convolution layer</span> By noting $I$ the length of the input volume size, $F$ the length of the filter, $P$ the amount of zero padding, $S$ the stride, then the output size $O$ of the feature map along that dimension is given by:</p>
<div class=mobile-container>
\[\boxed{O=\frac{I-F+P_\text{start} + P_\text{end}}{S}+1}\]
</div>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=parameter-compatibility-en.jpeg?bc91caf0473dc42f1a495946f67726d3 style=width:100%;max-width:630px>
</center>
</div>
<br>
<p><span class=remark>Remark: often times, $P_\text{start} = P_\text{end} \triangleq P$, in which case we can replace $P_\text{start} + P_\text{end}$ by $2P$ in the formula above.</span></p>
<br>
<p><span class="new-item item-r">Understanding the complexity of the model</span> In order to assess the complexity of a model, it is often useful to determine the number of parameters that its architecture will have. In a given layer of a convolutional neural network, it is done as follows:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:680px; max-width:900px;">
<colgroup>
<col style=width:120px>
<col style=width:33%>
<col style=width:33%>
<col style=width:33%>
</colgroup>
<tbody>
<tr>
<td align=center></td>
<td align=center><b>CONV</b></td>
<td align=center><b>POOL</b></td>
<td align=center><b>FC</b></td>
</tr>
<tr>
<td align=center><b>Illustration</b></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=table-conv.png?79f617dcf0ac221ddfaf21694f6e08ad style="width:100%; max-width:155px;"></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=table-pool.png?e8528df02bafea0840916a83482e42e9 style="width:100%; max-width:155px;"></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=table-fc.png?0074d2fdaa632e724022c13e94a49a22 style="width:100%; max-width:155px;"></td>
</tr>
<tr>
<td align=center><b>Input size</b></td>
<td align=center>$I \times I \times C$</td>
<td align=center>$I \times I \times C$</td>
<td align=center>$N_{\text{in}}$</td>
</tr>
<tr>
<td align=center><b>Output size</b></td>
<td align=center>$O \times O \times K$</td>
<td align=center>$O \times O \times C$</td>
<td align=center>$N_{\text{out}}$</td>
</tr>
<tr>
<td align=center><b>Number of parameters</b></td>
<td align=center>$(F \times F \times C + 1) \cdot K$</td>
<td align=center>$0$</td>
<td align=center>$(N_{\text{in}} + 1 ) \times N_{\text{out}}$</td>
</tr>
<tr>
<td align=center><b>Remarks</b></td>
<td align=left>• One bias parameter per filter<br>
                 • In most cases, $S &lt; F$<br>
                 • A common choice for $K$ is $2C$</td>
<td align=left>• Pooling operation done channel-wise<br>
                 • In most cases, $S = F$</td>
<td align=left>• Input is flattened<br>
                 • One bias parameter per neuron<br>
                 • The number of FC neurons is free of structural constraints</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-g">Receptive field</span> The receptive field at layer $k$ is the area denoted $R_k \times R_k$ of the input that each pixel of the $k$-th activation map can 'see'.
By calling $F_j$ the filter size of layer $j$ and $S_i$ the stride value of layer $i$ and with the convention $S_0 = 1$, the receptive field at layer $k$ can be computed with the formula:
</p><div class=mobile-container>
\[\boxed{R_k = 1 + \sum_{j=1}^{k} (F_j - 1) \prod_{i=0}^{j-1} S_i}\]
</div>
<p><i>In the example below, we have $F_1 = F_2 = 3$ and $S_1 = S_2 = 1$, which gives $R_2 = 1 + 2\cdot 1 + 2\cdot 1 = 5$.</i></p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=receptive-field-a.png?3f718275d9c2de56f2255b2a4797ea87 style=width:100%;max-width:450px>
</center>
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#activation-function id=activation-function></a>Commonly used activation functions</h2>
<p><span class="new-item item-g">Rectified Linear Unit</span> The rectified linear unit layer (ReLU) is an activation function $g$ that is used on all elements of the volume. It aims at introducing non-linearities to the network. Its variants are summarized in the table below:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:650px;">
<colgroup>
<col style=width:33%>
<col style=width:33%>
<col style=width:33%>
</colgroup>
<tbody>
<tr>
<td align=center><b>ReLU</b></td>
<td align=center><b>Leaky ReLU</b></td>
<td align=center><b>ELU</b></td>
</tr>
<tr>
<td align=center>$g(z)=\max(0,z)$</td>
<td align=center>$g(z)=\max(\epsilon z,z)$<br>
                   with $\epsilon\ll1$</td>
<td align=center>$g(z)=\max(\alpha(e^z-1),z)$<br>
                   with $\alpha\ll1$</td>
</tr>
<tr>
<td align=center><img alt=ReLU class=img-responsive netsrc=teaching/cs-229/illustrations/ src=relu.png?6c1d78551355db5c6e4f6f8b5282cfa8 style="width:100%; max-width:200px;"></td>
<td align=center><img alt="Leaky ReLU" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=leaky-relu.png?73b2b4303d1880c69b63d7dfe2be852e style="width:100%; max-width:200px;"></td>
<td align=center><img alt=ELU class=img-responsive netsrc=teaching/cs-230/illustrations/ src=elu.png?d195c8a479724512b56ff0da101361a6 style="width:100%; max-width:200px;"></td>
</tr>
<tr>
<td align=left>• Non-linearity complexities biologically interpretable</td>
<td align=left>• Addresses dying ReLU issue for negative values</td>
<td align=left>• Differentiable everywhere</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-g">Softmax</span> The softmax step can be seen as a generalized logistic function that takes as input a vector of scores $x\in\mathbb{R}^n$ and outputs a vector of output probability $p\in\mathbb{R}^n$ through a softmax function at the end of the architecture. It is defined as follows:</p>
<div class=mobile-container>
\[\boxed{p=\begin{pmatrix}p_1\\\vdots\\p_n\end{pmatrix}}\quad\textrm{where}\quad\boxed{p_i=\frac{e^{x_i}}{\displaystyle\sum_{j=1}^ne^{x_j}}}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#object-detection id=object-detection></a>Object detection</h2>
<p><span class="new-item item-r">Types of models</span> There are 3 main types of object recognition algorithms, for which the nature of what is predicted is different. They are described in the table below:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:600px; max-width:780px;">
<colgroup>
<col style=width:33%>
<col style=width:33%>
<col style=width:33%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Image classification</b></td>
<td align=center><b>Classification w. localization</b></td>
<td align=center><b>Detection</b></td>
</tr>
<tr>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=object-detection-clas-en.jpeg?f380203d7e5d88936e654205473e86c2></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=object-detection-loc-en.jpeg?f8ec96e14fb13f2515f2c179cd326545></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=object-detection-det-en.jpeg?f735de7d1c1bccd2ff1c864b587ad842></td>
</tr>
<tr>
<td align=left>• Classifies a picture<br>
                 • Predicts probability of object</td>
<td align=left>• Detects an object in a picture<br>
                 • Predicts probability of object and where it is located</td>
<td align=left>• Detects up to several objects in a picture<br>
                 • Predicts probabilities of objects and where they are located</td>
</tr>
<tr>
<td align=left>Traditional CNN</td>
<td align=left>Simplified YOLO, R-CNN</td>
<td align=left>YOLO, R-CNN</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-g">Detection</span> In the context of object detection, different methods are used depending on whether we just want to locate the object or detect a more complex shape in the image. The two main ones are summed up in the table below:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:300px; max-width:800px;">
<colgroup>
<col style=width:50%>
<col style=width:50%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Bounding box detection</b></td>
<td align=center><b>Landmark detection</b></td>
</tr>
<tr>
<td align=left>• Detects the part of the image where the object is located</td>
<td align=left>• Detects a shape or characteristics of an object (e.g. eyes)<br>
                 • More granular</td>
</tr>
<tr>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=detection-bounding-box.jpeg?4fab94de8d967605519bfc6bafd14df3 style="width:100%; max-width:300px;"></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=detection-landmark.jpeg?ed85ca8d6c2673f35e8c266c5476690f style="width:100%; max-width:300px;"></td>
</tr>
<tr>
<td align=left>Box of center $(b_x,b_y)$, height $b_h$ and width $b_w$</td>
<td align=left>Reference points $(l_{1x},l_{1y}),$ $...,$ $(l_{nx},l_{ny})$</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-g">Intersection over Union</span> Intersection over Union, also known as $\textrm{IoU}$, is a function that quantifies how correctly positioned a predicted bounding box $B_p$ is over the actual bounding box $B_a$. It is defined as:</p>
<div class=mobile-container>
\[\boxed{\textrm{IoU}(B_p,B_a)=\frac{B_p\cap B_a}{B_p\cup B_a}}\]
</div>
<p><span class=remark>Remark: we always have $\textrm{IoU}\in[0,1]$. By convention, a predicted bounding box $B_p$ is considered as being reasonably good if $\textrm{IoU}(B_p,B_a)\geqslant0.5$.</span></p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=intersection-over-union.jpeg?43dc827ff1461e0391da237a455d69ef style="width:100%; max-width:750px;">
</center>
</div>
<br>
<p><span class="new-item item-b">Anchor boxes</span> Anchor boxing is a technique used to predict overlapping bounding boxes. In practice, the network is allowed to predict more than one box simultaneously, where each box prediction is constrained to have a given set of geometrical properties. For instance, the first prediction can potentially be a rectangular box of a given form, while the second will be another rectangular box of a different geometrical form.</p>
<br>
<p><span class="new-item item-b">Non-max suppression</span> The non-max suppression technique aims at removing duplicate overlapping bounding boxes of a same object by selecting the most representative ones. After having removed all boxes having a probability prediction lower than 0.6, the following steps are repeated while there are boxes remaining:</p>
<p>For a given class,
<br>• Step 1: Pick the box with the largest prediction probability.
<br>• Step 2: Discard any box having an $\textrm{IoU}\geqslant0.5$ with the previous box.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=non-max-suppression-en.jpeg?9cca7cb1256642553de7ffeba7fe0577 style=width:100%;>
</center>
</div>
<br>
<p><span class="new-item item-r">YOLO</span> You Only Look Once (YOLO) is an object detection algorithm that performs the following steps:</p>
<p>• Step 1: Divide the input image into a $G\times G$ grid.
<br>• Step 2: For each grid cell, run a CNN that predicts $y$ of the following form:
</p><div class=mobile-container>
\[\boxed{y=\big[\underbrace{p_c,b_x,b_y,b_h,b_w,c_1,c_2,...,c_p}_{\textrm{repeated }k\textrm{ times}},...\big]^T\in\mathbb{R}^{G\times G\times k\times(5+p)}}\]
</div>
where $p_c$ is the probability of detecting an object, $b_x,b_y,b_h,b_w$ are the properties of the detected bouding box, $c_1,...,c_p$ is a one-hot representation of which of the $p$ classes were detected, and $k$ is the number of anchor boxes.
<br>• Step 3: Run the non-max suppression algorithm to remove any potential duplicate overlapping bounding boxes.<p></p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=yolo-en.jpeg?1eff4444ea01396f48e793bec5f2ef3b style=width:100%;>
</center>
</div>
<br>
<p><span class=remark>Remark: when $p_c=0$, then the network does not detect any object. In that case, the corresponding predictions $b_x, ..., c_p$ have to be ignored.</span></p>
<br>
<p><span class="new-item item-r">R-CNN</span> Region with Convolutional Neural Networks (R-CNN) is an object detection algorithm that first segments the image to find potential relevant bounding boxes and then run the detection algorithm to find most probable objects in those bounding boxes.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=r-cnn-en.jpeg?5bb82ac3b9da8171d5c8a741861c3aa6 style=width:100%;>
</center>
</div>
<br>
<p><span class=remark>Remark: although the original algorithm is computationally expensive and slow, newer architectures enabled the algorithm to run faster, such as Fast R-CNN and Faster R-CNN.</span></p>
<br>
<h2><a aria-hidden=true class=anchor href=#face id=face></a>Face verification and recognition</h2>
<p><span class="new-item item-g">Types of models</span> Two main types of model are summed up in table below:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:300px; max-width: 700px;">
<colgroup>
<col style=width:50%>
<col style=width:50%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Face verification</b></td>
<td align=center><b>Face recognition</b></td>
</tr>
<tr>
<td align=left>• Is this the correct person?<br>
                 • One-to-one lookup</td>
<td align=left>• Is this one of the $K$ persons in the database?<br>
                 • One-to-many lookup</td>
</tr>
<tr>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=face-verification-en.jpeg?9b6ea3c15805347193943fe09692f71b style="width:100%; max-width:300px;"></td>
<td align=center><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=face-recognition-en.jpeg?ed30504000897087c2549f4f964c9441 style="width:100%; max-width:300px;"></td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-b">One Shot Learning</span> One Shot Learning is a face verification algorithm that uses a limited training set to learn a similarity function that quantifies how different two given images are. The similarity function applied to two images is often noted $d(\textrm{image 1}, \textrm{image 2}).$</p>
<br>
<p><span class="new-item item-r">Siamese Network</span> Siamese Networks aim at learning how to encode images to then quantify how different two images are. For a given input image $x^{(i)}$, the encoded output is often noted as $f(x^{(i)})$.</p>
<br>
<p><span class="new-item item-g">Triplet loss</span> The triplet loss $\ell$ is a loss function computed on the embedding representation of a triplet of images $A$ (anchor), $P$ (positive) and $N$ (negative). The anchor and the positive example belong to a same class, while the negative example to another one. By calling $\alpha\in\mathbb{R}^+$ the margin parameter, this loss is defined as follows:</p>
<div class=mobile-container>
\[\boxed{\ell(A,P,N)=\max\left(d(A,P)-d(A,N)+\alpha,0\right)}\]
</div>
<center>
  <div class=row>
      <div class="col-xs-6 col-md-6"><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=triplet-loss-1.png?b18c14e9714ee258b805cb71ac498f4e style="width:100%; max-width:280px"></div>
      <div class="col-xs-6 col-md-6"><img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=triplet-loss-2.png?952990aae18c538c4bb2d93434342c52 style="width:100%; max-width:280px"></div>
  </div>
</center>
<br>
<h2><a aria-hidden=true class=anchor href=#style-transfer id=style-transfer></a>Neural style transfer</h2>
<p><span class="new-item item-g">Motivation</span> The goal of neural style transfer is to generate an image $G$ based on a given content $C$ and a given style $S$.</p>
<div class=mobile-container>
<center>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=neural-style-motivation-en.jpeg?6b66c6e6a11a720c49301837f7834a61 style=width:100%;>
</center>
</div>
<br>
<p><span class="new-item item-b">Activation</span> In a given layer $l$, the activation is noted $a^{[l]}$ and is of dimensions $n_H\times n_w\times n_c$</p>
<br>
<p><span class="new-item item-r">Content cost function</span> The content cost function $J_{\textrm{content}}(C,G)$ is used to determine how the generated image $G$ differs from the original content image $C$. It is defined as follows:</p>
<div class=mobile-container>
\[\boxed{J_{\textrm{content}}(C,G)=\frac{1}{2}||a^{[l](C)}-a^{[l](G)}||^2}\]
</div>
<br>
<p><span class="new-item item-g">Style matrix</span> The style matrix $G^{[l]}$ of a given layer $l$ is a Gram matrix where each of its elements $G_{kk'}^{[l]}$ quantifies how correlated the channels $k$ and $k'$ are. It is defined with respect to activations $a^{[l]}$ as follows:</p>
<div class=mobile-container>
\[\boxed{G_{kk'}^{[l]}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_w^{[l]}}a_{ijk}^{[l]}a_{ijk'}^{[l]}}\]
</div>
<p><span class=remark>Remark: the style matrix for the style image and the generated image are noted $G^{[l](S)}$ and $G^{[l](G)}$ respectively.</span></p>
<br>
<p><span class="new-item item-r">Style cost function</span> The style cost function $J_{\textrm{style}}(S,G)$ is used to determine how the generated image $G$ differs from the style $S$. It is defined as follows:</p>
<div class=mobile-container>
\[\boxed{J_{\textrm{style}}^{[l]}(S,G)=\frac{1}{(2n_Hn_wn_c)^2}||G^{[l](S)}-G^{[l](G)}||_F^2=\frac{1}{(2n_Hn_wn_c)^2}\sum_{k,k'=1}^{n_c}\Big(G_{kk'}^{[l](S)}-G_{kk'}^{[l](G)}\Big)^2}\]
</div>
<br>
<p><span class="new-item item-b">Overall cost function</span> The overall cost function is defined as being a combination of the content and style cost functions, weighted by parameters $\alpha,\beta$, as follows:</p>
<div class=mobile-container>
\[\boxed{J(G)=\alpha J_{\textrm{content}}(C,G)+\beta J_{\textrm{style}}(S,G)}\]
</div>
<p><span class=remark>Remark: a higher value of $\alpha$ will make the model care more about the content while a higher value of $\beta$ will make it care more about the style.</span></p>
<br>
<h2><a aria-hidden=true class=anchor href=#ct-architectures id=ct-architectures></a>Architectures using computational tricks</h2>
<p><span class="new-item item-r">Generative Adversarial Network</span> Generative adversarial networks, also known as GANs, are composed of a generative and a discriminative model, where the generative model aims at generating the most truthful output that will be fed into the discriminative which aims at differentiating the generated and true image.</p>
<div class=mobile-container>
<img class=img-responsive netsrc=teaching/cs-230/illustrations/ src=discriminator-generator-en.jpeg?850896147a318376ce537812fbefe3a8>
</div>
<br>
<p><span class=remark>Remark: use cases using variants of GANs include text to image, music generation and synthesis.</span></p>
<br>
<p><span class="new-item item-r">ResNet</span> The Residual Network architecture (also called ResNet) uses residual blocks with a high number of layers meant to decrease the training error. The residual block has the following characterizing equation:</p>
<div class=mobile-container>
\[\boxed{a^{[l+2]}=g(a^{[l]}+z^{[l+2]})}\]
</div>
<br>
<p><span class="new-item item-r">Inception Network</span> This architecture uses inception modules and aims at giving a try at different convolutions in order to increase its performance through features diversification. In particular, it uses the $1\times1$ convolution trick to limit the computational burden. </p>
<br>
</article> </div> <!-- FOOTER <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-twitter fa-3x fa-fw"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class="fa fa-linkedin fa-3x fa-fw"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-github fa-3x fa-fw"></i></a> <a href="https://scholar.google.com/citations?user=nMnMTm8AAAAJ" onclick=trackOutboundLink(this);><i class="fa fa-google fa-3x fa-fw"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick="trackOutboundLink(this); window.location.href = 'mailto:' + this.dataset.name + '@' + this.dataset.domain + '.' + this.dataset.tld"><i class="fa fa-envelope fa-3x fa-fw"></i></a> </div> </div> </footer> --> </body></html>
<!DOCTYPE html><html lang=en><head><!-- base href=../../ --><title>CS 221 - States-based Models Cheatsheet</title><meta charset=utf-8><meta content="Teaching page of Shervine Amidi, Graduate Student at Stanford University." name=description><meta content="teaching, shervine, shervine amidi, data science" name=keywords><meta content="width=device-width, initial-scale=1" name=viewport>
<link href=https://stanford.edu/~shervine/teaching/cs-221/cheatsheet-states-models rel=canonical>
<link href=../../css/bootstrap.min.css rel=stylesheet>
<link href=../../css/font-awesome.min.css rel=stylesheet>
<link href=../../css/katex.min.css rel=stylesheet>
<link href=../../css/style.min.css rel=stylesheet type=text/css>
<link href=../../css/article.min.css rel=stylesheet>
<script src=../../js/jquery.min.js></script>
<script src=../../js/bootstrap.min.js></script>
<script defer src=../../js/underscore-min.js type=text/javascript></script>
<script defer src=../../js/katex.min.js></script>
<script defer src=../../js/auto-render.min.js></script>
<script defer src=../../js/article.min.js></script>
<script defer src=../../js/lang.min.js></script>
<script async defer src=../../js/buttons.js></script>
</head> <body data-offset=50 data-spy=scroll data-target=.navbar> <!-- HEADER <nav class="navbar navbar-inverse navbar-static-top"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class="collapse navbar-collapse" id=myNavbar> <ul class="nav navbar-nav"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class="nav navbar-nav navbar-center"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class="collapse navbar-collapse" data-target=None id=HiddenNavbar> <ul class="nav navbar-nav navbar-right"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style="padding: 0px;"> <img alt=MIT netsrc=images/ src=../../images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style="padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;"> </a> </ul> </div> </div> </div> </nav> --> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title style=display:none;> <!-- DISPLAY:NONE --> <a href=teaching/cs-221 onclick=trackOutboundLink(this);><img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48 style="width: 12px;">   <b>CS 221 - Artificial Intelligence</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>States-based models</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#tree-search>Tree search</a></div> <div class=dropdown-container> <a href=#tree-search><span>Backtracking search</span></a> <a href=#tree-search><span>Breadth-first search</span></a> <a href=#tree-search><span>Depth-first search</span></a> <a href=#tree-search><span>Iterative deepening</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#graph-search>Graph search</a></div> <div class=dropdown-container> <a href=#graph-search><span>Dynamic programming</span></a> <a href=#graph-search><span>Uniform cost search</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#learning-costs>Learning costs</a></div> <div class=dropdown-container> <a href=#learning-costs><span>Structured perceptron</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#a-star>A star search</a></div> <div class=dropdown-container> <a href=#a-star><span>Heuristic function</span></a> <a href=#a-star><span>Algorithm</span></a> <a href=#a-star><span>Consistency, correctness</span></a> <a href=#a-star><span>Admissibility, efficiency</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#relaxation>Relaxation</a></div> <div class=dropdown-container> <a href=#relaxation><span>Relaxed search problem</span></a> <a href=#relaxation><span>Relaxed heuristic</span></a> <a href=#relaxation><span>Max heuristic</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#markov-decision-processes>Markov decision processes</a></div> <div class=dropdown-container> <a href=#markov-decision-processes><span>Overview</span></a> <a href=#markov-decision-processes><span>Policy evaluation</span></a> <a href=#markov-decision-processes><span>Value iteration</span></a> <a href=#markov-decision-processes><span>Transitions, rewards</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#game-playing>Game playing</a></div> <div class=dropdown-container> <a href=#game-playing><span>Expectimax</span></a> <a href=#game-playing><span>Minimax</span></a> <a href=#game-playing><span>Speeding up minimax</span></a> <a href=#game-playing><span>Simultaneous games</span></a> <a href=#game-playing><span>Non-zero-sum games</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-221-artificial-intelligence/blob/master/en/cheatsheet-states-models.pdf onclick=trackOutboundLink(this); style="color: white; text-decoration:none;"> <i aria-hidden=false class="fa fa-github fa-fw"></i> View PDF version on GitHub </a> </li> </div> </center> </div> <article class="markdown-body entry-content" itemprop=text>
<div class="alert alert-primary" role=alert style=display:none;> <!-- DISPLAY:NONE -->
  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!
</div>
<div class=title-lang style=display:none;> <!-- DISPLAY:NONE -->
  <a aria-hidden=true class=anchor-bis href=#cs-221---artificial-intelligence id=cs-221---artificial-intelligence></a><a href=teaching/cs-221 onclick=trackOutboundLink(this);>CS 221 - Artificial Intelligence</a>
  
  <div style=float:right;display:none;> <!-- DISPLAY:NONE -->
    <div class=input-group>
      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>
        <option selected value=en>English</option>
        <option value=fr>Français</option>
        <option value=tr>Türkçe</option>
      </select>
      <div class=input-group-addon><i class=fa></i></div>
    </div>
  </div>
</div>
<br>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button><b>CS 221 - Artificial Intelligence</b></button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button>CS 229 - Machine Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button>CS 230 - Deep Learning</button>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button>Reflex</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-221/states-models/index.html'; oldhref='teaching/cs-221/cheatsheet-states-models'" type=button><b>States</b></button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/variables-models/index.html'; oldhref='teaching/cs-221/cheatsheet-variables-models'" type=button>Variables</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/logic-models/index.html'; oldhref='teaching/cs-221/cheatsheet-logic-models'" type=button>Logic</button>
  </div>
</div>
<h1>
  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>States-based models with search optimization and MDP
</h1>
<i>By <a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> and <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a></i>
  <div style=float:right;display:none;> <!-- DISPLAY:NONE --><a aria-label="Star afshinea/stanford-cs-221-artificial-intelligence on GitHub" class="github-button fa-fw" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-221-artificial-intelligence onclick=trackOutboundLink(this);>Star</a></div>
<h2>Search optimization</h2>
<p>In this section, we assume that by accomplishing action $a$ from state $s$, we deterministically arrive in state $\textrm{Succ}(s,a)$. The goal here is to determine a sequence of actions $(a_1,a_2,a_3,a_4,...)$ that starts from an initial state and leads to an end state. In order to solve this kind of problem, our objective will be to find the minimum cost path by using states-based models.</p>
<h3><a aria-hidden=true class=anchor href=#tree-search id=ts></a>Tree search</h3>
<p>This category of states-based algorithms explores all possible states and actions. It is quite memory efficient, and is suitable for huge state spaces but the runtime can become exponential in the worst cases.</p>
<div class=mobile-container>
<center>
  <img alt=Tree class=img-responsive netsrc=teaching/cs-221/illustrations/ src=tree.png?2d18a75bbffc54c7db56dd83aed1e224 style=width:100%;max-width:900px>
</center>
</div>
<br>
<p><span class="new-item item-g">Search problem</span> A search problem is defined with:
</p><ul>
  <li>a starting state $s_{\textrm{start}}$
  </li><li>possible actions $\textrm{Actions}(s)$ from state $s$
  </li><li>action cost $\textrm{Cost}(s,a)$ from state $s$ with action $a$
  </li><li>successor $\textrm{Succ}(s,a)$ of state $s$ after action $a$
  </li><li>whether an end state was reached $\textrm{IsEnd}(s)$
</li></ul>
<p></p>
<div class=mobile-container>
<center>
  <img alt="Search problem" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=search-problem.png?5b6272e510ca6d44c87c01a68ccaca3a style=width:100%;max-width:465px>
</center>
</div>
<p>The objective is to find a path that minimizes the cost.</p>
<br>
<p><span class="new-item item-r">Backtracking search</span> Backtracking search is a naive recursive algorithm that tries all possibilities to find the minimum cost path. Here, action costs can be either positive or negative.</p>
<br>
<p><span class="new-item item-b">Breadth-first search (BFS)</span> Breadth-first search is a graph search algorithm that does a level-by-level traversal. We can implement it iteratively with the help of a queue that stores at each step future nodes to be visited. For this algorithm, we can assume action costs to be equal to a constant $c\geqslant0$.</p>
<div class=mobile-container>
<center>
  <img alt=BFS class=img-responsive netsrc=teaching/cs-221/illustrations/ src=bfs-stack.png?28d6deef49cde3d173b8dc167e28ad4a style=width:100%;max-width:420px>
</center>
</div>
<br>
<p><span class="new-item item-b">Depth-first search (DFS)</span> Depth-first search is a search algorithm that traverses a graph by following each path as deep as it can. We can implement it recursively, or iteratively with the help of a stack that stores at each step future nodes to be visited. For this algorithm, action costs are assumed to be equal to 0.</p>
<div class=mobile-container>
<center>
  <img alt=DFS class=img-responsive netsrc=teaching/cs-221/illustrations/ src=dfs-stack.png?5616ba59bd361e621870e036829b95d9 style=width:100%;max-width:420px>
</center>
</div>
<br>
<p><span class="new-item item-b">Iterative deepening</span> The iterative deepening trick is a modification of the depth-first search algorithm so that it stops after reaching a certain depth, which guarantees optimality when all action costs are equal. Here, we assume that action costs are equal to a constant $c\geqslant0$.</p>
<br>
<p><span class="new-item item-r">Tree search algorithms summary</span> By noting $b$ the number of actions per state, $d$ the solution depth, and $D$ the maximum depth, we have:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:500px;">
<colgroup>
<col style=width:35%>
<col style=width:21.75%>
<col style=width:21.75%>
<col style=width:21.75%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Algorithm</b></td>
<td align=center><b>Action costs</b></td>
<td align=center><b>Space</b></td>
<td align=center><b>Time</b></td>
</tr>
<tr>
<td align=center valign=top>Backtracking search</td>
<td align=center valign=top>any</td>
<td align=center valign=top>$\mathcal{O}(D)$</td>
<td align=center valign=top>$\mathcal{O}(b^D)$</td>
</tr>
<tr>
<td align=center valign=top>Breadth-first search</td>
<td align=center valign=top>$c\geqslant0$</td>
<td align=center valign=top>$\mathcal{O}(b^d)$</td>
<td align=center valign=top>$\mathcal{O}(b^d)$</td>
</tr>
<tr>
<td align=center valign=top>Depth-first search</td>
<td align=center valign=top>0</td>
<td align=center valign=top>$\mathcal{O}(D)$</td>
<td align=center valign=top>$\mathcal{O}(b^D)$</td>
</tr>
<tr>
<td align=center valign=top>DFS-Iterative deepening</td>
<td align=center valign=top>$c\geqslant0$</td>
<td align=center valign=top>$\mathcal{O}(d)$</td>
<td align=center valign=top>$\mathcal{O}(b^d)$</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<h3><a aria-hidden=true class=anchor href=#graph-search id=graph-search></a>Graph search</h3>
<p>This category of states-based algorithms aims at constructing optimal paths, enabling exponential savings. In this section, we will focus on dynamic programming and uniform cost search.</p>
<p><span class="new-item item-g">Graph</span> A graph is comprised of a set of vertices $V$ (also called nodes) as well as a set of edges $E$ (also called links).</p>
<div class=mobile-container>
<center>
  <img alt=Graph class=img-responsive netsrc=teaching/cs-221/illustrations/ src=graph.png?76351911a7eff56d9f7de999bd7997ab style=width:100%;max-width:450px>
</center>
</div>
<p><span class=remark>Remark: a graph is said to be acylic when there is no cycle.</span></p>
<br>
<p><span class="new-item item-r">State</span> A state is a summary of all past actions sufficient to choose future actions optimally.</p>
<br>
<p><span class="new-item item-b">Dynamic programming</span> Dynamic programming (DP) is a backtracking search algorithm with memoization (i.e. partial results are saved) whose goal is to find a minimum cost path from state $s$ to an end state $s_\textrm{end}$. It can potentially have exponential savings compared to traditional graph search algorithms, and has the property to only work for acyclic graphs. For any given state $s$, the future cost is computed as follows:</p>
<div class=mobile-container>
\[\boxed{\textrm{FutureCost}(s)=\left\{\begin{array}{lc}0 &amp; \textrm{if IsEnd}(s)\\\underset{a\in\textrm{Actions}(s)}{\textrm{min}}\big[\textrm{Cost}(s,a)+\textrm{FutureCost(Succ}(s,a))\big] &amp; \textrm{otherwise}\end{array}\right.}\]
</div>
<div class=mobile-container>
<center>
  <img alt="Dynamic Programming" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=dynamic-programming.png?6efc6c44c8c7bce9c2c576ecb82ec98c style=width:100%;max-width:450px>
</center>
</div>
<p><span class=remark>Remark: the figure above illustrates a bottom-to-top approach whereas the formula provides the intuition of a top-to-bottom problem resolution.</span></p>
<br>
<p><span class="new-item item-g">Types of states</span> The table below presents the terminology when it comes to states in the context of uniform cost search:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:350px;">
<colgroup>
<col style=width:140px>
<col style=width:100%>
</colgroup>
<tbody>
<tr>
<td align=center><b>State</b></td>
<td align=center><b>Explanation</b></td>
</tr>
<tr>
<td align=center valign=center>Explored $\mathcal{E}$</td>
<td align=left valign=top>States for which the optimal path has already been found</td>
</tr>
<tr>
<td align=center valign=center>Frontier $\mathcal{F}$</td>
<td align=left valign=top>States seen for which we are still figuring out how to get there with the cheapest cost</td>
</tr>
<tr>
<td align=center valign=center>Unexplored $\mathcal{U}$</td>
<td align=left valign=top>States not seen yet</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-g">Uniform cost search</span> Uniform cost search (UCS) is a search algorithm that aims at finding the shortest path from a state $s_\textrm{start}$ to an end state $s_\textrm{end}$. It explores states $s$ in increasing order of $\textrm{PastCost}(s)$ and relies on the fact that all action costs are non-negative.</p>
<div class=mobile-container>
<center>
  <img alt="Uniform Cost Search" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=ucs-example.png?e565704b2370d49fa14f56f8259d3515 style=width:100%;max-width:450px>
</center>
</div>
<p><span class=remark>Remark 1: the UCS algorithm is logically equivalent to Dijkstra's algorithm.</span></p>
<p><span class=remark>Remark 2: the algorithm would not work for a problem with negative action costs, and adding a positive constant to make them non-negative would not solve the problem since this would end up being a different problem.</span></p>
<br>
<p><span class="new-item item-r">Correctness theorem</span> When a state $s$ is popped from the frontier $\mathcal{F}$ and moved to explored set $\mathcal{E}$, its priority is equal to $\textrm{PastCost}(s)$ which is the minimum cost path from $s_\textrm{start}$ to $s$.</p>
<br>
<p><span class="new-item item-b">Graph search algorithms summary</span> By noting $N$ the number of total states, $n$ of which are explored before the end state $s_\textrm{end}$, we have:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:500px;">
<colgroup>
<col style=width:35%>
<col style=width:21.75%>
<col style=width:21.75%>
<col style=width:21.75%>
</colgroup>
<tbody>
<tr>
<td align=center><b>Algorithm</b></td>
<td align=center><b>Acyclicity</b></td>
<td align=center><b>Costs</b></td>
<td align=center><b>Time/space</b></td>
</tr>
<tr>
<td align=center valign=top>Dynamic programming</td>
<td align=center valign=top>yes</td>
<td align=center valign=top>any</td>
<td align=center valign=top>$\mathcal{O}(N)$</td>
</tr>
<tr>
<td align=center valign=top>Uniform cost search</td>
<td align=center valign=top>no</td>
<td align=center valign=top>$c\geqslant0$</td>
<td align=center valign=top>$\mathcal{O}(n\log(n))$</td>
</tr>
</tbody>
</table>
</center>
</div>
<p><span class=remark>Remark: the complexity countdown supposes the number of possible actions per state to be constant.</span></p>
<br>
<h3><a aria-hidden=true class=anchor href=#learning-costs id=learning-costs></a>Learning costs</h3>
<p>Suppose we are not given the values of $\textrm{Cost}(s,a)$, we want to estimate these quantities from a training set of minimizing-cost-path sequence of actions $(a_1, a_2, ..., a_k)$.</p>
<p><span class="new-item item-g">Structured perceptron</span> The structured perceptron is an algorithm aiming at iteratively learning the cost of each state-action pair. At each step, it:
</p><ul>
<li> decreases the estimated cost of each state-action of the true minimizing path $y$ given by the training data,
</li><li> increases the estimated cost of each state-action of the current predicted path $y'$ inferred from the learned weights.
</li></ul><p></p>
<p><span class=remark>Remark: there are several versions of the algorithm, one of which simplifies the problem to only learning the cost of each action $a$, and the other parametrizes $\textrm{Cost}(s,a)$ to a feature vector of learnable weights.</span></p>
<br>
<h3><a aria-hidden=true class=anchor href=#a-star id=a-star></a>$\textrm{A}^{\star}$ search</h3>
<p><span class="new-item item-g">Heuristic function</span> A heuristic is a function $h$ over states $s$, where each $h(s)$ aims at estimating $\textrm{FutureCost}(s)$, the cost of the path from $s$ to $s_\textrm{end}$.</p>
<div class=mobile-container>
<center>
  <img alt="A star" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=a-star.png?e4f6b57d6b77c9ecf9bdd5d7d491e703 style=width:100%;max-width:465px>
</center>
</div>
<br>
<p><span class="new-item item-r">Algorithm</span> $A^{*}$ is a search algorithm that aims at finding the shortest path from a state $s$ to an end state $s_\textrm{end}$. It explores states $s$ in increasing order of $\textrm{PastCost}(s) + h(s)$. It is equivalent to a uniform cost search with edge costs $\textrm{Cost}'(s,a)$ given by:</p>
<div class=mobile-container>
\[\boxed{\textrm{Cost}'(s,a)=\textrm{Cost}(s,a)+h(\textrm{Succ}(s,a))-h(s)}\]
</div>
<p><span class=remark>Remark: this algorithm can be seen as a biased version of UCS exploring states estimated to be closer to the end state.</span></p>
<br>
<p><span class="new-item item-b">Consistency</span> A heuristic $h$ is said to be consistent if it satisfies the two following properties:
</p><ul>
<li>For all states $s$ and actions $a$,
<div class=mobile-container>
\[\boxed{h(s) \leqslant \textrm{Cost}(s,a)+h(\textrm{Succ}(s,a))}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Consistency class=img-responsive netsrc=teaching/cs-221/illustrations/ src=consistency-1.png?4cbccf6f546df017acee0dc45d740b92 style=width:100%;max-width:465px>
</center>
</div>
</li><li>The end state verifies the following:
<div class=mobile-container>
\[\boxed{h(s_{\textrm{end}})=0}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Consistency class=img-responsive netsrc=teaching/cs-221/illustrations/ src=consistency-2.png?539ae50a1bbf1b85bf3f8d0b32c48445 style=width:100%;max-width:203px>
</center>
</div>
</li></ul>
<p></p>
<br>
<p><span class="new-item item-r">Correctness</span> If $h$ is consistent, then $A^*$ returns the minimum cost path.</p>
<br>
<p><span class="new-item item-g">Admissibility</span> A heuristic $h$ is said to be admissible if we have:</p>
<div class=mobile-container>
\[\boxed{h(s)\leqslant\textrm{FutureCost}(s)}\]
</div>
<br>
<p><span class="new-item item-r">Theorem</span> Let $h(s)$ be a given heuristic. We have:</p>
<div class=mobile-container>
\[\boxed{h(s)\textrm{ consistent}\Longrightarrow h(s)\textrm{ admissible}}\]
</div>
<br>
<p><span class="new-item item-r">Efficiency</span> $A^*$ explores all states $s$ satisfying the following equation:</p>
<div class=mobile-container>
\[\boxed{\textrm{PastCost}(s)\leqslant\textrm{PastCost}(s_{\textrm{end}})-h(s)}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Efficiency class=img-responsive netsrc=teaching/cs-221/illustrations/ src=efficiency.png?6306c2c390e4b08c0fe37bc99e6caf5d style=width:100%;max-width:465px>
</center>
</div>
<p><span class=remark>Remark: larger values of $h(s)$ is better as this equation shows it will restrict the set of states $s$ going to be explored.</span></p>
<br>
<h3><a aria-hidden=true class=anchor href=#relaxation id=relaxation></a>Relaxation</h3>
<p>It is a framework for producing consistent heuristics. The idea is to find closed-form reduced costs by removing constraints and use them as heuristics.</p>
<br>
<p><span class="new-item item-g">Relaxed search problem</span> The relaxation of search problem $P$ with costs $\textrm{Cost}$ is denoted $P_{\textrm{rel}}$ with costs $\textrm{Cost}_{\textrm{rel}}$, and satisfies the identity:</p>
<div class=mobile-container>
\[\boxed{\textrm{Cost}_{\textrm{rel}}(s,a)\leqslant\textrm{Cost}(s,a)}\]
</div>
<br>
<p><span class="new-item item-r">Relaxed heuristic</span> Given a relaxed search problem $P_{\textrm{rel}}$, we define the relaxed heuristic $h(s)=\textrm{FutureCost}_{\textrm{rel}}(s)$ as the minimum cost path from $s$ to an end state in the graph of costs $\textrm{Cost}_{\textrm{rel}}(s,a)$.</p>
<br>
<p><span class="new-item item-b">Consistency of relaxed heuristics</span> Let $P_{\textrm{rel}}$ be a given relaxed problem. By theorem, we have:</p>
<div class=mobile-container>
\[\boxed{h(s)=\textrm{FutureCost}_{\textrm{rel}}(s)\Longrightarrow h(s)\textrm{ consistent}}\]
</div>
<br>
<p><span class="new-item item-g">Tradeoff when choosing heuristic</span> We have to balance two aspects in choosing a heuristic:
</p><ul>
<li>Computational efficiency: $h(s)=\textrm{FutureCost}_{\textrm{rel}}(s)$ must be easy to compute. It has to produce a closed form, easier search and independent subproblems.
</li><li>Good enough approximation: the heuristic $h(s)$ should be close to $\textrm{FutureCost}(s)$ and we have thus to not remove too many constraints.
</li></ul><p></p>
<br>
<p><span class="new-item item-g">Max heuristic</span> Let $h_1(s)$, $h_2(s)$ be two heuristics. We have the following property:</p>
<div class=mobile-container>
\[\boxed{h_1(s),\textrm{ }h_2(s)\textrm{ consistent}\Longrightarrow h(s)=\max\{h_1(s),\textrm{ }h_2(s)\}\textrm{ consistent}}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#markov-decision-processes id=markov-decision-processes></a>Markov decision processes</h2>
<p>In this section, we assume that performing action $a$ from state $s$ can lead to several states $s_1',s_2',...$ in a probabilistic manner. In order to find our way between an initial state and an end state, our objective will be to find the maximum value policy by using Markov decision processes that help us cope with randomness and uncertainty.</p>
<h3>Notations</h3>
<p><span class="new-item item-b">Definition</span> The objective of a Markov decision process is to maximize rewards. It is defined with:
  </p><ul>
    <li>a starting state $s_{\textrm{start}}$
    </li><li>possible actions $\textrm{Actions}(s)$ from state $s$
    </li><li>transition probabilities $T(s,a,s')$ from $s$ to $s'$ with action $a$
    </li><li>rewards $\textrm{Reward}(s,a,s')$ from $s$ to $s'$ with action $a$
    </li><li>whether an end state was reached $\textrm{IsEnd}(s)$
    </li><li>a discount factor $0\leqslant\gamma\leqslant1$
  </li></ul>
<p></p>
<div class=mobile-container>
<center>
  <img alt="Markov Decision Process" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=markov-decision-process.png?bdaeab5d6b9b89652d54601199c83c85 style=width:100%;max-width:465px>
</center>
</div>
<br>
<p><span class="new-item item-r">Transition probabilities</span> The transition probability $T(s,a,s')$ specifies the probability of going to state $s'$ after action $a$ is taken in state $s$. Each $s' \mapsto T(s,a,s')$ is a probability distribution, which means that:</p>
<div class=mobile-container>
\[\forall s,a,\quad\boxed{\displaystyle\sum_{s'\in\textrm{ States}}T(s,a,s')=1}\]
</div>
<br>
<p><span class="new-item item-b">Policy</span> A policy $\pi$ is a function that maps each state $s$ to an action $a$, i.e.</p>
<div class=mobile-container>
\[\boxed{\pi : s \mapsto a}\]
</div>
<br>
<p><span class="new-item item-g">Utility</span> The utility of a path $(s_0, ..., s_k)$ is the discounted sum of the rewards on that path. In other words,</p>
<div class=mobile-container>
\[\boxed{u(s_0,...,s_k)=\sum_{i=1}^{k}r_i\gamma^{i-1}}\]
</div>
<div class=mobile-container>
<center>
  <img alt=Utility class=img-responsive netsrc=teaching/cs-221/illustrations/ src=utility.png?858a1983bd7bb62a99dcbdad079e57c7 style=width:100%;max-width:465px>
</center>
</div>
<p><i>The figure above is an illustration of the case $k=4$.</i></p>
<br>
<p><span class="new-item item-g">Q-value</span> The $Q$-value of a policy $\pi$ at state $s$ with action $a$, also denoted $Q_{\pi}(s,a)$, is the expected utility from state $s$ after taking action $a$ and then following policy $\pi$. It is defined as follows:</p>
<div class=mobile-container>
\[\boxed{Q_{\pi}(s,a)=\sum_{s'\in\textrm{ States}}T(s,a,s')\left[\textrm{Reward}(s,a,s')+\gamma V_\pi(s')\right]}\]
</div>
<br>
<p><span class="new-item item-g">Value of a policy</span> The value of a policy $\pi$ from state $s$, also denoted $V_{\pi}(s)$, is the expected utility by following policy $\pi$ from state $s$ over random paths. It is defined as follows:</p>
<div class=mobile-container>
\[\boxed{V_\pi(s)=Q_\pi(s,\pi(s))}\]
</div>
<p><span class=remark>Remark: $V_\pi(s)$ is equal to 0 if $s$ is an end state.</span></p>
<br>
<h3>Applications</h3>
<p><span class="new-item item-r">Policy evaluation</span> Given a policy $\pi$, policy evaluation is an iterative algorithm that aims at estimating $V_\pi$. It is done as follows:
</p><ul>
<li><u>Initialization</u>: for all states $s$, we have
<div class=mobile-container>
\[\boxed{V_\pi^{(0)}(s)\longleftarrow0}\]
</div>
</li><li><u>Iteration</u>: for $t$ from 1 to $T_{\textrm{PE}}$, we have
<div class=mobile-container>
\[\forall s,\quad\boxed{V_\pi^{(t)}(s)\longleftarrow Q_\pi^{(t-1)}(s,\pi(s))}\]
</div>
with
<div class=mobile-container>
\[\boxed{Q_\pi^{(t-1)}(s,\pi(s))=\sum_{s'\in\textrm{ States}}T(s,\pi(s),s')\Big[\textrm{Reward}(s,\pi(s),s')+\gamma V_\pi^{(t-1)}(s')\Big]}\]
</div>
</li></ul><p></p>
<p><span class=remark>Remark: by noting $S$ the number of states, $A$ the number of actions per state, $S'$ the number of successors and $T$ the number of iterations, then the time complexity is of $\mathcal{O}(T_{\textrm{PE}}SS')$.</span></p>
<br>
<p><span class="new-item item-b">Optimal Q-value</span> The optimal $Q$-value $Q_{\textrm{opt}}(s,a)$ of state $s$ with action $a$ is defined to be the maximum $Q$-value attained by any policy. It is computed as follows:</p>
<div class=mobile-container>
\[\boxed{Q_{\textrm{opt}}(s,a)=\sum_{s'\in\textrm{ States}}T(s,a,s')\left[\textrm{Reward}(s,a,s')+\gamma V_\textrm{opt}(s')\right]}\]
</div>
<br>
<p><span class="new-item item-b">Optimal value</span> The optimal value $V_{\textrm{opt}}(s)$ of state $s$ is defined as being the maximum value attained by any policy. It is computed as follows:</p>
<div class=mobile-container>
\[\boxed{V_{\textrm{opt}}(s)=\underset{a\in\textrm{ Actions}(s)}{\textrm{max}}Q_\textrm{opt}(s,a)}\]
</div>
<br>
<p><span class="new-item item-b">Optimal policy</span> The optimal policy $\pi_{\textrm{opt}}$ is defined as being the policy that leads to the optimal values. It is defined by:</p>
<div class=mobile-container>
\[\forall s,\quad\boxed{\pi_{\textrm{opt}}(s)=\underset{a\in\textrm{ Actions}(s)}{\textrm{argmax}}Q_\textrm{opt}(s,a)}\]
</div>
<br>
<p><span class="new-item item-r">Value iteration</span> Value iteration is an algorithm that finds the optimal value $V_{\textrm{opt}}$ as well as the optimal policy $\pi_{\textrm{opt}}$. It is done as follows:
</p><ul>
<li><u>Initialization</u>: for all states $s$, we have
<div class=mobile-container>
\[\boxed{V_{\textrm{opt}}^{(0)}(s)\longleftarrow0}\]
</div>
</li><li><u>Iteration</u>: for $t$ from 1 to $T_{\textrm{VI}}$, we have
<div class=mobile-container>
\[\forall s,\quad\boxed{V_\textrm{opt}^{(t)}(s)\longleftarrow \underset{a\in\textrm{ Actions}(s)}{\textrm{max}}Q_\textrm{opt}^{(t-1)}(s,a)}\]
</div>
with
<div class=mobile-container>
\[\boxed{Q_\textrm{opt}^{(t-1)}(s,a)=\sum_{s'\in\textrm{ States}}T(s,a,s')\Big[\textrm{Reward}(s,a,s')+\gamma V_\textrm{opt}^{(t-1)}(s')\Big]}\]
</div>
</li></ul><p></p>
<p><span class=remark>Remark: if we have either $\gamma &lt; 1$ or the MDP graph being acyclic, then the value iteration algorithm is guaranteed to converge to the correct answer.</span></p>
<br>
<h3>When unknown transitions and rewards</h3>
<p>Now, let's assume that the transition probabilities and the rewards are unknown.</p>
<p><span class="new-item item-r">Model-based Monte Carlo</span> The model-based Monte Carlo method aims at estimating $T(s,a,s')$ and $\textrm{Reward}(s,a,s')$ using Monte Carlo simulation with:
</p><div class=mobile-container>
\[\boxed{\widehat{T}(s,a,s')=\frac{\#\textrm{ times }(s,a,s')\textrm{ occurs}}{\#\textrm{ times }(s,a)\textrm{ occurs}}}\]
</div>
and
<div class=mobile-container>
\[\boxed{\widehat{\textrm{Reward}}(s,a,s')=r\textrm{ in }(s,a,r,s')}\]
</div>
These estimations will be then used to deduce $Q$-values, including $Q_\pi$ and $Q_\textrm{opt}.$<p></p>
<p><span class=remark>Remark: model-based Monte Carlo is said to be off-policy, because the estimation does not depend on the exact policy.</span></p>
<br>
<p><span class="new-item item-r">Model-free Monte Carlo</span> The model-free Monte Carlo method aims at directly estimating $Q_{\pi}$, as follows:
</p><div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)=\textrm{average of }u_t\textrm{ where }s_{t-1}=s, a_t=a}\]
</div>
where $u_t$ denotes the utility starting at step $t$ of a given episode.<p></p>
<p><span class=remark>Remark: model-free Monte Carlo is said to be on-policy, because the estimated value is dependent on the policy $\pi$ used to generate the data.</span></p>
<br>
<p><span class="new-item item-r">Equivalent formulation</span> By introducing the constant $\eta=\frac{1}{1+(\#\textrm{updates to }(s,a))}$ and for each $(s,a,u)$ of the training set, the update rule of model-free Monte Carlo has a convex combination formulation:
</p><div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)\leftarrow(1-\eta)\widehat{Q}_\pi(s,a)+\eta u}\]
</div>
as well as a stochastic gradient formulation:
<div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)\leftarrow\widehat{Q}_\pi(s,a) - \eta (\widehat{Q}_\pi(s,a) - u)}\]
</div>
<p></p>
<br>
<p><span class="new-item item-g">SARSA</span> State-action-reward-state-action (SARSA) is a boostrapping method estimating $Q_\pi$ by using both raw data and estimates as part of the update rule. For each $(s,a,r,s',a')$, we have:</p>
<div class=mobile-container>
\[\boxed{\widehat{Q}_\pi(s,a)\longleftarrow(1-\eta)\widehat{Q}_\pi(s,a)+\eta\Big[r+\gamma\widehat{Q}_\pi(s',a')\Big]}\]
</div>
<p><span class=remark>Remark: the SARSA estimate is updated on the fly as opposed to the model-free Monte Carlo one where the estimate can only be updated at the end of the episode.</span></p>
<br>
<p><span class="new-item item-b">Q-learning</span> $Q$-learning is an off-policy algorithm that produces an estimate for $Q_\textrm{opt}$. On each $(s,a,r,s',a')$, we have:</p>
<div class=mobile-container>
\[\boxed{\widehat{Q}_{\textrm{opt}}(s,a)\leftarrow(1-\eta)\widehat{Q}_{\textrm{opt}}(s,a)+\eta\Big[r+\gamma\underset{a'\in\textrm{ Actions}(s')}{\textrm{max}}\widehat{Q}_{\textrm{opt}}(s',a')\Big]}\]
</div>
<br>
<p><span class="new-item item-g">Epsilon-greedy</span> The epsilon-greedy policy is an algorithm that balances exploration with probability $\epsilon$ and exploitation with probability $1-\epsilon$. For a given state $s$, the policy $\pi_{\textrm{act}}$ is computed as follows:</p>
<div class=mobile-container>
\[\boxed{\pi_\textrm{act}(s)=\left\{\begin{array}{ll}\underset{a\in\textrm{ Actions}}{\textrm{argmax }}\widehat{Q}_\textrm{opt}(s,a) &amp; \textrm{with proba }1-\epsilon\\\textrm{random from Actions}(s) &amp; \textrm{with proba }\epsilon\end{array}\right.}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#game-playing id=game-playing></a>Game playing</h2>
<p>In games (e.g. chess, backgammon, Go), other agents are present and need to be taken into account when constructing our policy.</p>
<p><span class="new-item item-b">Game tree</span> A game tree is a tree that describes the possibilities of a game. In particular, each node is a decision point for a player and each root-to-leaf path is a possible outcome of the game.</p>
<br>
<p><span class="new-item item-b">Two-player zero-sum game</span> It is a game where each state is fully observed and such that players take turns. It is defined with:
</p><ul>
  <li>a starting state $s_{\textrm{start}}$
  </li><li>possible actions $\textrm{Actions}(s)$ from state $s$
  </li><li>successors $\textrm{Succ}(s,a)$ from states $s$ with actions $a$
  </li><li>whether an end state was reached $\textrm{IsEnd}(s)$
  </li><li>the agent's utility $\textrm{Utility}(s)$ at end state $s$
  </li><li>the player $\textrm{Player}(s)$ who controls state $s$
</li></ul><p></p>
<p><span class=remark>Remark: we will assume that the utility of the agent has the opposite sign of the one of the opponent.</span></p>
<br>
<p><span class="new-item item-g">Types of policies</span> There are two types of policies:
</p><ul>
<li>Deterministic policies, denoted $\pi_p(s)$, which are actions that player $p$ takes in state $s$.
</li><li>Stochastic policies, denoted $\pi_p(s,a)\in[0,1]$, which are probabilities that player $p$ takes action $a$ in state $s$.
</li></ul><p></p>
<br>
<p><span class="new-item item-g">Expectimax</span> For a given state $s$, the expectimax value $V_{\textrm{exptmax}}(s)$ is the maximum expected utility of any agent policy when playing with respect to a fixed and known opponent policy $\pi_{\textrm{opp}}$. It is computed as follows:</p>
<div class=mobile-container>
\[\boxed{V_{\textrm{exptmax}}(s)=\left\{\begin{array}{ll}\textrm{Utility}(s) &amp; \textrm{IsEnd}(s)\\\underset{a\in\textrm{Actions}(s)}{\textrm{max}}V_{\textrm{exptmax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{agent}\\\displaystyle\sum_{a\in\textrm{Actions}(s)}\pi_{\textrm{opp}}(s,a)V_{\textrm{exptmax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{opp}\end{array}\right.}\]
</div>
<p><span class=remark>Remark: expectimax is the analog of value iteration for MDPs.</span></p>
<div class=mobile-container>
<center>
  <img alt=Expectimax class=img-responsive netsrc=teaching/cs-221/illustrations/ src=expectimax.png?e45e88a1bab91386c3e1d4ff401a1d97 style=width:100%;max-width:600px>
</center>
</div>
<br>
<p><span class="new-item item-b">Minimax</span> The goal of minimax policies is to find an optimal policy against an adversary by assuming the worst case, i.e. that the opponent is doing everything to minimize the agent's utility. It is done as follows:</p>
<div class=mobile-container>
\[\boxed{V_{\textrm{minimax}}(s)=\left\{\begin{array}{ll}\textrm{Utility}(s) &amp; \textrm{IsEnd}(s)\\\underset{a\in\textrm{Actions}(s)}{\textrm{max}}V_{\textrm{minimax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{agent}\\\underset{a\in\textrm{Actions}(s)}{\textrm{min}}V_{\textrm{minimax}}(\textrm{Succ}(s,a)) &amp; \textrm{Player}(s)=\textrm{opp}\end{array}\right.}\]
</div>
<p><span class=remark>Remark: we can extract $\pi_{\textrm{max}}$ and $\pi_{\textrm{min}}$ from the minimax value $V_{\textrm{minimax}}$.</span></p>
<div class=mobile-container>
<center>
  <img alt=Minimax class=img-responsive netsrc=teaching/cs-221/illustrations/ src=minimax.png?a10b34193ca7a93d1c7a764ee19de949 style=width:100%;max-width:600px>
</center>
</div>
<br>
<p><span class="new-item item-r">Minimax properties</span> By noting $V$ the value function, there are 3 properties around minimax to have in mind:
</p><ul>
<li><i>Property 1</i>: if the agent were to change its policy to any $\pi_{\textrm{agent}}$, then the agent would be no better off.
<div class=mobile-container>
\[\boxed{\forall \pi_{\textrm{agent}},\quad V(\pi_{\textrm{max}},\pi_{\textrm{min}})\geqslant V(\pi_{\textrm{agent}},\pi_{\textrm{min}})}\]
</div>
</li><li><i>Property 2</i>: if the opponent changes its policy from $\pi_{\textrm{min}}$ to $\pi_{\textrm{opp}}$, then he will be no better off.
<div class=mobile-container>
\[\boxed{\forall \pi_{\textrm{opp}},\quad V(\pi_{\textrm{max}},\pi_{\textrm{min}})\leqslant V(\pi_{\textrm{max}},\pi_{\textrm{opp}})}\]
</div>
</li><li><i>Property 3</i>: if the opponent is known to be not playing the adversarial policy, then the minimax policy might not be optimal for the agent.
<div class=mobile-container>
\[\boxed{\forall \pi,\quad V(\pi_{\textrm{max}},\pi)\leqslant V(\pi_{\textrm{exptmax}},\pi)}\]
</div>
</li></ul>
In the end, we have the following relationship:
<div class=mobile-container>
\[\boxed{V(\pi_{\textrm{exptmax}},\pi_{\textrm{min}})\leqslant V(\pi_{\textrm{max}},\pi_{\textrm{min}})\leqslant V(\pi_{\textrm{max}},\pi)\leqslant V(\pi_{\textrm{exptmax}},\pi)}\]
</div>
<p></p>
<br>
<h3>Speeding up minimax</h3>
<p><span class="new-item item-g">Evaluation function</span> An evaluation function is a domain-specific and approximate estimate of the value $V_{\textrm{minimax}}(s)$. It is denoted $\textrm{Eval}(s)$.</p>
<p><span class=remark>Remark: $\textrm{FutureCost}(s)$ is an analogy for search problems.</span></p>
<br>
<p><span class="new-item item-b">Alpha-beta pruning</span> Alpha-beta pruning is a domain-general exact method optimizing the minimax algorithm by avoiding the unnecessary exploration of parts of the game tree. To do so, each player keeps track of the best value they can hope for (stored in $\alpha$ for the maximizing player and in $\beta$ for the minimizing player). At a given step, the condition $\beta &lt; \alpha$ means that the optimal path is not going to be in the current branch as the earlier player had a better option at their disposal.</p>
<div class=mobile-container>
<center>
  <img alt="Alpha beta pruning" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=alpha-beta-pruning.png?30f2f19c5b0621ca55fcce4bdb6f914c style=width:100%;max-width:600px>
</center>
</div>
<br>
<p><span class="new-item item-b">TD learning</span> Temporal difference (TD) learning is used when we don't know the transitions/rewards. The value is based on exploration policy. To be able to use it, we need to know rules of the game $\textrm{Succ}(s,a)$. For each $(s,a,r,s')$, the update is done as follows:</p>
<div class=mobile-container>
\[\boxed{w\longleftarrow w-\eta\big[V(s,w)-(r+\gamma V(s',w))\big]\nabla_wV(s,w)}\]
</div>
<br>
<h3>Simultaneous games</h3>
<p>This is the contrary of turn-based games, where there is no ordering on the player's moves.</p>
<br>
<p><span class="new-item item-g">Single-move simultaneous game</span> Let there be two players $A$ and $B$, with given possible actions. We note $V(a,b)$ to be $A$'s utility if $A$ chooses action $a$, $B$ chooses action $b$. $V$ is called the payoff matrix.</p>
<br>
<p><span class="new-item item-r">Strategies</span> There are two main types of strategies:
</p><ul>
<li>A pure strategy is a single action:
<div class=mobile-container>
\[\boxed{a\in\textrm{Actions}}\]
</div>
</li><li>A mixed strategy is a probability distribution over actions:
<div class=mobile-container>
\[\forall a\in\textrm{Actions},\quad\boxed{0\leqslant\pi(a)\leqslant1}\]
</div>
</li></ul><p></p>
<br>
<p><span class="new-item item-b">Game evaluation</span> The value of the game $V(\pi_A,\pi_B)$ when player $A$ follows $\pi_A$ and player $B$ follows $\pi_B$ is such that:</p>
<div class=mobile-container>
\[\boxed{V(\pi_A,\pi_B)=\sum_{a,b}\pi_A(a)\pi_B(b)V(a,b)}\]
</div>
<br>
<p><span class="new-item item-r">Minimax theorem</span> By noting $\pi_A,\pi_B$ ranging over mixed strategies, for every simultaneous two-player zero-sum game with a finite number of actions, we have:</p>
<div class=mobile-container>
\[\boxed{\max_{\pi_A}\min_{\pi_B}V(\pi_A,\pi_B)=\min_{\pi_B}\max_{\pi_A}V(\pi_A,\pi_B)}\]
</div>
<br>
<h3>Non-zero-sum games</h3>
<p><span class="new-item item-b">Payoff matrix</span> We define $V_p(\pi_A,\pi_B)$ to be the utility for player $p$.</p>
<br>
<p><span class="new-item item-r">Nash equilibrium</span> A Nash equilibrium is $(\pi_A^*,\pi_B^*)$ such that no player has an incentive to change its strategy. We have:</p>
<div class=mobile-container>
\[\boxed{\forall \pi_A, V_A(\pi_A^*,\pi_B^*)\geqslant V_A(\pi_A,\pi_B^*)}\quad\textrm{and}\quad\boxed{\forall \pi_B, V_B(\pi_A^*,\pi_B^*)\geqslant V_B(\pi_A^*,\pi_B)}\]
</div>
<p><span class=remark>Remark: in any finite-player game with finite number of actions, there exists at least one Nash equilibrium.</span></p>
</article> </div> <!-- FOOTER <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-twitter fa-3x fa-fw"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class="fa fa-linkedin fa-3x fa-fw"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-github fa-3x fa-fw"></i></a> <a href="https://scholar.google.com/citations?user=nMnMTm8AAAAJ" onclick=trackOutboundLink(this);><i class="fa fa-google fa-3x fa-fw"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick="trackOutboundLink(this); window.location.href = 'mailto:' + this.dataset.name + '@' + this.dataset.domain + '.' + this.dataset.tld"><i class="fa fa-envelope fa-3x fa-fw"></i></a> </div> </div> </footer> --> </body></html>
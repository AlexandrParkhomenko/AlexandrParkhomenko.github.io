<!DOCTYPE html><html lang=en><head><!-- base href=../../ --><title>CS 229 - Unsupervised Learning Cheatsheet</title><meta charset=utf-8><meta content="Teaching page of Shervine Amidi, Graduate Student at Stanford University." name=description><meta content="teaching, shervine, shervine amidi, data science" name=keywords><meta content="width=device-width, initial-scale=1" name=viewport>
<link href=https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-unsupervised-learning rel=canonical>
<link href=../../css/bootstrap.min.css rel=stylesheet>
<link href=../../css/font-awesome.min.css rel=stylesheet>
<link href=../../css/katex.min.css rel=stylesheet>
<link href=../../css/style.min.css rel=stylesheet type=text/css>
<link href=../../css/article.min.css rel=stylesheet>
<script src=../../js/jquery.min.js></script>
<script src=../../js/bootstrap.min.js></script>
<script defer src=../../js/underscore-min.js type=text/javascript></script>
<script defer src=../../js/katex.min.js></script>
<script defer src=../../js/auto-render.min.js></script>
<script defer src=../../js/article.min.js></script>
<script defer src=../../js/lang.min.js></script>
<script async defer src=../../js/buttons.js></script>
</head> <body data-offset=50 data-spy=scroll data-target=.navbar> <!-- HEADER <nav class="navbar navbar-inverse navbar-static-top"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class="collapse navbar-collapse" id=myNavbar> <ul class="nav navbar-nav"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class="nav navbar-nav navbar-center"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class="collapse navbar-collapse" data-target=None id=HiddenNavbar> <ul class="nav navbar-nav navbar-right"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style="padding: 0px;"> <img alt=MIT netsrc=images/ src=../../images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style="padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;"> </a> </ul> </div> </div> </div> </nav> --> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title style=display:none;> <!-- DISPLAY:NONE --> <a href=teaching/cs-229 onclick=trackOutboundLink(this);><img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48 style="width: 15px;">   <b>CS 229 - Machine Learning</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>Unsupervised Learning</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#intro>Введение</a></div> <div class=dropdown-container> <a href=#intro><span>Мотивация</span></a> <a href=#intro><span>Неравенство Йенсена</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#clustering>Кластеризация</a></div> <div class=dropdown-container> <a href=#clustering><span>Максимизация Ожидания</span></a> <a href=#clustering><span>k-средние</span></a> <a href=#clustering><span>Иерархическая кластеризация</span></a> <a href=#clustering><span>Метрики</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#dimension-reduction>Уменьшение размерности</a></div> <div class=dropdown-container> <a href=#dimension-reduction><span>PCA</span></a> <a href=#dimension-reduction><span>ICA</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-unsupervised-learning.pdf onclick=trackOutboundLink(this); style="color: white; text-decoration:none;"> <i aria-hidden=false class="fa fa-github fa-fw"></i> View PDF version on GitHub </a> </li> </div> </center> </div> <article class="markdown-body entry-content" itemprop=text>
<div class="alert alert-primary" role=alert style=display:none;> <!-- DISPLAY:NONE -->
  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!
</div>
<div class=title-lang style=display:none;> <!-- DISPLAY:NONE -->
  <a aria-hidden=true class=anchor-bis href=#cs-229---machine-learning id=cs-229---machine-learning></a><a href=teaching/cs-229 onclick=trackOutboundLink(this);>CS 229 - Machine Learning</a>
  
  <div style=float:right;display:none;> <!-- DISPLAY:NONE -->
    <div class=input-group>
      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>
        <option value=ar>العربية</option>
        <option selected value=en>English</option>
        <option value=es>Español</option>
        <option value=fa>فارسی</option>
        <option value=fr>Français</option>
        <option value=ko>한국어</option>
        <option value=pt>Português</option>
        <option value=tr>Türkçe</option>
        <option value=vi>Tiếng Việt</option>
        <option value=zh>简中</option>
        <option value=zh-tw>繁中</option>
      </select>
      <div class=input-group-addon><i class=fa></i></div>
    </div>
  </div>
</div>
<br>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button>CS 221 - Artificial Intelligence</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button><B>CS 229 - Machine Learning</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button>CS 230 - Deep Learning</button>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/probability/index.html'; oldhref='teaching/cs-229/refresher-probabilities-statistics'" type=button>Probabilities</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/linear-algebra/index.html'; oldhref='teaching/cs-229/refresher-algebra-calculus'" type=button>Algebra</button>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button>Supervised Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-229/unsupervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-unsupervised-learning'" type=button><B>Unsupervised Learning</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/deep-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-deep-learning'" type=button>Deep Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/machine-learning-tips-and-tricks/index.html'; oldhref='teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks'" type=button>Tips and tricks</button>
  </div>
</div>
<h1>
  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>Шпаргалка по обучению без учителя
  <div style=float:right;display:none;> <!-- DISPLAY:NONE --><a aria-label="Star afshinea/stanford-cs-229-machine-learning on GitHub" class="github-button fa-fw" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-229-machine-learning onclick=trackOutboundLink(this);>Star</a></div>
</h1>
<i><!-- By --><a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> и <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a>;<a href=https://github.com/AlexandrParkhomenko onclick=trackOutboundLink(this);> Alexandr Parkhomenko</a> и {здесь можете быть Вы}</i>
<h2><a aria-hidden=true class=anchor href=#intro id=intro></a>Введение в обучение без учителя</h2>
<p><span class="new-item item-b">Мотивация</span> цель обучения без учителя - найти скрытые закономерности в неразмеченных данных $\{x^{(1)},...,x^{(m)}\}$.</p>
<br>
<p><span class="new-item item-r">Неравенство Йенсена</span> Пусть $f$ - выпуклая функция, а $X$ - случайная величина. $E$ - математическое ожидание. Имеем следующее неравенство:</p>
<div class=mobile-container>
\[\boxed{E[f(X)]\geqslant f(E[X])}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#clustering id=clustering></a>Кластеризация</h2>
<h3>Максимизация Ожидания</h3>
<p><span class="new-item item-r">Скрытые величины</span> это скрытые/ненаблюдаемые величины, которые затрудняют задачи оценки, и часто обозначаются буквой $z$. Вот наиболее распространенные настройки, в которых присутствуют скрытые величины:</p>
<div class=mobile-container>
<center>
<table>
<tbody>
<tr>
<td align=center><b>Настройка</b></td>
<td align=center><b>Latent variable $z$</b></td>
<td align=center>$x|z$</td>
<td align=center><b>Комментарии</b></td>
</tr>
<tr>
<td align=center>Смесь $k$ гауссианов</td>
<td align=center>$\textrm{Multinomial}(\phi)$</td>
<td align=center>$\mathcal{N}(\mu_j,\Sigma_j)$</td>
<td align=center>$\mu_j\in\mathbb{R}^n, \phi\in\mathbb{R}^k$</td>
</tr>
<tr>
<td align=center>Факторный анализ</td>
<td align=center>$\mathcal{N}(0,I)$</td>
<td align=center>$\mathcal{N}(\mu+\Lambda z,\psi)$</td>
<td align=center>$\mu_j\in\mathbb{R}^n$</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-g">Алгоритм</span> Алгоритм ожидания-максимизации (Expectation-Maximization, EM) дает эффективный метод оценки параметра $\theta$ посредством оценки максимального правдоподобия путем многократного построения нижней границы правдоподобия (E-шаг) и оптимизации этой нижней границы (M-шаг) следующим образом:
</p><ul>
<li>E-step: Evaluate the posterior probability $Q_{i}(z^{(i)})$ that each data point $x^{(i)}$ came from a particular cluster $z^{(i)}$ as follows:
<div class=mobile-container>
\[\boxed{Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\theta)}\]
</div>
</li><li>M-step: Use the posterior probabilities $Q_i(z^{(i)})$ as cluster specific weights on data points $x^{(i)}$ to separately re-estimate each cluster model as follows:
<div class=mobile-container>
\[\boxed{\theta_i=\underset{\theta}{\textrm{argmax }}\sum_i\int_{z^{(i)}}Q_i(z^{(i)})\log\left(\frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\right)dz^{(i)}}\]<p></p>
</div>
</li></ul>
<center>
<img alt=Illustration class=img-responsive netsrc=teaching/cs-229/illustrations/ src=expectation-maximization-en.png?ed72a10f73a6d201e2ad20e04d145e82>
</center>
<br>
<h3>$k$-means clustering</h3>
<p>Мы обозначаем $c^{(i)}$ кластер точки данных $i$ и $\mu_j$ центр кластера $j$.</p>
<br>
<p><span class="new-item item-g">Алгоритм</span> после случайной инициализации центроидов кластера $\mu_1,\mu_2,...,\mu_k\in\mathbb{R}^n$ алгоритм $k$-средних повторяет следующий шаг до сходимости:</p>
<div class=mobile-container>
\[\boxed{c^{(i)}=\underset{j}{\textrm{arg min}}||x^{(i)}-\mu_j||^2}\quad\textrm{and}\quad\boxed{\mu_j=\frac{\displaystyle\sum_{i=1}^m1_{\{c^{(i)}=j\}}x^{(i)}}{\displaystyle\sum_{i=1}^m1_{\{c^{(i)}=j\}}}}\]
</div>
<center>
<img alt=Illustration class=img-responsive netsrc=teaching/cs-229/illustrations/ src=k-means-en.png?9925605d814ddadebcae2ae4754ab0a4>
</center>
<br>
<p><span class="new-item item-b">Функция искажения</span> Чтобы увидеть, сходится ли алгоритм, мы смотрим на функцию искажения, определенную следующим образом:</p>
<div class=mobile-container>
\[\boxed{J(c,\mu)=\sum_{i=1}^m||x^{(i)}-\mu_{c^{(i)}}||^2}\]
</div>
<br>
<h3>Иерархическая кластеризация</h3>
<p><span class="new-item item-g">Алгоритм</span> Это алгоритм кластеризации с агломеративным иерархическим подходом, который последовательно создает вложенные кластеры.</p>
<br>
<p><span class="new-item item-b">Типы</span> Существуют различные виды алгоритмов иерархической кластеризации, которые направлены на оптимизацию различных целевых функций, которые приведены в таблице ниже:</p>
<div class=mobile-container>
<center>
<table>
<colgroup><col width=33%>
<col width=33%>
<col width=33%>
</colgroup><tbody>
<tr>
<td align=center><b>Связь Уорда</b></td>
<td align=center><b>Средняя связь</b></td>
<td align=center><b>Полная связь</b></td>
</tr>
<tr>
<td align=center>Минимизирует расстояние в пределах кластера</td>
<td align=center>Минимизирует среднее расстояние между парами кластеров</td>
<td align=center>Минимизирует максимальное расстояние между парами кластеров</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<h3>Кластеризация показателей оценки</h3>
<p>В условиях обучения без учителя часто бывает трудно оценить производительность модели, поскольку у нас нет основных меток истинности, как это было в условиях обучения с учителем.</p>
<p><span class="new-item item-b">Silhouette coefficient</span> By noting $a$ and $b$ the mean distance between a sample and all other points in the same class, and between a sample and all other points in the next nearest cluster, the silhouette coefficient $s$ for a single sample is defined as follows:</p>
<div class=mobile-container>
\[\boxed{s=\frac{b-a}{\max(a,b)}}\]
</div>
<br>
<p><span class="new-item item-b">Индекс Калински-Харабаза</span> Обозначим $k$ количество кластеров, $B_k$ и $W_k$ матрицы дисперсии между кластерами и внутри кластеров, соответственно определяемые как
</p><div class=mobile-container>
\[B_k=\sum_{j=1}^kn_{c^{(i)}}(\mu_{c^{(i)}}-\mu)(\mu_{c^{(i)}}-\mu)^T,\quad\quad W_k=\sum_{i=1}^m(x^{(i)}-\mu_{c^{(i)}})(x^{(i)}-\mu_{c^{(i)}})^T\]
</div>
индекс Калински-Харабаза $s(k)$ показывает, насколько хорошо модель кластеризации определяет свои кластеры, так что чем выше оценка, тем более плотными и хорошо разделенными являются кластеры. Это определяется следующим образом:<p></p>
<div class=mobile-container>
\[\boxed{s(k)=\frac{\textrm{Tr}(B_k)}{\textrm{Tr}(W_k)}\times\frac{N-k}{k-1}}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#dimension-reduction id=dimension-reduction></a>Уменьшение размерности</h2>
<h3>Метод главных компонент</h3>
<p>Это метод уменьшения размерности, который находит направления максимизации дисперсии для проецирования данных.</p>
<p><span class="new-item item-b">Собственное значение, собственный вектор</span> Для матрицы $A\in\mathbb{R}^{n\times n}$ $\lambda$ называется собственным значением $A$, если существует вектор $z\in\mathbb{R}^n\backslash\{0\}$, называемый собственным вектором, так что у нас есть:</p>
<div class=mobile-container>
\[\boxed{Az=\lambda z}\]
</div>
<br>
<p><span class="new-item item-r">Спектральная теорема</span> Пусть $A\in\mathbb{R}^{n\times n}$. Если $A$ симметрична, то $A$ диагонализуема вещественной ортогональной матрицей $U\in\mathbb{R}^{n\times n}$. Обозначим $\Lambda=\textrm{diag}(\lambda_1,...,\lambda_n)$, у нас есть:</p>
<div class=mobile-container>
\[\boxed{\exists\Lambda\textrm{ diagonal},\quad A=U\Lambda U^T}\]
</div>
<br>
<p><span class=remark>Примечание: собственный вектор, связанный с наибольшим собственным значением, называется главным собственным вектором матрицы $A$.</span></p>
<br>
<p><span class="new-item item-g">Алгоритм</span> Principal Component Analysis (PCA) - это метод уменьшения размерности, который проецирует данные на $k$ измерений, максимизируя дисперсию данных следующим образом:
</p><ul>
<li><u>Step 1</u>: Normalize the data to have a mean of 0 and standard deviation of 1.
<div class=mobile-container>
\[\boxed{x_j^{(i)}\leftarrow\frac{x_j^{(i)}-\mu_j}{\sigma_j}}\quad\textrm{where}\quad\boxed{\mu_j = \frac{1}{m}\sum_{i=1}^mx_j^{(i)}}\quad\textrm{and}\quad\boxed{\sigma_j^2=\frac{1}{m}\sum_{i=1}^m(x_j^{(i)}-\mu_j)^2}\]
</div>
</li><li><u>Step 2</u>: Compute $\displaystyle\Sigma=\frac{1}{m}\sum_{i=1}^mx^{(i)}{x^{(i)}}^T\in\mathbb{R}^{n\times n}$, which is symmetric with real eigenvalues.
</li><li><u>Step 3</u>: Compute $u_1, ..., u_k\in\mathbb{R}^n$ the $k$ orthogonal principal eigenvectors of $\Sigma$, i.e. the orthogonal eigenvectors of the $k$ largest eigenvalues.
</li><li><u>Step 4</u>: Project the data on $\textrm{span}_\mathbb{R}(u_1,...,u_k)$.
</li></ul><p></p>
<p>Эта процедура максимизирует дисперсию всех $k$-мерных пространств.</p>
<center>
<img alt=Illustration class=img-responsive netsrc=teaching/cs-229/illustrations/ src=pca-en.png?4be4617788fd40f4e998b530b75f4149>
</center>
<br>
<h3>Метод независимых компонент</h3>
<p>Это метод, предназначенный для поиска основных источников генерации.</p>
<p><span class="new-item item-r">Предположения</span> Мы предполагаем, что наши данные $x$ были сгенерированы $n$-мерным исходным вектором $s=(s_1,...,s_n)$, где $s_i$ - независимые случайные величины, посредством смешивающей и невырожденной матрицы $A$ следующим образом:</p>
<div class=mobile-container>
\[\boxed{x=As}\]
</div>
<p>Цель состоит в том, чтобы найти матрицу разложения $W=A^{-1}$.</p>
<br>
<p><span class="new-item item-g">Алгоритм анализа независимых компонент Белла и Сейновского</span> Bell and Sejnowski Independent Component Analysis, ICA - Этот алгоритм находит матрицу разложения $W$, выполнив следующие шаги:
</p><ul>
<li>Записать вероятность $x=As=W^{-1}s$ как:
<div class=mobile-container>
\[p(x)=\prod_{i=1}^np_s(w_i^Tx)\cdot|W|\]
</div>
</li><li>Write the log likelihood given our training data $\{x^{(i)}, i\in[\![1,m]\!]\}$ and by noting $g$ the sigmoid function as:
<div class=mobile-container>
\[l(W)=\sum_{i=1}^m\left(\sum_{j=1}^n\log\Big(g'(w_j^Tx^{(i)})\Big)+\log|W|\right)\]
</div>
</li></ul>
Следовательно, правило обучения стохастическому градиентному восхождению таково, что для каждого обучающего примера $x^{(i)}$ мы обновляем $W$ следующим образом:
<div class=mobile-container>
\[\boxed{W\longleftarrow W+\alpha\left(\begin{pmatrix}1-2g(w_1^Tx^{(i)})\\1-2g(w_2^Tx^{(i)})\\\vdots\\1-2g(w_n^Tx^{(i)})\end{pmatrix}{x^{(i)}}^T+(W^T)^{-1}\right)}\]
</div>
<p></p>
</article> </div> <!-- FOOTER <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-twitter fa-3x fa-fw"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class="fa fa-linkedin fa-3x fa-fw"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-github fa-3x fa-fw"></i></a> <a href="https://scholar.google.com/citations?user=nMnMTm8AAAAJ" onclick=trackOutboundLink(this);><i class="fa fa-google fa-3x fa-fw"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick="trackOutboundLink(this); window.location.href = 'mailto:' + this.dataset.name + '@' + this.dataset.domain + '.' + this.dataset.tld"><i class="fa fa-envelope fa-3x fa-fw"></i></a> </div> </div> </footer> --> </body></html>
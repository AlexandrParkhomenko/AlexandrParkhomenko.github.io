<!DOCTYPE html><html lang=en><head><!-- base href=../../ --><title>CS 221 - Reflex-based Models Cheatsheet</title><meta charset=utf-8><meta content="Teaching page of Shervine Amidi, Graduate Student at Stanford University." name=description><meta content="teaching, shervine, shervine amidi, data science" name=keywords><meta content="width=device-width, initial-scale=1" name=viewport><link href=https://stanford.edu/~shervine/teaching/cs-221/cheatsheet-reflex-models rel=canonical>
<link href=../../css/bootstrap.min.css rel=stylesheet>
<link href=../../css/font-awesome.min.css rel=stylesheet>
<link href=../../css/katex.min.css rel=stylesheet>
<link href=../../css/style.min.css rel=stylesheet type=text/css>
<link href=../../css/article.min.css rel=stylesheet>
<script src=../../js/jquery.min.js></script>
<script src=../../js/bootstrap.min.js></script>
<script defer src=../../js/underscore-min.js type=text/javascript></script>
<script defer src=../../js/katex.min.js></script>
<script defer src=../../js/auto-render.min.js></script>
<script defer src=../../js/article.min.js></script>
<script defer src=../../js/lang.min.js></script>
<script async defer src=../../js/buttons.js></script>
</head> <body data-offset=50 data-spy=scroll data-target=.navbar> <!-- HEADER <nav class="navbar navbar-inverse navbar-static-top"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class="collapse navbar-collapse" id=myNavbar> <ul class="nav navbar-nav"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class="nav navbar-nav navbar-center"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class="collapse navbar-collapse" data-target=None id=HiddenNavbar> <ul class="nav navbar-nav navbar-right"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style="padding: 0px;"> <img alt=MIT netsrc=images/ src=../../images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style="padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;"> </a> </ul> </div> </div> </div> </nav> --> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title style=display:none;> <!-- DISPLAY:NONE --> <a href=teaching/cs-221 onclick=trackOutboundLink(this);><img alt=Stanford netsrc=images/ src=../../images/stanford-logo.png?f7176222abba492681ca93190e078e48 style="width: 12px;">   <b>CS 221 - Artificial Intelligence</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>Reflex-based models</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#linear-predictors>Linear predictors</a></div> <div class=dropdown-container> <a href=#linear-predictors><span>Feature vector</span></a> <a href=#linear-predictors><span>Linear classifier/regression</span></a> <a href=#linear-predictors><span>Margin</span></a> <a href=#linear-predictors><span>Residual</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#loss-minimization>Loss minimization</a></div> <div class=dropdown-container> <a href=#loss-minimization><span>Loss function</span></a> <a href=#loss-minimization><span>Framework</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#non-linear-predictors>Non-linear predictors</a></div> <div class=dropdown-container> <a href=#non-linear-predictors><span>k-nearest neighbors</span></a> <a href=#non-linear-predictors><span>Neural networks</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#stochastic-gradient-descent>Stochastic gradient descent</a></div> <div class=dropdown-container> <a href=#stochastic-gradient-descent><span>Gradient</span></a> <a href=#stochastic-gradient-descent><span>Stochastic updates</span></a> <a href=#stochastic-gradient-descent><span>Batch updates</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#fine-tuning-models>Fine-tuning models</a></div> <div class=dropdown-container> <a href=#fine-tuning-models><span>Hypothesis class</span></a> <a href=#fine-tuning-models><span>Backpropagation</span></a> <a href=#fine-tuning-models><span>Regularization</span></a> <a href=#fine-tuning-models><span>Sets vocabulary</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#unsupervised-learning>Unsupervised Learning</a></div> <div class=dropdown-container> <a href=#unsupervised-learning><span>k-means</span></a> <a href=#unsupervised-learning><span>Principal components analysis</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-221-artificial-intelligence/blob/master/en/cheatsheet-reflex-models.pdf onclick=trackOutboundLink(this); style="color: white; text-decoration:none;"> <i aria-hidden=false class="fa fa-github fa-fw"></i> Посмотреть PDF-версию на GitHub </a> </li> </div> </center> </div> <article class="markdown-body entry-content" itemprop=text>
<div class="alert alert-primary" role=alert style=display:none;> <!-- DISPLAY:NONE -->
  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!
</div>
<div class=title-lang style=display:none;> <!-- DISPLAY:NONE -->
  <a aria-hidden=true class=anchor-bis href=#cs-221---artificial-intelligence id=cs-221---artificial-intelligence></a><a href=teaching/cs-221 onclick=trackOutboundLink(this);>CS 221 - Artificial Intelligence</a>
  
  <div style=float:right;display:none;> <!-- DISPLAY:NONE -->
    <div class=input-group>
      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>
        <option selected value=en>English</option>
        <option value=fr>Français</option>
        <option value=tr>Türkçe</option>
      </select>
      <div class=input-group-addon><i class=fa></i></div>
    </div>
  </div>
</div>
<br>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button><B>CS 221 - Artificial Intelligence</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-229/supervised-learning/index.html'; oldhref='teaching/cs-229/cheatsheet-supervised-learning'" type=button>CS 229 - Machine Learning</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-230/convolutional-neural-networks/index.html'; oldhref='teaching/cs-230/cheatsheet-convolutional-neural-networks'" type=button>CS 230 - Deep Learning</button>
  </div>
</div>
<div aria-label=... class="btn-group btn-group-justified" role=group>
  <div class=btn-group role=group>
    <button class="btn btn-default active" onclick="location.href='../../cs-221/reflex-models/index.html'; oldhref='teaching/cs-221/cheatsheet-reflex-models'" type=button><B>Reflex</B></BUTTON>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/states-models/index.html'; oldhref='teaching/cs-221/cheatsheet-states-models'" type=button>States</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/variables-models/index.html'; oldhref='teaching/cs-221/cheatsheet-variables-models'" type=button>Variables</button>
  </div>
  <div class=btn-group role=group>
    <button class="btn btn-default" onclick="location.href='../../cs-221/logic-models/index.html'; oldhref='teaching/cs-221/cheatsheet-logic-models'" type=button>Logic</button>
  </div>
</div>
<h1>
  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>Модели на основе рефлексов с машинным обучением
</h1>
<i>By <a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> and <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a></i>
  <div style=float:right;display:none;> <!-- DISPLAY:NONE --><a aria-label="Star afshinea/stanford-cs-221-artificial-intelligence on GitHub" class="github-button fa-fw" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-221-artificial-intelligence onclick=trackOutboundLink(this);>Star</a></div>
<h2><a aria-hidden=true class=anchor href=#linear-predictors id=linear-predictors></a>Linear predictors</h2>
<p>В этом разделе мы рассмотрим модели, основанные на рефлексах, которые можно улучшить с опытом, просмотрев образцы с парами вход-выход.</p>
<br>
<p><span class="new-item item-g">Feature vector</span> The feature vector of an input $x$ is denoted $\phi(x)$ and is such that:</p>
<div class=mobile-container>
\[\boxed{\phi(x)=\left[\begin{array}{c}\phi_1(x)\\\vdots\\\phi_d(x)\end{array}\right]\in\mathbb{R}^d}\]
</div>
<br>
<p><span class="new-item item-r">Score</span> The score $s(x,w)$ of an example $(\phi(x),y) \in \mathbb{R}^d \times \mathbb{R}$ associated to a linear model of weights $w \in \mathbb{R}^d$ is given by the inner product:</p>
<div class=mobile-container>
\[\boxed{s(x,w)=w\cdot\phi(x)}\]
</div>
<br>
<h3>Classification</h3>
<p><span class="new-item item-b">Linear classifier</span> Given a weight vector $w\in\mathbb{R}^d$ and a feature vector $\phi(x)\in\mathbb{R}^d$, the binary linear classifier $f_w$ is given by:</p>
<div class=mobile-container>
\[\boxed{f_w(x)=\textrm{sign}(s(x,w))=\left\{
  \begin{array}{cr}
  +1 &amp; \textrm{if }w\cdot\phi(x)&gt;0\\
  -1 &amp; \textrm{if }w\cdot\phi(x)&lt; 0\\
  ? &amp; \textrm{if }w\cdot\phi(x)=0
  \end{array}\right.}\]
</div>
<div class=mobile-container>
<center>
<img alt="Linear classifier" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=linear-classifier.png?79f320ac5ba3e9d5dae2c573007dbfb6 style=width:100%;max-width:550px>
</center>
</div>
<br>
<p><span class="new-item item-g">Margin</span> The margin $m(x,y,w) \in \mathbb{R}$ of an example $(\phi(x),y) \in \mathbb{R}^d \times \{-1,+1\}$ associated to a linear model of weights $w\in \mathbb{R}^d$ quantifies the confidence of the prediction: larger values are better. It is given by:</p>
<div class=mobile-container>
\[\boxed{m(x,y,w)=s(x,w)\times y}\]
</div>
<br>
<h3>Regression</h3>
<p><span class="new-item item-g">Linear regression</span> Given a weight vector $w\in\mathbb{R}^d$ and a feature vector $\phi(x)\in\mathbb{R}^d$, the output of a linear regression of weights $w$ denoted as $f_w$ is given by:</p>
<div class=mobile-container>
\[\boxed{f_w(x)=s(x,w)}\]
</div>
<p><span class="new-item item-g">Residual</span> The residual $\textrm{res}(x,y,w) \in \mathbb{R}$ is defined as being the amount by which the prediction $f_w(x)$ overshoots the target $y$:</p>
<div class=mobile-container>
\[\boxed{\textrm{res}(x,y,w)=f_w(x)-y}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#loss-minimization id=loss-minimization></a>Loss minimization</h2>
<p><span class="new-item item-g">Loss function</span> A loss function $\textrm{Loss}(x,y,w)$ quantifies how unhappy we are with the weights $w$ of the model in the prediction task of output $y$ from input $x$. It is a quantity we want to minimize during the training process.</p><p>
<br>
</p><p><span class="new-item item-r">Classification case</span> The classification of a sample $x$ of true label $y\in \{-1,+1\}$ with a linear model of weights $w$ can be done with the predictor $f_w(x) \triangleq \textrm{sign}(s(x,w))$. In this situation, a metric of interest quantifying the quality of the classification is given by the margin $m(x,y,w)$, and can be used with the following loss functions:</p>
<div class=mobile-container>
  <table style="table-layout:fixed; width:100%; min-width:725px; max-width:800px;">
    <colgroup>
      <col style=width:125px>
      <col style=width:33%>
      <col style=width:33%>
      <col style=width:33%>
    </colgroup>
<tbody>
<tr>
<td align=center><b>Name</b></td>
<td align=center>Zero-one loss</td>
<td align=center>Hinge loss</td>
<td align=center>Logistic loss</td>
</tr>
<tr>
<td align=center>$\textrm{Loss}(x,y,w)$</td>
<td align=center>$1_{\{m(x,y,w) \leqslant 0\}}$</td>
<td align=center>$\max(1-m(x,y,w), 0)$</td>
<td align=center>$\log(1+e^{-m(x,y,w)})$</td>
</tr>
<tr>
<td align=center><b>Illustration</b></td>
<td align=center style=width:100%;max-width:150px;><img alt="Zero-one loss" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=loss-zero-one.png?f9d7b6349bfa78f70ba7aee99af5e0dc></td>
<td align=center style=width:100%;max-width:150px;><img alt="Hinge loss" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=loss-hinge.png?47a1e04a25970a1ad583d8821ba51ff2></td>
<td align=center style=width:100%;max-width:150px;><img alt="Logistic loss" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=loss-logistic.png?8891a0a53cb3999a126d3df87080e6e5></td>
</tr>
</tbody>
</table>
</div>
<br>
<p><span class="new-item item-b">Regression case</span> The prediction of a sample $x$ of true label $y \in \mathbb{R}$ with a linear model of weights $w$ can be done with the predictor $f_w(x) \triangleq s(x,w)$. In this situation, a metric of interest quantifying the quality of the regression is given by the margin $\textrm{res}(x,y,w)$ and can be used with the following loss functions:</p>
<div class=mobile-container>
  <table style="table-layout:fixed; width:100%; min-width:525px; max-width:575px;">
    <colgroup>
      <col style=width:125px>
      <col style=width:50%>
      <col style=width:50%>
    </colgroup>
<tbody>
<tr>
<td align=center><b>Name</b></td>
<td align=center>Squared loss</td>
<td align=center>Absolute deviation loss</td>
</tr>
<tr>
<td align=center>$\textrm{Loss}(x,y,w)$</td>
<td align=center>$(\textrm{res}(x,y,w))^2$</td>
<td align=center>$|\textrm{res}(x,y,w)|$</td>
</tr>
<tr>
<td align=center><b>Illustration</b></td>
<td align=center style=width:100%;max-width:150px;><img alt="Squared loss" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=loss-squared.png?a1af36015fa59fa7f2e0cabeeee938a5></td>
<td align=center style=width:100%;max-width:150px;><img alt="Absolute deviation loss" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=loss-absolute-deviation.png?06b20ac340980e2a131f3de34e7647a3></td>
</tr>
</tbody>
</table>
</div>
<br>
<p><span class="new-item item-g">Loss minimization framework</span> In order to train a model, we want to minimize the training loss defined as follows:</p>
<div class=mobile-container>
\[\boxed{\textrm{TrainLoss}(w)=\frac{1}{|\mathcal{D}_{\textrm{train}}|}\sum_{(x,y)\in\mathcal{D}_{\textrm{train}}}\textrm{Loss}(x,y,w)}\]
</div>
<br>
<h2><a aria-hidden=true class=anchor href=#non-linear-predictors id=non-linear-predictors></a>Non-linear predictors</h2>
<p><span class="new-item item-g">$k$-nearest neighbors</span> The $k$-nearest neighbors algorithm, commonly known as $k$-NN, is a non-parametric approach where the response of a data point is determined by the nature of its $k$ neighbors from the training set. It can be used in both classification and regression settings.</p>
<div class=mobile-container>
<center>
<img alt="k nearest neighbors" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=k-nearest-neighbors.png?02f80a524bb11e2b7a70b58c9ed3b0f4 style=width:100%;max-width:740px>
</center>
</div>
<p><span class=remark>Примечание: чем выше параметр $k$, тем выше смещение, а чем ниже параметр $k$, тем выше дисперсия.</span></p>
<br>
<p><span class="new-item item-r">Нейронные сети</span> Нейронные сети - это класс моделей, построенных с использованием слоёв. Обычно используемые типы нейронных сетей включают сверточные и рекуррентные нейронные сети. Словарь архитектур нейронных сетей представлен на рисунке ниже:</p>
<div class=mobile-container>
<center>
<img alt="Neural network" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=neural-network-en.png?835862d448ad85bc5a038848d7d7df0b style=width:100%;max-width:600px>
</center>
</div>
<p>By noting $i$ the $i^{th}$ layer of the network and $j$ the $j^{th}$ hidden unit of the layer, we have:</p>
<div class=mobile-container>
\[\boxed{z_j^{[i]}={w_j^{[i]}}^Tx+b_j^{[i]}}\]
</div>
<p>где мы обозначаем $w$, $b$, $x$, $z$ вес, смещение, вход и неактивированный выход нейрона соответственно.</p>
<br>
<div class="alert alert-warning" role=alert>For a more detailed overview of the concepts above, check out the <a class=alert-link nethref=teaching/cs-229/cheatsheet-supervised-learning  href=../../cs-229/supervised-learning/index.html onclick=trackOutboundLink(this);>Supervised Learning cheatsheets</a>!</div>
<h2><a aria-hidden=true class=anchor href=#stochastic-gradient-descent id=stochastic-gradient-descent></a>Stochastic gradient descent</h2>
<p><span class="new-item item-g">Gradient descent</span> By noting $\eta\in\mathbb{R}$ the learning rate (also called step size), the update rule for gradient descent is expressed with the learning rate and the loss function $\textrm{Loss}(x,y,w)$ as follows:</p>
<div class=mobile-container>
\[\boxed{w\longleftarrow w-\eta\nabla_w \textrm{Loss}(x,y,w)}\]
</div>
<br>
<div class=mobile-container>
<center>
<img alt="Gradient descent" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=gradient-descent.png?7d7bdb054f6ff01cb4fc07dd4b33fa7d style=width:100%;max-width:500px>
</center>
</div>
<br>
<p><span class="new-item item-r">Stochastic updates</span> Stochastic gradient descent (SGD) updates the parameters of the model one training example $(\phi(x),y)\in\mathcal{D}_{\textrm{train}}$ at a time. This method leads to sometimes noisy, but fast updates.</p>
<br>
<p><span class="new-item item-b">Пакетные обновления</span> Batch gradient descent (BGD) обновляет параметры модели по одной партии примеров (например, половина обучающего набора) за раз. Этот метод вычисляет стабильные направления обновления с большими вычислительными затратами.</p>
<br>
<h2><a aria-hidden=true class=anchor href=#fine-tuning-models id=fine-tuning-models></a>Fine-tuning models</h2>
<p><span class="new-item item-g">Hypothesis class</span> A hypothesis class $\mathcal{F}$ is the set of possible predictors with a fixed $\phi(x)$ and varying $w$:</p>
<div class=mobile-container>
\[\boxed{\mathcal{F}=\left\{f_w:w\in\mathbb{R}^d\right\}}\]
</div>
<br>
<p><span class="new-item item-r">Logistic function</span> The logistic function $\sigma$, also called the sigmoid function, is defined as:</p>
<div class=mobile-container>
\[\boxed{\forall z\in]-\infty,+\infty[,\quad\sigma(z)=\frac{1}{1+e^{-z}}}\]
</div>
<p><span class=remark>Remark: we have $\sigma'(z)=\sigma(z)(1-\sigma(z))$.</span></p>
<br>
<p><span class="new-item item-b">Backpropagation</span> The forward pass is done through $f_i$, which is the value for the subexpression rooted at $i$, while the backward pass is done through $g_i=\frac{\partial\textrm{out}}{\partial f_i}$ and represents how $f_i$ influences the output.</p>
<div class=mobile-container>
<center>
<img alt=Backpropagation class=img-responsive netsrc=teaching/cs-221/illustrations/ src=backpropagation.png?24671572de19eb4f92606f950c21409d style=width:100%;max-width:450px>
</center>
</div>
<br>
<p><span class="new-item item-g">Approximation and estimation error</span> The approximation error $\epsilon_\text{approx}$ represents how far the entire hypothesis class $\mathcal{F}$ is from the target predictor $g^*$, while the estimation error $\epsilon_{\text{est}}$ quantifies how good the predictor $\hat{f}$ is with respect to the best predictor $f^{*}$ of the hypothesis class $\mathcal{F}$.</p>
<div class=mobile-container>
<center>
<img alt="Approximation and estimation error" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=approximation-model.png?2598ff172e014fd68e0663c2703b69ef style=width:100%;max-width:450px>
</center>
</div>
<br>
<p><span class="new-item item-g">Regularization</span> Regularization aims to keep the model from overfitting to the data and thus deals with high variance issues. The following table sums up the different types of commonly used regularization techniques:</p>
<div class=mobile-container>
<center>
<table style="table-layout:fixed; width:100%; min-width:760px;">
  <colgroup>
    <col style=width:33%>
    <col style=width:33%>
    <col style=width:33%>
  </colgroup>
<tbody>
<tr>
<td align=center><b>LASSO</b></td>
<td align=center><b>Ridge</b></td>
<td align=center><b>Elastic Net</b></td>
</tr>
<tr>
<td align=left>• Уменьшает коэффициенты до 0<br>• Подходит для выбора переменных</td>
<td align=left>Makes coefficients smaller</td>
<td align=left>Компромисс между выбором переменных и небольшими коэффициентами</td>
</tr>
<tr>
<td align=center><img alt=Lasso class=img-responsive netsrc=teaching/cs-221/illustrations/ src=regularization-lasso.png?39ee3006c1ac3594160474a29df13d2b></td>
<td align=center><img alt=Ridge class=img-responsive netsrc=teaching/cs-221/illustrations/ src=regularization-ridge.png?229d65c13a8e0bfe4ac8cdcb04972052></td>
<td align=center><img alt="Elastic Net" class=img-responsive netsrc=teaching/cs-221/illustrations/ src=regularization-elastic-net.png?854b4ab93db8881002a2f2ca72d536be></td>
</tr>
<tr>
<td align=left>$...+\lambda||w||_1$<br>$\lambda\in\mathbb{R}$</td>
<td align=left>$...+\lambda||w||_2^2$<br>$\lambda\in\mathbb{R}$</td>
<td align=left>$...+\lambda\Big[(1-\alpha)||w||_1+\alpha||w||_2^2\Big]$<br>$\lambda\in\mathbb{R},\alpha\in[0,1]$</td>
</tr>
</tbody>
</table>
</center>
</div>
<br>
<p><span class="new-item item-r">Hyperparameters</span> Hyperparameters are the properties of the learning algorithm, and include architecture-related features, the regularization parameter $\lambda$, number of iterations $T$, step size $\eta$, etc.</p>
<br>
<p><span class="new-item item-b">Наборы словарей</span> при выборе модели мы выделяем 3 разные части данных, которые у нас есть, а именно:</p>
<div class=mobile-container>
<center>
<table>
<colgroup><col width=33%>
<col width=33%>
<col width=33%>
</colgroup><tbody>
<tr>
<td align=center><b>Training set</b></td>
<td align=center><b>Validation set</b></td>
<td align=center><b>Testing set</b></td>
</tr>
<tr>
<td align=left>• Model is trained<br>
                 • Обычно 80% набора данных</td>
<td align=left>• Model is assessed<br>
                 • Обычно 20% набора данных<br>
                 • Также называется отложенным или набором для разработки</td>
<td align=left>• Model gives predictions<br>
                 • Unseen data</td>
</tr>
</tbody>
</table>
</center>
</div>
<p>Как только модель выбрана, она обучается на всем наборе данных и тестируется на невиданном тестовом наборе. Они представлены на рисунке ниже:</p>
<div class=mobile-container>
<center>
<img alt="Partition of the dataset" class=img-responsive netsrc=teaching/cs-229/illustrations/ src=train-val-test-en.png?0949795ac868562e193efdc249ae1066 style=width:100%;max-width:550px>
</center>
</div>
<br>
<div class="alert alert-warning" role=alert>For a more detailed overview of the concepts above, check out the <a class=alert-link nethref=teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks href=../../cs-229/machine-learning-tips-and-tricks/index.html onclick=trackOutboundLink(this);>Machine Learning tips and tricks cheatsheets</a>!</div>
<h2><a aria-hidden=true class=anchor href=#unsupervised-learning id=unsupervised-learning></a>Unsupervised Learning</h2>
<p>The class of unsupervised learning methods aims at discovering the structure of the data, which may have rich latent structures.</p>
<h3>$k$-means</h3>
<p><span class="new-item item-g">Clustering</span> Given a training set of $n$ input points $\mathcal{D}_{\textrm{train}}$, the goal of a clustering algorithm is to assign each point $\phi(x_i)$ to a cluster $z_i\in\{1,...,k\}$.</p>
<br>
<p><span class="new-item item-r">Целевая функция</span> функция потерь для одного из основных алгоритмов кластеризации, $k$-средних, определяется выражением:</p>
<div class=mobile-container>
\[\boxed{\textrm{Loss}_{\textrm{k-means}}(x,\mu)=\sum_{i=1}^n||\phi(x_i)-\mu_{z_i}||^2}\]
</div>
<br>
<p><span class="new-item item-b">Algorithm</span> After randomly initializing the cluster centroids $\mu_1,\mu_2,...,\mu_k\in\mathbb{R}^d$, the $k$-means algorithm repeats the following step until convergence:</p>
<div class=mobile-container>
\[\boxed{z_i=\underset{j}{\textrm{arg min}}||\phi(x_i)-\mu_j||^2}\quad\textrm{and}\quad\boxed{\mu_j=\frac{\displaystyle\sum_{i=1}^n1_{\{z_i=j\}}\phi(x_i)}{\displaystyle\sum_{i=1}^n1_{\{z_i=j\}}}}\]
</div>
<center>
<img alt=Illustration class=img-responsive netsrc=teaching/cs-229/illustrations/ src=k-means-en.png?9925605d814ddadebcae2ae4754ab0a4>
</center>
<br>
<h3>Principal Component Analysis</h3>
<p><span class="new-item item-g">Eigenvalue, eigenvector</span> Given a matrix $A\in\mathbb{R}^{d\times d}$, $\lambda$ is said to be an eigenvalue of $A$ if there exists a vector $z\in\mathbb{R}^d\backslash\{0\}$, called eigenvector, such that we have:</p>
<div class=mobile-container>
\[\boxed{Az=\lambda z}\]
</div>
<br>
<p><span class="new-item item-g">Spectral theorem</span> Let $A\in\mathbb{R}^{d\times d}$. If $A$ is symmetric, then $A$ is diagonalizable by a real orthogonal matrix $U\in\mathbb{R}^{d\times d}$. By noting $\Lambda=\textrm{diag}(\lambda_1,...,\lambda_d)$, we have:</p>
<div class=mobile-container>
\[\boxed{\exists\Lambda\textrm{ diagonal},\quad A=U\Lambda U^T}\]
</div>
<br>
<p><span class=remark>Примечание: собственный вектор, связанный с наибольшим собственным значением, называется главным собственным вектором матрицы $A$. (примечание переводчика: Смотри минимизацию нормы Фробениуса матрицы ошибок)</span></p>
<br>
<p><span class="new-item item-r">Алгоритм</span> процедура метода главных компонент - это метод уменьшения размерности, который проецирует данные по $k$ измерениям, максимизируя дисперсию данных следующим образом:
<br>- <u>Step 1</u>: Normalize the data to have a mean of 0 and standard deviation of 1.
</p><div class=mobile-container>
\[\boxed{\phi_j(x_i)\leftarrow\frac{\phi_j(x_i)-\mu_j}{\sigma_j}}\quad\textrm{where}\quad\boxed{\mu_j = \frac{1}{n}\sum_{i=1}^n\phi_j(x_i)}\quad\textrm{and}\quad\boxed{\sigma_j^2=\frac{1}{n}\sum_{i=1}^n(\phi_j(x_i)-\mu_j)^2}\]
</div>
<br>- <u>Step 2</u>: Compute $\displaystyle\Sigma=\frac{1}{n}\sum_{i=1}^n\phi(x_i){\phi(x_i)}^T\in\mathbb{R}^{d\times d}$, которая симметрична действительным собственным значениям.
<br>- <u>Step 3</u>: Compute $u_1, ..., u_k\in\mathbb{R}^d$ the $k$ orthogonal principal eigenvectors of $\Sigma$, т.е. ортогональные собственные векторы $k$ наибольших собственных значений.
<br>- <u>Step 4</u>: Project the data on $\textrm{span}_\mathbb{R}(u_1,...,u_k)$.<p></p>
<p>Эта процедура максимизирует дисперсию всех $k$-мерных пространств.</p>
<center>
<img alt=Illustration class=img-responsive netsrc=teaching/cs-229/illustrations/ src=pca-en.png?4be4617788fd40f4e998b530b75f4149>
</center>
<br>
<div class="alert alert-warning" role=alert>For a more detailed overview of the concepts above, check out the <a class=alert-link nethref=teaching/cs-229/cheatsheet-unsupervised-learning href=../../cs-229/unsupervised-learning/index.html onclick=trackOutboundLink(this);>Unsupervised Learning cheatsheets</a>!</div>
</article> </div> <!-- FOOTER <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-twitter fa-3x fa-fw"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class="fa fa-linkedin fa-3x fa-fw"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class="fa fa-github fa-3x fa-fw"></i></a> <a href="https://scholar.google.com/citations?user=nMnMTm8AAAAJ" onclick=trackOutboundLink(this);><i class="fa fa-google fa-3x fa-fw"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick="trackOutboundLink(this); window.location.href = 'mailto:' + this.dataset.name + '@' + this.dataset.domain + '.' + this.dataset.tld"><i class="fa fa-envelope fa-3x fa-fw"></i></a> </div> </div> </footer> --> </body></html>